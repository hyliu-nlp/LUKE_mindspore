{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d72c1c",
   "metadata": {},
   "source": [
    "# 数据集创建&处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelarts.session import Session\n",
    "# session = Session()\n",
    "# session.obs.download_file(src_obs_file=\"obs://llddy/LUKE_mindspore/data/dev_data.npy\", dst_local_dir=\"./dev_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "324e9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.build_dataset import build_dataset\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f8d1f",
   "metadata": {},
   "source": [
    "# 创建mindrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ccec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSRStatus.SUCCESS"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES_FILE = \"./data/dev_data.npy\"\n",
    "features = np.load(FEATURES_FILE)\n",
    "list_dict = []\n",
    "for item in features:\n",
    "    dict_temp = json.loads(item)\n",
    "    list_dict.append(dict_temp)\n",
    "SQUAD_MINDRECORD_FILE = \"./data/dev_features.mindrecord\"\n",
    "pad = lambda a,i : a[0:i] if len(a) > i else a + [0] * (i-len(a))\n",
    "pad_entity = lambda a,i : a[0:i] if len(a) > i else np.append(a,[-1] * (i-len(a)))\n",
    "\n",
    "for slist in list_dict:\n",
    "    slist[\"entity_attention_mask\"] = pad(slist[\"entity_attention_mask\"], 24)\n",
    "    slist[\"entity_ids\"] = pad(slist[\"entity_attention_mask\"], 24)\n",
    "    slist[\"entity_segment_ids\"] = pad(slist[\"entity_segment_ids\"], 24)\n",
    "    \n",
    "    slist[\"word_ids\"] = pad(slist[\"word_ids\"], 256)\n",
    "    slist[\"word_segment_ids\"] = pad(slist[\"word_segment_ids\"], 256)\n",
    "    slist[\"word_attention_mask\"] = pad(slist[\"word_attention_mask\"], 256)\n",
    "    slist[\"entity_position_ids\"] = np.array(slist[\"entity_position_ids\"]).flatten()\n",
    "    slist[\"entity_position_ids\"] = pad_entity(slist[\"entity_position_ids\"], 256)\n",
    "\n",
    "\n",
    "if os.path.exists(SQUAD_MINDRECORD_FILE):\n",
    "    os.remove(SQUAD_MINDRECORD_FILE)\n",
    "    os.remove(SQUAD_MINDRECORD_FILE + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=SQUAD_MINDRECORD_FILE, shard_num=1)\n",
    "\n",
    "data_schema = {\n",
    "    \"unique_id\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_position_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    #\"start_positions\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    #\"end_positions\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "}\n",
    "writer.add_schema(data_schema, \"it is a preprocessed squad dataset\")\n",
    "\n",
    "data = []\n",
    "i = 0\n",
    "for item in list_dict:\n",
    "    i += 1\n",
    "    sample = {\n",
    "        \"unique_id\": np.array(item[\"unique_id\"], dtype=np.int32),\n",
    "        \"word_ids\": np.array(item[\"word_ids\"], dtype=np.int32),\n",
    "        \"word_segment_ids\": np.array(item[\"word_segment_ids\"], dtype=np.int32),\n",
    "        \"word_attention_mask\": np.array(item[\"word_attention_mask\"], dtype=np.int32),\n",
    "        \"entity_ids\": np.array(item[\"entity_ids\"], dtype=np.int32),\n",
    "        \"entity_position_ids\": np.array(item[\"entity_position_ids\"], dtype=np.int32),\n",
    "        \"entity_segment_ids\": np.array(item[\"entity_segment_ids\"], dtype=np.int32),\n",
    "        \"entity_attention_mask\": np.array(item[\"entity_attention_mask\"], dtype=np.int32),\n",
    "        #\"start_positions\": np.array(item[\"start_positions\"], dtype=np.int32),\n",
    "        #\"end_positions\": np.array(item[\"end_positions\"], dtype=np.int32),\n",
    "    }\n",
    "\n",
    "    data.append(sample)\n",
    "    #print(sample)\n",
    "    if i % 10 == 0:\n",
    "        writer.write_raw_data(data)\n",
    "        data = []\n",
    "\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3819700",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features = np.array(list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d95683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10779 samples\n"
     ]
    }
   ],
   "source": [
    "#SQUAD_MINDRECORD_FILE = \"./data/dev_features.mindrecord\"\n",
    "data_set = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator():\n",
    "    #print(item)\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f70b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_attention_mask': Tensor(shape=[24, 24], dtype=Int32, value=\n",
       " [[0, 0, 0 ... 0, 0, 0],\n",
       "  [1, 1, 0 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 0, 0 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 0 ... 0, 0, 0]]),\n",
       " 'entity_ids': Tensor(shape=[24, 24], dtype=Int32, value=\n",
       " [[0, 0, 0 ... 0, 0, 0],\n",
       "  [1, 1, 0 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 0, 0 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 0 ... 0, 0, 0]]),\n",
       " 'entity_position_ids': Tensor(shape=[24, 256], dtype=Int32, value=\n",
       " [[ -1,  -1,  -1 ...  -1,  -1,  -1],\n",
       "  [ 32,  -1,  -1 ...  -1,  -1,  -1],\n",
       "  [  8,   9,  -1 ...  -1,  -1,  -1],\n",
       "  ...\n",
       "  [121,  -1,  -1 ...  -1,  -1,  -1],\n",
       "  [  1,  -1,  -1 ...  -1,  -1,  -1],\n",
       "  [ 27,  28,  29 ...  -1,  -1,  -1]]),\n",
       " 'entity_segment_ids': Tensor(shape=[24, 24], dtype=Int32, value=\n",
       " [[0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  ...\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0]]),\n",
       " 'unique_id': Tensor(shape=[24, 1], dtype=Int32, value=\n",
       " [[1000006804],\n",
       "  [1000007240],\n",
       "  [1000003967],\n",
       "  ...\n",
       "  [1000010394],\n",
       "  [1000009919],\n",
       "  [1000006805]]),\n",
       " 'word_attention_mask': Tensor(shape=[24, 256], dtype=Int32, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " 'word_ids': Tensor(shape=[24, 256], dtype=Int32, value=\n",
       " [[    0,  2264,   761 ...     0,     0,     0],\n",
       "  [    0,  2264,   247 ...     0,     0,     0],\n",
       "  [    0,  2264,   222 ...     0,     0,     0],\n",
       "  ...\n",
       "  [    0, 32251,   194 ...     0,     0,     0],\n",
       "  [    0, 21336,    34 ...     0,     0,     0],\n",
       "  [    0,  2264,    16 ...     0,     0,     0]]),\n",
       " 'word_segment_ids': Tensor(shape=[24, 256], dtype=Int32, value=\n",
       " [[0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  ...\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_set.batch(24)\n",
    "data_sample = next(data_set.create_dict_iterator())\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d09bd5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ce8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readingcomprehension.models.luke import LukeForReadingComprehension, LukeEntityAwareAttentionModel\n",
    "import mindspore.common.dtype as mstype\n",
    "from model.bert_model import BertConfig\n",
    "from mindspore import context\n",
    "from model.luke import LukeModel, EntityAwareEncoder\n",
    "from mindspore import Tensor, context\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "from model.bert_model import BertOutput\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore.ops import composite as C\n",
    "import mindspore\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.train.model import Model\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c282",
   "metadata": {},
   "source": [
    "## 简单测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b4a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BertConfig()\n",
    "# Luke_model = LukeForReadingComprehension(config)\n",
    "# model = Model(Luke_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520a6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits = model.predict(word_ids,word_segment_ids,word_attention_mask,entity_ids,entity_position_ids,entity_segment_ids,entity_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61683528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[24, 256], dtype=Float32, value=\n",
       " [[ 2.92592585e-01,  2.70484447e-01,  2.65146255e-01 ...  2.63161600e-01,  2.59273797e-01,  2.95242786e-01],\n",
       "  [ 3.18681479e-01,  2.97804475e-01,  2.70562023e-01 ...  2.99310863e-01,  2.90683925e-01,  3.07460636e-01],\n",
       "  [ 2.56083131e-01,  2.35625535e-01,  2.13921010e-01 ...  2.38565832e-01,  2.26389676e-01,  2.43567199e-01],\n",
       "  ...\n",
       "  [ 2.66339034e-01,  2.45516524e-01,  2.45532036e-01 ...  2.48828977e-01,  2.38703132e-01,  2.56241202e-01],\n",
       "  [ 3.29715371e-01,  3.08946311e-01,  3.06750625e-01 ...  3.09931695e-01,  3.09648693e-01,  2.97910243e-01],\n",
       "  [ 2.54786551e-01,  2.23702818e-01,  2.21459359e-01 ...  2.36167192e-01,  2.27386415e-01,  2.43492305e-01]]),\n",
       " Tensor(shape=[24, 256], dtype=Float32, value=\n",
       " [[-4.45168614e-01, -4.17687595e-01, -4.18160260e-01 ... -4.17240530e-01, -4.13319558e-01, -4.30681825e-01],\n",
       "  [-4.23104525e-01, -3.96651030e-01, -3.95476073e-01 ... -3.96843135e-01, -3.93367976e-01, -4.17906284e-01],\n",
       "  [-4.35993910e-01, -4.09789205e-01, -4.17405427e-01 ... -4.08947974e-01, -4.02795613e-01, -4.31634933e-01],\n",
       "  ...\n",
       "  [-4.21840847e-01, -3.94027650e-01, -3.76842052e-01 ... -3.94310832e-01, -3.91166568e-01, -4.16604161e-01],\n",
       "  [-4.15894836e-01, -3.93932432e-01, -3.69493723e-01 ... -3.72763693e-01, -3.88462126e-01, -4.17109966e-01],\n",
       "  [-4.32556689e-01, -4.23182189e-01, -4.05571908e-01 ... -4.07639861e-01, -4.03096080e-01, -4.28082407e-01]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2952056",
   "metadata": {},
   "source": [
    "# do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bebf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://100.125.0.87:32021/repository/pypi/simple\n",
      "Requirement already satisfied: tqdm in /home/ma-user/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages (4.62.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bcef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.model import Model\n",
    "import collections\n",
    "def do_eval(dataset=None, load_checkpoint_path=\"\", eval_batch_size=1):\n",
    "    config = BertConfig()\n",
    "    Luke_model = LukeForReadingComprehension(config)\n",
    "    Luke_model.set_train(False)\n",
    "    model = Model(Luke_model)\n",
    "    param_dict = load_checkpoint(load_checkpoint_path)\n",
    "    load_param_into_net(Luke_model, param_dict)\n",
    "    output = []\n",
    "    #model = Model(Luke_model)\n",
    "    \n",
    "    RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "    columns_list = [\"unique_id\", \"word_ids\", \"word_segment_ids\", \"word_attention_mask\", \"entity_ids\", \"entity_position_ids\", \"entity_segment_ids\", \"entity_attention_mask\"]\n",
    "    data_set = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "    data_set = data_set.batch(eval_batch_size,drop_remainder=True)\n",
    "    for data in tqdm(data_set.create_dict_iterator(num_epochs=1)):\n",
    "        input_data = []\n",
    "        for i in columns_list:\n",
    "            input_data.append(data[i])\n",
    "\n",
    "        unique_id, word_ids,word_segment_ids,word_attention_mask,entity_ids,entity_position_ids,entity_segment_ids,entity_attention_mask = input_data\n",
    "        #print(unique_id)\n",
    "        logits = model.predict(word_ids,word_segment_ids,word_attention_mask,entity_ids,entity_position_ids,entity_segment_ids,entity_attention_mask)\n",
    "        ids = unique_id.asnumpy()\n",
    "        start = logits[0].asnumpy()\n",
    "        end = logits[1].asnumpy()\n",
    "\n",
    "        for i in range(eval_batch_size):\n",
    "            unique_id = int(ids[i])\n",
    "            start_logits = [float(x) for x in start[i].flat]\n",
    "            end_logits = [float(x) for x in end[i].flat]\n",
    "            output.append(RawResult(\n",
    "                unique_id=unique_id,\n",
    "                start_logits=start_logits,\n",
    "                end_logits=end_logits))\n",
    "        #print(word_ids.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f54dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [2:18:12, 18.47s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "outputs = do_eval(dev_dataset, load_checkpoint_path = \"./luke-large-qa.ckpt\", eval_batch_size = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c1f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('dev_outputs.npy',outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5b4d6",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e63c34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.eval_squad import read_squad_examples, write_predictions, SQuad_postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bc1ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.006307158625039419\n",
      "{\"exact_match\": 0.0, \"f1\": 0.006307158625039419}\n"
     ]
    }
   ],
   "source": [
    "output = np.load(\"./dev_outputs.npy\", allow_pickle=True)\n",
    "eval_examples = read_squad_examples(\"./dataset/dev-v1.1.json\", False)\n",
    "all_predictions = write_predictions(eval_examples, output_features, output, 20, 30, True)\n",
    "SQuad_postprocess(\"./dataset/dev-v1.1.json\", all_predictions, output_metrics=\"output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daf8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore-python3.7-aarch64",
   "language": "python",
   "name": "mindspore-python3.7-aarch64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
