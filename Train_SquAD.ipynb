{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(25711:140251066578752,MainProcess):2021-09-26-20:35:00.317.18 [mindspore/run_check/_check_version.py:181] Cuda ['10.1', '11.1'] version(need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(25711:140251066578752,MainProcess):2021-09-26-20:35:00.393.12 [mindspore/run_check/_check_version.py:181] Cuda ['10.1', '11.1'] version(need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, please refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "from dataset.build_dataset import build_dataset\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"./data/train_data.npy\"\n",
    "features = np.load(FEATURES_FILE)\n",
    "list_dict = []\n",
    "for item in features:\n",
    "    dict_temp = json.loads(item)\n",
    "    list_dict.append(dict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dict[0]['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 598,\n",
       " 2661,\n",
       " 222,\n",
       " 5,\n",
       " 9880,\n",
       " 2708,\n",
       " 2346,\n",
       " 2082,\n",
       " 11,\n",
       " 504,\n",
       " 4432,\n",
       " 11,\n",
       " 226,\n",
       " 2126,\n",
       " 10067,\n",
       " 1470,\n",
       " 116,\n",
       " 2,\n",
       " 2,\n",
       " 29474,\n",
       " 28108,\n",
       " 6,\n",
       " 5,\n",
       " 334,\n",
       " 34,\n",
       " 10,\n",
       " 4019,\n",
       " 2048,\n",
       " 4,\n",
       " 497,\n",
       " 1517,\n",
       " 5,\n",
       " 4326,\n",
       " 6919,\n",
       " 18,\n",
       " 1637,\n",
       " 31346,\n",
       " 16,\n",
       " 10,\n",
       " 9030,\n",
       " 9577,\n",
       " 9,\n",
       " 5,\n",
       " 9880,\n",
       " 2708,\n",
       " 4,\n",
       " 29261,\n",
       " 11,\n",
       " 760,\n",
       " 9,\n",
       " 5,\n",
       " 4326,\n",
       " 6919,\n",
       " 8,\n",
       " 2114,\n",
       " 24,\n",
       " 6,\n",
       " 16,\n",
       " 10,\n",
       " 7621,\n",
       " 9577,\n",
       " 9,\n",
       " 4845,\n",
       " 19,\n",
       " 3701,\n",
       " 62,\n",
       " 33161,\n",
       " 19,\n",
       " 5,\n",
       " 7875,\n",
       " 22,\n",
       " 39043,\n",
       " 1459,\n",
       " 1614,\n",
       " 1464,\n",
       " 13292,\n",
       " 4977,\n",
       " 845,\n",
       " 4130,\n",
       " 7,\n",
       " 5,\n",
       " 4326,\n",
       " 6919,\n",
       " 16,\n",
       " 5,\n",
       " 26429,\n",
       " 2426,\n",
       " 9,\n",
       " 5,\n",
       " 25095,\n",
       " 6924,\n",
       " 4,\n",
       " 29261,\n",
       " 639,\n",
       " 5,\n",
       " 32394,\n",
       " 2426,\n",
       " 16,\n",
       " 5,\n",
       " 7461,\n",
       " 26187,\n",
       " 6,\n",
       " 10,\n",
       " 19035,\n",
       " 317,\n",
       " 9,\n",
       " 9621,\n",
       " 8,\n",
       " 12456,\n",
       " 4,\n",
       " 85,\n",
       " 16,\n",
       " 10,\n",
       " 24633,\n",
       " 9,\n",
       " 5,\n",
       " 11491,\n",
       " 26187,\n",
       " 23,\n",
       " 226,\n",
       " 2126,\n",
       " 10067,\n",
       " 6,\n",
       " 1470,\n",
       " 147,\n",
       " 5,\n",
       " 9880,\n",
       " 2708,\n",
       " 2851,\n",
       " 13735,\n",
       " 352,\n",
       " 1382,\n",
       " 7,\n",
       " 6130,\n",
       " 6552,\n",
       " 625,\n",
       " 3398,\n",
       " 208,\n",
       " 22895,\n",
       " 853,\n",
       " 1827,\n",
       " 11,\n",
       " 504,\n",
       " 4432,\n",
       " 4,\n",
       " 497,\n",
       " 5,\n",
       " 253,\n",
       " 9,\n",
       " 5,\n",
       " 1049,\n",
       " 1305,\n",
       " 36,\n",
       " 463,\n",
       " 11,\n",
       " 10,\n",
       " 2228,\n",
       " 516,\n",
       " 14,\n",
       " 15230,\n",
       " 149,\n",
       " 155,\n",
       " 19638,\n",
       " 8,\n",
       " 5,\n",
       " 2610,\n",
       " 25336,\n",
       " 238,\n",
       " 16,\n",
       " 10,\n",
       " 2007,\n",
       " 6,\n",
       " 2297,\n",
       " 7326,\n",
       " 9577,\n",
       " 9,\n",
       " 2708,\n",
       " 4,\n",
       " 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict[0]['word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def _get_bucket_length(x, bts):\n",
    "    x_len = len(x)\n",
    "    for index in range(1, len(bts)):\n",
    "        if bts[index - 1] < x_len <= bts[index]:\n",
    "            return bts[index]\n",
    "    return bts[0]\n",
    "\n",
    "\n",
    "class Pad:\n",
    "    \"\"\"\n",
    "    Pads the input data samples to the largest length.\n",
    "\n",
    "    Args:\n",
    "        max_length (int): The required length of the data. If the input data less than\n",
    "            the max_length, it indicates we pad it to the max_length with the pad_val.\n",
    "        pad_val (Union[float, int]): The padding value, default 0.\n",
    "        buckets (List[int], Optional): Padding row to the length of buckets, default None.\n",
    "        pad_right (bool): The position of the PAD. If True, it indicates we\n",
    "        pad to the right side, while False indicates we pad to the left side, default True.\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, max_length: int = 0, pad_val: Union[float, int] = 0, buckets: List[int] = None,\n",
    "                 pad_right: bool = True):\n",
    "        self.pad_val = pad_val\n",
    "        self.max_length = max_length\n",
    "        self.buckets = buckets\n",
    "        self.pad_right = pad_right\n",
    "\n",
    "    def __call__(self, data: List[int]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (List[int]): The input data.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The input data which has been pad to max_length.\n",
    "\n",
    "        Examples:\n",
    "            .. code-block:: python\n",
    "            >>> pad = Pad(max_length=10)\n",
    "            >>> input_data = [1, 2, 3, 4, 5, 6]\n",
    "            >>> result = Pad(input_data)\n",
    "            >>> print(result)\n",
    "            (1,2,3,4,5,6,0,0,0,0)\n",
    "        \"\"\"\n",
    "        if self.buckets is not None:\n",
    "            self.max_length = _get_bucket_length(data, self.buckets)\n",
    "        if self.pad_right:\n",
    "            data = data + (self.max_length - len(data)) * [self.pad_val]\n",
    "        else:\n",
    "            data = (self.max_length - len(data)) * [self.pad_val] + data\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def padding(data: List[int], max_length: int, pad_val: Union[float, int] = 0, pad_right: bool = True) -> List[int]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (List[int]): The input data.\n",
    "            max_length (int): The required length of the data. If the input data less than\n",
    "                the max_length, it indicates we pad it to the max_length with the pad_val.\n",
    "            pad_val (Union[float, int]): The padding value, default 0.\n",
    "            pad_right (bool): The position of the PAD. If True, it indicates we\n",
    "            pad to the right side, while False indicates we pad to the left side, default True.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: the input data which has been pad to max_length.\n",
    "        \"\"\"\n",
    "        if pad_right:\n",
    "            data = data + (max_length - len(data)) * [pad_val]\n",
    "        else:\n",
    "            data = (max_length - len(data)) * [pad_val] + data\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90,) (38,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_25711/155279547.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"word_attention_mask\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpad_0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"word_attention_mask\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"entity_position_ids\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"entity_position_ids\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"entity_position_ids\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpad_n1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mslist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"entity_position_ids\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_25711/3319208921.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_length\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_bucket_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuckets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_right\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_length\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_val\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_length\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_val\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (90,) (38,) "
     ]
    }
   ],
   "source": [
    "SQUAD_MINDRECORD_FILE = \"./data/train_features.mindrecord\"\n",
    "pad_1= Pad(max_length=512,pad_val=1)\n",
    "pad_0= Pad(max_length=512,pad_val=0)\n",
    "pad_n1=Pad(max_length=128,pad_val=-1)\n",
    "pad_entity = lambda a,i : a[0:i] if len(a) > i else np.append(a,[-1] * (i-len(a)))\n",
    "for slist in list_dict:\n",
    "    slist[\"entity_attention_mask\"] = pad_128(slist[\"entity_attention_mask\"])\n",
    "    slist[\"entity_ids\"] = pad_128(slist[\"entity_attention_mask\"])\n",
    "    slist[\"entity_segment_ids\"] = pad_128(slist[\"entity_segment_ids\"])\n",
    "    \n",
    "    slist[\"word_ids\"] = pad_1(slist[\"word_ids\"])\n",
    "    slist[\"word_segment_ids\"] = pad_0(slist[\"word_segment_ids\"])\n",
    "    slist[\"word_attention_mask\"] = pad_0(slist[\"word_attention_mask\"])\n",
    "    slist[\"entity_position_ids\"] = np.array(slist[\"entity_position_ids\"]).flatten()\n",
    "    slist[\"entity_position_ids\"] = pad_entity(slist[\"entity_position_ids\"])\n",
    "\n",
    "\n",
    "if os.path.exists(SQUAD_MINDRECORD_FILE):\n",
    "    os.remove(SQUAD_MINDRECORD_FILE)\n",
    "    os.remove(SQUAD_MINDRECORD_FILE + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=SQUAD_MINDRECORD_FILE, shard_num=1)\n",
    "\n",
    "data_schema = {\n",
    "    \"unique_id\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_position_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"start_positions\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"end_positions\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "}\n",
    "writer.add_schema(data_schema, \"it is a preprocessed squad dataset\")\n",
    "\n",
    "data = []\n",
    "i = 0\n",
    "for item in list_dict:\n",
    "    i += 1\n",
    "    sample = {\n",
    "        \"unique_id\": np.array(item[\"unique_id\"], dtype=np.int32),\n",
    "        \"word_ids\": np.array(item[\"word_ids\"], dtype=np.int32),\n",
    "        \"word_segment_ids\": np.array(item[\"word_segment_ids\"], dtype=np.int32),\n",
    "        \"word_attention_mask\": np.array(item[\"word_attention_mask\"], dtype=np.int32),\n",
    "        \"entity_ids\": np.array(item[\"entity_ids\"], dtype=np.int32),\n",
    "        \"entity_position_ids\": np.array(item[\"entity_position_ids\"], dtype=np.int32),\n",
    "        \"entity_segment_ids\": np.array(item[\"entity_segment_ids\"], dtype=np.int32),\n",
    "        \"entity_attention_mask\": np.array(item[\"entity_attention_mask\"], dtype=np.int32),\n",
    "        \"start_positions\": np.array(item[\"start_positions\"], dtype=np.int32),\n",
    "        \"end_positions\": np.array(item[\"end_positions\"], dtype=np.int32),\n",
    "    }\n",
    "\n",
    "    data.append(sample)\n",
    "    #print(sample)\n",
    "    if i % 10 == 0:\n",
    "        writer.write_raw_data(data)\n",
    "        data = []\n",
    "print(data[0])\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features = np.array(list_dict)\n",
    "#SQUAD_MINDRECORD_FILE = \"./data/dev_features.mindrecord\"\n",
    "data_set = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator():\n",
    "    #print(item)\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.batch(1)\n",
    "data_sample = next(data_set.create_dict_iterator())\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readingcomprehension.models.luke import LukeForReadingComprehension, LukeEntityAwareAttentionModel, LukeSquadCell\n",
    "import mindspore.common.dtype as mstype\n",
    "from model.bert_model import BertConfig\n",
    "from mindspore import context\n",
    "from model.luke import LukeModel, EntityAwareEncoder\n",
    "from mindspore import Tensor, context\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "from model.bert_model import BertOutput\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore.ops import composite as C\n",
    "import mindspore\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.train.model import Model\n",
    "from tqdm import tqdm\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.model import Model\n",
    "import collections\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "context.set_context(enable_graph_kernel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_train(dataset=None, network=None, load_checkpoint_path=\"\", save_checkpoint_path=\"\", epoch_num=1):\n",
    "    \"\"\" do train \"\"\"\n",
    "    if load_checkpoint_path == \"\":\n",
    "        raise ValueError(\"Pretrain model missed, finetune task must load pretrain model!\")\n",
    "    steps_per_epoch = dataset.get_dataset_size()\n",
    "    # optimizer\n",
    "    if optimizer_cfg.optimizer == 'AdamWeightDecay':\n",
    "        lr_schedule = BertLearningRate(learning_rate=15e-6,\n",
    "                                       end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                                       warmup_steps=int(steps_per_epoch * epoch_num * 0.1),\n",
    "                                       decay_steps=steps_per_epoch * epoch_num,\n",
    "                                       power=optimizer_cfg.AdamWeightDecay.power)\n",
    "        params = network.trainable_params()\n",
    "        decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "        other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "        group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                        {'params': other_params, 'weight_decay': 0.0}]\n",
    "\n",
    "        optimizer = AdamWeightDecay(group_params, lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "    elif optimizer_cfg.optimizer == 'Lamb':\n",
    "        lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.Lamb.learning_rate,\n",
    "                                       end_learning_rate=optimizer_cfg.Lamb.end_learning_rate,\n",
    "                                       warmup_steps=int(steps_per_epoch * epoch_num * 0.1),\n",
    "                                       decay_steps=steps_per_epoch * epoch_num,\n",
    "                                       power=optimizer_cfg.Lamb.power)\n",
    "        optimizer = Lamb(network.trainable_params(), learning_rate=lr_schedule)\n",
    "    elif optimizer_cfg.optimizer == 'Momentum':\n",
    "        optimizer = Momentum(network.trainable_params(), learning_rate=optimizer_cfg.Momentum.learning_rate,\n",
    "                             momentum=optimizer_cfg.Momentum.momentum)\n",
    "    else:\n",
    "        raise Exception(\"Optimizer not supported. support: [AdamWeightDecay, Lamb, Momentum]\")\n",
    "\n",
    "    # load checkpoint into network\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=steps_per_epoch, keep_checkpoint_max=1)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"squad\",\n",
    "                                 directory=None if save_checkpoint_path == \"\" else save_checkpoint_path,\n",
    "                                 config=ckpt_config)\n",
    "    param_dict = load_checkpoint(load_checkpoint_path)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=2 ** 32, scale_factor=2, scale_window=1000)\n",
    "    netwithgrads = BertSquadCell(network, optimizer=optimizer, scale_update_cell=update_cell)\n",
    "    model = Model(netwithgrads)\n",
    "    callbacks = [TimeMonitor(dataset.get_dataset_size()), LossCallBack(dataset.get_dataset_size()), ckpoint_cb]\n",
    "    model.train(epoch_num, dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn.learning_rate_schedule import LearningRateSchedule, PolynomialDecayLR, WarmUpLR\n",
    "class BertLearningRate(LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate for Bert network.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, end_learning_rate, warmup_steps, decay_steps, power):\n",
    "        super(BertLearningRate, self).__init__()\n",
    "        self.warmup_flag = False\n",
    "        if warmup_steps > 0:\n",
    "            self.warmup_flag = True\n",
    "            self.warmup_lr = WarmUpLR(learning_rate, warmup_steps)\n",
    "        self.decay_lr = PolynomialDecayLR(learning_rate, end_learning_rate, decay_steps, power)\n",
    "        self.warmup_steps = Tensor(np.array([warmup_steps]).astype(np.float32))\n",
    "\n",
    "        self.greater = P.Greater()\n",
    "        self.one = Tensor(np.array([1.0]).astype(np.float32))\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, global_step):\n",
    "        decay_lr = self.decay_lr(global_step)\n",
    "        if self.warmup_flag:\n",
    "            is_warmup = self.cast(self.greater(self.warmup_steps, global_step), mstype.float32)\n",
    "            warmup_lr = self.warmup_lr(global_step)\n",
    "            lr = (self.one - is_warmup) * decay_lr + is_warmup * warmup_lr\n",
    "        else:\n",
    "            lr = decay_lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import Callback\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.model import Model\n",
    "import collections\n",
    "import time\n",
    "def get_ms_timestamp():\n",
    "    t = time.time()\n",
    "    return int(round(t * 1000))\n",
    "\n",
    "class LossCallBack(Callback):\n",
    "    def __init__(self, per_print_times=1,rank_ids=0):\n",
    "        super(LossCallBack,self).__init__()\n",
    "        if not isinstance(per_print_times, int) or per_print_times < 0:\n",
    "            raise ValueError(\"print_step must be int and >=0.\")\n",
    "        self._per_print_times = per_print_times\n",
    "        self.rank_id = rank_ids\n",
    "        self.time_stamp_first = get_ms_timestamp()\n",
    "        \n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"Monitor the loss in training.\"\"\"\n",
    "        global time_stamp_first\n",
    "        time_stamp_current = get_ms_timestamp()\n",
    "        cb_params = run_context.original_args()\n",
    "        print(\"time: {}, epoch: {}, step: {}, outputs are {}\".format(time_stamp_current - time_stamp_first,\n",
    "                                                                     cb_params.cur_epoch_num,\n",
    "                                                                     cb_params.cur_step_num,\n",
    "                                                                     str(cb_params.net_outputs)))\n",
    "        with open(\"./loss_{}.log\".format(self.rank_id), \"a+\") as f:\n",
    "            f.write(\"time: {}, epoch: {}, step: {}, loss: {}\".format(\n",
    "                time_stamp_current - time_stamp_first,\n",
    "                cb_params.cur_epoch_num,\n",
    "                cb_params.cur_step_num,\n",
    "                str(cb_params.net_outputs.asnumpy())))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1\n",
    "\n",
    "luke_config = BertConfig()\n",
    "LUKEModel = LukeForReadingComprehension(luke_config)\n",
    "param_dict = load_checkpoint('./luke-large-qa.ckpt')\n",
    "load_param_into_net(LUKEModel,param_dict)\n",
    "\n",
    "#lr_schedule = BertLearningRate()\n",
    "params = LUKEModel.trainable_params()\n",
    "optimizer = mindspore.nn.AdamWeightDecay(params,learning_rate=15e-6, beta1=0.9,beta2=0.98,eps=1e-06)\n",
    "\n",
    "# lr_schedule\n",
    "warmup_steps=877\n",
    "num_train_steps=14629\n",
    "\n",
    "netwithgrads = LukeSquadCell(LUKEModel,optimizer=optimizer)\n",
    "model =Model(netwithgrads)\n",
    "\n",
    "loss_monitor = LossCallBack()\n",
    "model.train(epoch,data_set,callbacks=[loss_monitor],dataset_sink_mode=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}