{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfde629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.build_dataset import build_dataset\n",
    "from readingcomprehension.models.luke import LukeForReadingComprehensionWithLoss\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947df52",
   "metadata": {},
   "source": [
    "# Squad 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"./data/json_features.npy\"\n",
    "features = np.load(FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b71640",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "for item in features:\n",
    "    dict_temp = json.loads(item)\n",
    "    list_dict.append(dict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822df36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSRStatus.SUCCESS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQUAD_MINDRECORD_FILE = \"./data/squad_features.mindrecord\"\n",
    "\n",
    "if os.path.exists(SQUAD_MINDRECORD_FILE):\n",
    "    os.remove(SQUAD_MINDRECORD_FILE)\n",
    "    os.remove(SQUAD_MINDRECORD_FILE + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=SQUAD_MINDRECORD_FILE, shard_num=1)\n",
    "\n",
    "data_schema = {\n",
    "    \"word_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_position_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"start_positions\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"end_positions\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "}\n",
    "writer.add_schema(data_schema, \"it is a preprocessed squad dataset\")\n",
    "\n",
    "data = []\n",
    "i = 0\n",
    "for item in list_dict:\n",
    "    i += 1\n",
    "    sample = {\n",
    "        \"word_ids\": np.array(item[\"word_ids\"], dtype=np.int32),\n",
    "        \"word_segment_ids\": np.array(item[\"word_segment_ids\"], dtype=np.int32),\n",
    "        \"word_attention_mask\": np.array(item[\"word_attention_mask\"], dtype=np.int32),\n",
    "        \"entity_ids\": np.array(item[\"entity_ids\"], dtype=np.int32),\n",
    "        \"entity_position_ids\": np.array(item[\"entity_position_ids\"], dtype=np.int32),\n",
    "        \"entity_segment_ids\": np.array(item[\"entity_segment_ids\"], dtype=np.int32),\n",
    "        \"entity_attention_mask\": np.array(item[\"entity_attention_mask\"], dtype=np.int32),\n",
    "        \"start_positions\": np.array(item[\"start_positions\"], dtype=np.int32),\n",
    "        \"end_positions\": np.array(item[\"end_positions\"], dtype=np.int32),\n",
    "    }\n",
    "\n",
    "    data.append(sample)\n",
    "    #print(sample)\n",
    "    if i % 10 == 0:\n",
    "        writer.write_raw_data(data)\n",
    "        data = []\n",
    "\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee34fc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 269 samples\n"
     ]
    }
   ],
   "source": [
    "data_set = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator():\n",
    "    #print(item)\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d955417",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e171d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readingcomprehension.models.luke import LukeForReadingComprehension\n",
    "import mindspore.common.dtype as mstype\n",
    "from model.bert_model import BertConfig\n",
    "from mindspore import context\n",
    "from model.luke import LukeModel, EntityAwareEncoder\n",
    "import numpy as np\n",
    "from mindspore import Tensor, context\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "from model.bert_model import BertOutput\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "import math\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069d55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "luke_net_cfg = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c398a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LukeForReadingComprehension(luke_net_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d6eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore\n",
    "x = Tensor(np.ones([20, 5, 10, 10]), mindspore.float32)\n",
    "shape1 = x.shape[1:]\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b36bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_positions': Tensor(shape=[1], dtype=Int32, value= [64]),\n",
       " 'entity_attention_mask': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_position_ids': Tensor(shape=[60], dtype=Int32, value= [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " 'entity_segment_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'start_positions': Tensor(shape=[1], dtype=Int32, value= [64]),\n",
       " 'word_attention_mask': Tensor(shape=[145], dtype=Int32, value= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1]),\n",
       " 'word_ids': Tensor(shape=[145], dtype=Int32, value= [    0,  1121,    99,    76,    21,     5, 26164,   256,     4, 32899, 24035,  5672,    23, 10579,  9038,  1550,   116,     2,     2,   133, 31674, 19675,  1116,   627, \n",
       "    879, 31104,   354, 26037,  8598, 25784,   627, 17894, 31674,   463, 37782,  1116,   627,  9119,  4308,   293,   463,  8813,    29,     4,   133, 17894, 11557,   354, \n",
       "    627,  1570,    12,  6462,   133, 47648,   448,     4,   725,   293, 24035, 44855,     6,   175, 42711,   179, 44993,     6,  5488,   354,   627, 12347, 11557,   560, \n",
       "   3138,   627, 17894, 44443,  1116, 17788,     4,   133,  9289,  1116,   627, 31674,   354,   625, 39606,  5632,   627, 44051,  1116, 12116,   119,  9799, 23866,  1409, \n",
       "  42921, 26377,  1120,  2515,  2580,     4,   713,   119,  9799,   354, 34410,   352,  6421,   281,   113, 40121,  3955, 33877,   113, 13437,  1116,  2629,  4892,  1178, \n",
       "  45853,   560,  7199,   241,   495,  4344,  5320, 22131,   463, 33877,   108, 26852,  3340, 10852,   560, 19746,   627, 13033,   337,  1990,   102, 26673,  3955,     4, \n",
       "      2]),\n",
       " 'word_segment_ids': Tensor(shape=[145], dtype=Int32, value= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = next(data_set.create_dict_iterator())\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e5ff1",
   "metadata": {},
   "source": [
    "# RobertaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25cdacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEmbeddings(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                            config.hidden_size,\n",
    "                                            padding_idx=config.pad_token_id\n",
    "                                            )\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,\n",
    "                                                  config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm([config.hidden_size],\n",
    "                                      epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        # self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        # self.register_buffer(\"position_ids\", nn.Range(config.max_position_embeddings).expand((1, -1)))\n",
    "        # self.register_buffer(\"token_type_ids\",\n",
    "        #                      ops.Zeros(self.position_ids.size(), dtype=mstype.int64),  # dtype used to torch.long\n",
    "        #                      persistent=False)\n",
    "        # End copy\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size,\n",
    "                                                padding_idx=self.padding_idx)\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids=None,\n",
    "                  token_type_ids=None,\n",
    "                  position_ids=None,\n",
    "                  inputs_embeds=None,\n",
    "                  past_key_values_length=0):\n",
    "        if position_ids is None:\n",
    "            if input_ids is not None:\n",
    "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
    "            else:\n",
    "                position_ids = create_position_ids_from_input_ids(inputs_embeds)\n",
    "        #if input_ids is not None:\n",
    "        input_shape = input_ids.shape\n",
    "        seq_length = input_shape[1]\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.Zeros(input_shape, dtype=mstype.int64)\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "    Args:\n",
    "       x: torch.Tensor x:\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    pad_id = np.array(padding_idx)\n",
    "    mask = Tensor(1 * np.array(input_ids.asnumpy() != pad_id))\n",
    "    #mask = input_ids.ne(padding_idx).int()  # 可能有问题\n",
    "    cumsum = ops.CumSum()\n",
    "    incremental_indices = (cumsum(mask, 1) + past_key_values_length) * mask\n",
    "    return incremental_indices + padding_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226cefc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 145, 768], dtype=Float32, value=\n",
       "[[[-8.96249235e-001, 1.08513725e+000, -6.46710694e-001 ... 4.27388102e-001, -2.70509213e-001, -2.73888886e-001],\n",
       "  [-9.89435136e-001, 1.30102769e-001, 2.84801412e+000 ... -1.80186534e+000, -1.49782979e+000, -1.22969711e+000],\n",
       "  [-1.79017889e+000, 1.34049964e+000, -1.52884889e+000 ... -1.19033086e+000, 1.01526451e+000, -5.69615006e-001],\n",
       "  ...\n",
       "  [-1.34727120e+000, 4.21805590e-001, 2.73645759e+000 ... -7.55560637e-001, 1.01165867e+000, -1.53356659e+000],\n",
       "  [-1.11658418e+000, 9.03310627e-002, 2.31084451e-001 ... 8.54442835e-001, 1.49357915e+000, -1.15361941e+000],\n",
       "  [-1.24042761e+000, 2.07411671e+000, 1.29438841e+000 ... -1.21696162e+000, 1.20948052e+000, -9.10845101e-002]],\n",
       " [[-8.96249235e-001, 1.08513725e+000, -6.46710694e-001 ... 4.27388102e-001, -2.70509213e-001, -2.73888886e-001],\n",
       "  [-9.89435136e-001, 1.30102769e-001, 2.84801412e+000 ... -1.80186534e+000, -1.49782979e+000, -1.22969711e+000],\n",
       "  [-1.79017889e+000, 1.34049964e+000, -1.52884889e+000 ... -1.19033086e+000, 1.01526451e+000, -5.69615006e-001],\n",
       "  ...\n",
       "  [-1.34727120e+000, 4.21805590e-001, 2.73645759e+000 ... -7.55560637e-001, 1.01165867e+000, -1.53356659e+000],\n",
       "  [-1.11658418e+000, 9.03310627e-002, 2.31084451e-001 ... 8.54442835e-001, 1.49357915e+000, -1.15361941e+000],\n",
       "  [-1.24042761e+000, 2.07411671e+000, 1.29438841e+000 ... -1.21696162e+000, 1.20948052e+000, -9.10845101e-002]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_stack = ops.Stack()\n",
    "word_ids = op_stack([data_sample[\"word_ids\"], data_sample[\"word_ids\"]])\n",
    "word_segment_ids = op_stack([data_sample[\"word_segment_ids\"], data_sample[\"word_segment_ids\"]])\n",
    "embeddings = RobertaEmbeddings(luke_net_cfg)\n",
    "word_embeddings = embeddings.construct(word_ids, word_segment_ids)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef05039",
   "metadata": {},
   "source": [
    "# EntityEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9f5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEmbeddings(nn.Cell):\n",
    "    \"\"\"entity embeddings for luke model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityEmbeddings, self).__init__()\n",
    "        self.config = config\n",
    "        #config.entity_vocab_size = 20\n",
    "        #config.entity_emb_size = config.hidden_size\n",
    "        #config.layer_norm_eps = 1e-6\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(config.entity_vocab_size, config.entity_emb_size, padding_idx=0)\n",
    "        \n",
    "        if config.entity_emb_size != config.hidden_size:\n",
    "            self.entity_embedding_dense = nn.Dense(config.entity_emb_size, config.hidden_size, has_bias=False)\n",
    "            \n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # TODO：[config.hidden_size] 和 torch有区别\n",
    "        self.layer_norm = nn.LayerNorm([config.hidden_size], epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.unsqueezee = ops.ExpandDims()\n",
    "\n",
    "    def construct(self, entity_ids, position_ids, token_type_ids=None):\n",
    "        \"\"\"EntityEmbeddings for luke\"\"\"\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.zeros_like(entity_ids)\n",
    "\n",
    "        entity_embeddings = self.entity_embeddings(entity_ids)\n",
    "        if self.config.entity_emb_size != self.config.hidden_size:\n",
    "            entity_embeddings = self.entity_embedding_dense(entity_embeddings)\n",
    "        entity_position_ids_int = clamp(position_ids)\n",
    "        entity_position_ids_int = Tensor(entity_position_ids_int.asnumpy().astype(np.int32))\n",
    "        position_embeddings = self.position_embeddings(entity_position_ids_int)\n",
    "        #position_embeddings = self.position_embeddings(position_ids)\n",
    "        position_embedding_mask = 1*self.unsqueezee((position_ids != -1), -1)\n",
    "        position_embeddings = position_embeddings * position_embedding_mask\n",
    "        position_embeddings = ops.reduce_sum(position_embeddings, -2)\n",
    "        position_embeddings = position_embeddings / clamp(ops.reduce_sum(position_embedding_mask, -2), minimum=1e-7)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = entity_embeddings + position_embeddings + token_type_embeddings\n",
    "        #embeddings = self.layer_norm(embeddings)\n",
    "        #embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def clamp(x, minimum=0.0):\n",
    "    mask = x > minimum\n",
    "    x = x * mask + minimum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c67c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       "[[[-6.91927504e-004, 1.00079114e-002, 2.76581966e-003 ... 1.19770016e-003, -8.96080025e-003, -1.13331415e-002],\n",
       "  [-6.91927504e-004, 1.00079114e-002, 2.76581966e-003 ... 1.19770016e-003, -8.96080025e-003, -1.13331415e-002]],\n",
       " [[-6.91927504e-004, 1.00079114e-002, 2.76581966e-003 ... 1.19770016e-003, -8.96080025e-003, -1.13331415e-002],\n",
       "  [-6.91927504e-004, 1.00079114e-002, 2.76581966e-003 ... 1.19770016e-003, -8.96080025e-003, -1.13331415e-002]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_EntityEmbeddings = EntityEmbeddings(luke_net_cfg)\n",
    "entity_ids = op_stack([data_sample[\"entity_ids\"],data_sample[\"entity_ids\"]])\n",
    "entity_position_ids = op_stack([data_sample[\"entity_position_ids\"],data_sample[\"entity_position_ids\"]])\n",
    "entity_segment_ids = op_stack([data_sample[\"entity_segment_ids\"],data_sample[\"entity_segment_ids\"]])\n",
    "eg_EntityEmbeddings = net_EntityEmbeddings.construct(entity_ids, entity_position_ids, entity_segment_ids)\n",
    "eg_EntityEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208ecfe",
   "metadata": {},
   "source": [
    "# attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f8ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_extended_attention_mask(word_attention_mask, entity_attention_mask):\n",
    "    attention_mask = word_attention_mask\n",
    "    if entity_attention_mask is not None:\n",
    "        op_Concat = ops.Concat(axis = 1)\n",
    "        attention_mask = op_Concat((attention_mask, entity_attention_mask))\n",
    "    unsqueezee = ops.ExpandDims()\n",
    "    extended_attention_mask = unsqueezee(unsqueezee(attention_mask, 1), 2)\n",
    "    extended_attention_mask = extended_attention_mask.astype(mstype.float32)\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "    return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d019e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 1, 1, 147], dtype=Float32, value=\n",
       "[[[[-0.00000000e+000, -0.00000000e+000, -0.00000000e+000 ... -0.00000000e+000, -1.00000000e+004, -1.00000000e+004]]],\n",
       " [[[-0.00000000e+000, -0.00000000e+000, -0.00000000e+000 ... -0.00000000e+000, -1.00000000e+004, -1.00000000e+004]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attention_mask = op_stack([data_sample[\"word_attention_mask\"],data_sample[\"word_attention_mask\"]])\n",
    "entity_attention_mask = op_stack([data_sample[\"entity_attention_mask\"],data_sample[\"entity_attention_mask\"]])\n",
    "attention_mask = _compute_extended_attention_mask(word_attention_mask, entity_attention_mask)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f27ed",
   "metadata": {},
   "source": [
    "# self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c6a54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareSelfAttention(nn.Cell):\n",
    "    \"\"\"EntityAwareSelfAttention\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.w2e_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.e2w_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.e2e_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.concat = ops.Concat(1)\n",
    "        self.concat2 = ops.Concat(2)\n",
    "        self.concat3 = ops.Concat(3)\n",
    "        self.sotfmax = ops.Softmax()\n",
    "        self.shape = ops.Shape()\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "        self.softmax = ops.Softmax(axis = -1)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = ops.shape(x)[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        out = self.reshape(x, new_x_shape)\n",
    "        out = self.transpose(out, (0, 2, 1, 3))\n",
    "        return out\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        \"\"\"EntityAwareSelfAttention construct\"\"\"\n",
    "        word_size = self.shape(word_hidden_states)[1]\n",
    "        w2w_query_layer = self.transpose_for_scores(self.query(word_hidden_states))\n",
    "        w2e_query_layer = self.transpose_for_scores(self.w2e_query(word_hidden_states))\n",
    "        e2w_query_layer = self.transpose_for_scores(self.e2w_query(entity_hidden_states))\n",
    "        e2e_query_layer = self.transpose_for_scores(self.e2e_query(entity_hidden_states))\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(self.concat([word_hidden_states, entity_hidden_states])))\n",
    "\n",
    "        w2w_key_layer = key_layer[:, :, :word_size, :]\n",
    "        e2w_key_layer = key_layer[:, :, :word_size, :]\n",
    "        w2e_key_layer = key_layer[:, :, word_size:, :]\n",
    "        e2e_key_layer = key_layer[:, :, word_size:, :]\n",
    "\n",
    "        w2w_attention_scores = ops.matmul(w2w_query_layer, ops.transpose(w2w_key_layer, (0,1, 3, 2)))\n",
    "        w2e_attention_scores = ops.matmul(w2e_query_layer, ops.transpose(w2e_key_layer, (0,1, 3, 2)))\n",
    "        e2w_attention_scores = ops.matmul(e2w_query_layer, ops.transpose(e2w_key_layer, (0,1, 3, 2)))\n",
    "        e2e_attention_scores = ops.matmul(e2e_query_layer, ops.transpose(e2e_key_layer, (0,1, 3, 2)))\n",
    "\n",
    "        word_attention_scores = self.concat3([w2w_attention_scores, w2e_attention_scores])\n",
    "        entity_attention_scores = self.concat3([e2w_attention_scores, e2e_attention_scores])\n",
    "        attention_scores = self.concat2([word_attention_scores, entity_attention_scores])\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        value_layer = self.transpose_for_scores(\n",
    "            self.value(self.concat([word_hidden_states, entity_hidden_states]))\n",
    "        )\n",
    "        context_layer = ops.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = ops.transpose(context_layer, (0, 2, 1, 3))\n",
    "        new_context_layer_shape = ops.shape(context_layer)[:-2] + (self.all_head_size,)\n",
    "        context_layer = self.reshape(context_layer, new_context_layer_shape)\n",
    "\n",
    "        return context_layer[:, :word_size, :], context_layer[:, word_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24de3776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 145, 768], dtype=Float32, value=\n",
       " [[[2.17561409e-001, -1.56858861e-002, 7.79247135e-002 ... 2.80333519e-001, 2.79967990e-002, 1.72938526e-001],\n",
       "   [2.14996785e-001, -1.71952695e-002, 7.91442618e-002 ... 2.81355500e-001, 2.57189497e-002, 1.72163039e-001],\n",
       "   [2.17163891e-001, -1.69164054e-002, 7.89279565e-002 ... 2.83393592e-001, 2.91815493e-002, 1.72117457e-001],\n",
       "   ...\n",
       "   [2.16648474e-001, -1.39814056e-002, 8.12138915e-002 ... 2.84419149e-001, 2.91797370e-002, 1.72928140e-001],\n",
       "   [2.16737911e-001, -1.59509853e-002, 7.85281211e-002 ... 2.82446623e-001, 2.72220504e-002, 1.71614423e-001],\n",
       "   [2.14882866e-001, -1.60509720e-002, 7.94505402e-002 ... 2.83801794e-001, 2.89273113e-002, 1.72072291e-001]],\n",
       "  [[2.17561409e-001, -1.56858861e-002, 7.79247135e-002 ... 2.80333519e-001, 2.79967990e-002, 1.72938526e-001],\n",
       "   [2.14996785e-001, -1.71952695e-002, 7.91442618e-002 ... 2.81355500e-001, 2.57189497e-002, 1.72163039e-001],\n",
       "   [2.17163891e-001, -1.69164054e-002, 7.89279565e-002 ... 2.83393592e-001, 2.91815493e-002, 1.72117457e-001],\n",
       "   ...\n",
       "   [2.16648474e-001, -1.39814056e-002, 8.12138915e-002 ... 2.84419149e-001, 2.91797370e-002, 1.72928140e-001],\n",
       "   [2.16737911e-001, -1.59509853e-002, 7.85281211e-002 ... 2.82446623e-001, 2.72220504e-002, 1.71614423e-001],\n",
       "   [2.14882866e-001, -1.60509720e-002, 7.94505402e-002 ... 2.83801794e-001, 2.89273113e-002, 1.72072291e-001]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[2.16891021e-001, -1.62892453e-002, 7.96158165e-002 ... 2.82803863e-001, 2.68404521e-002, 1.71653643e-001],\n",
       "   [2.16891021e-001, -1.62892453e-002, 7.96158165e-002 ... 2.82803863e-001, 2.68404521e-002, 1.71653643e-001]],\n",
       "  [[2.16891021e-001, -1.62892453e-002, 7.96158165e-002 ... 2.82803863e-001, 2.68404521e-002, 1.71653643e-001],\n",
       "   [2.16891021e-001, -1.62892453e-002, 7.96158165e-002 ... 2.82803863e-001, 2.68404521e-002, 1.71653643e-001]]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = luke_net_cfg\n",
    "self_attention = EntityAwareSelfAttention(config)\n",
    "self_attention.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996eaab0",
   "metadata": {},
   "source": [
    "# EntityAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6b12b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply a linear computation to hidden status and a residual computation to input.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Input channels.\n",
    "        out_channels (int): Output channels.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 initializer_range=0.02,\n",
    "                 dropout_prob=0.1,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Dense(in_channels, out_channels,\n",
    "                              weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.add = P.Add()\n",
    "        self.layernorm = nn.LayerNorm((out_channels,)).to_float(compute_type)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, hidden_status, input_tensor):\n",
    "        output = self.dense(hidden_status)\n",
    "        output = self.dropout(output)\n",
    "        output = self.add(input_tensor, output)\n",
    "        output = self.layernorm(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c764bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.ops import operations as P\n",
    "class BertSelfOutput(nn.Cell):\n",
    "    def __init__(self, config, compute_type=mstype.float32):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size,\n",
    "                             weight_init=TruncatedNormal(config.initializer_range)).to_float(compute_type)\n",
    "        self.LayerNorm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "        self.add = P.Add()\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.add(input_tensor, hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e084813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(config.hidden_dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "829030d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareAttention(nn.Cell):\n",
    "    \"\"\"EntityAwareAttention\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareAttention, self).__init__()\n",
    "        self.self_attention = EntityAwareSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        word_self_output, entity_self_output = self.self_attention.construct(word_hidden_states, entity_hidden_states, attention_mask)\n",
    "        hidden_states = self.concat([word_hidden_states, entity_hidden_states])\n",
    "        self_output = self.concat([word_self_output, entity_self_output])\n",
    "        out = self.output.construct(hidden_states, self_output)\n",
    "        out1 = out[:, : ops.shape(word_hidden_states)[1], :]\n",
    "        out2 = out[:, ops.shape(word_hidden_states)[1]:, :]\n",
    "        #return output[:, : ops.shape(word_hidden_states)[1], :], output[:, ops.shape(word_hidden_states)[1]:, :]\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6c2506a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 768)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AwareAttention = EntityAwareAttention(config)\n",
    "out1, out2 = AwareAttention.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6a9cd",
   "metadata": {},
   "source": [
    "# EntityAwareLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae3ed9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareLayer(nn.Cell):\n",
    "    \"\"\"EntityAwareLayer\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareLayer, self).__init__()\n",
    "\n",
    "        self.attention = EntityAwareAttention(config)\n",
    "        self.intermediate = nn.Dense(config.hidden_size, \n",
    "                                     config.intermediate_size,\n",
    "                                     activation=config.hidden_act,\n",
    "                                     weight_init=TruncatedNormal(config.initializer_range)).to_float(mstype.float32)\n",
    "        self.output = BertOutput(config.intermediate_size, config.hidden_size)\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        word_attention_output, entity_attention_output = self.attention.construct(\n",
    "            word_hidden_states, entity_hidden_states, attention_mask\n",
    "        )\n",
    "        attention_output = self.concat([word_attention_output, entity_attention_output])\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output.construct(intermediate_output, attention_output)\n",
    "\n",
    "        return layer_output[:, : ops.shape(word_hidden_states)[1], :], \\\n",
    "               layer_output[:, ops.shape(word_hidden_states)[1]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "951011bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 768)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EntityAwareLayer__ = EntityAwareLayer(config)\n",
    "out1, out2 = EntityAwareLayer__.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf91d0",
   "metadata": {},
   "source": [
    "# EntityAwareEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a028aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareEncoder(nn.Cell):\n",
    "    \"\"\"EntityAwareEncoder\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareEncoder, self).__init__()\n",
    "        #self.layer = EntityAwareLayer(config)\n",
    "        self.layer = nn.CellList([EntityAwareLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        for layer_module in self.layer:\n",
    "            word_hidden_states, entity_hidden_states = layer_module.construct(\n",
    "                word_hidden_states, entity_hidden_states, attention_mask\n",
    "            )\n",
    "        return word_hidden_states, entity_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c524f5e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 145, 768], dtype=Float32, value=\n",
       " [[[1.33237398e+000, 1.23425829e+000, -4.00682807e-001 ... -1.02727389e+000, -1.83848345e+000, -8.61697048e-002],\n",
       "   [1.89218199e+000, 9.40335453e-001, 1.62200749e-001 ... -5.60925782e-001, -1.74945402e+000, -6.26039922e-001],\n",
       "   [1.09759331e+000, 8.14009607e-001, 5.76156974e-001 ... -5.11911333e-001, -1.72314119e+000, -5.67311347e-001],\n",
       "   ...\n",
       "   [1.81955385e+000, 7.12169647e-001, -3.68020773e-001 ... -7.43022501e-001, -1.67423809e+000, -6.84729159e-001],\n",
       "   [1.60875440e+000, 8.65224123e-001, -7.84036219e-002 ... -7.59143293e-001, -1.78093600e+000, -4.34029877e-001],\n",
       "   [1.83177674e+000, 9.01558518e-001, -6.65654168e-002 ... -9.49517608e-001, -1.49044406e+000, -4.44699734e-001]],\n",
       "  [[1.33237398e+000, 1.23425829e+000, -4.00682807e-001 ... -1.02727389e+000, -1.83848345e+000, -8.61697048e-002],\n",
       "   [1.89218199e+000, 9.40335453e-001, 1.62200749e-001 ... -5.60925782e-001, -1.74945402e+000, -6.26039922e-001],\n",
       "   [1.09759331e+000, 8.14009607e-001, 5.76156974e-001 ... -5.11911333e-001, -1.72314119e+000, -5.67311347e-001],\n",
       "   ...\n",
       "   [1.81955385e+000, 7.12169647e-001, -3.68020773e-001 ... -7.43022501e-001, -1.67423809e+000, -6.84729159e-001],\n",
       "   [1.60875440e+000, 8.65224123e-001, -7.84036219e-002 ... -7.59143293e-001, -1.78093600e+000, -4.34029877e-001],\n",
       "   [1.83177674e+000, 9.01558518e-001, -6.65654168e-002 ... -9.49517608e-001, -1.49044406e+000, -4.44699734e-001]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[1.35666215e+000, 6.71176374e-001, -1.03761494e-001 ... -7.57069886e-001, -1.40431929e+000, -4.66343045e-001],\n",
       "   [1.35666215e+000, 6.71176374e-001, -1.03761494e-001 ... -7.57069886e-001, -1.40431929e+000, -4.66343045e-001]],\n",
       "  [[1.35666215e+000, 6.71176374e-001, -1.03761494e-001 ... -7.57069886e-001, -1.40431929e+000, -4.66343045e-001],\n",
       "   [1.35666215e+000, 6.71176374e-001, -1.03761494e-001 ... -7.57069886e-001, -1.40431929e+000, -4.66343045e-001]]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EntityAwareEncoder__ = EntityAwareEncoder(config)\n",
    "EntityAwareEncoder__.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d667532",
   "metadata": {},
   "source": [
    "# LukeEntityAwareAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ccbaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LukeEntityAwareAttentionModel(LukeModel):\n",
    "    \"\"\"LukeEntityAwareAttentionModel\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LukeEntityAwareAttentionModel, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.encoder = EntityAwareEncoder(config)\n",
    "\n",
    "    def construct(self, word_ids, word_segment_ids, word_attention_mask, entity_ids,\n",
    "                  entity_position_ids, entity_segment_ids, entity_attention_mask):\n",
    "        word_embeddings = self.embeddings.construct(word_ids, word_segment_ids)\n",
    "        entity_embeddings = self.entity_embeddings.construct(entity_ids, entity_position_ids, entity_segment_ids)\n",
    "        attention_mask = self._compute_extended_attention_mask(word_attention_mask, entity_attention_mask)\n",
    "\n",
    "        return self.encoder.construct(word_embeddings, entity_embeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "174a4b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_positions': Tensor(shape=[1], dtype=Int32, value= [64]),\n",
       " 'entity_attention_mask': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_position_ids': Tensor(shape=[60], dtype=Int32, value= [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " 'entity_segment_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'start_positions': Tensor(shape=[1], dtype=Int32, value= [64]),\n",
       " 'word_attention_mask': Tensor(shape=[145], dtype=Int32, value= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1]),\n",
       " 'word_ids': Tensor(shape=[145], dtype=Int32, value= [    0,  1121,    99,    76,    21,     5, 26164,   256,     4, 32899, 24035,  5672,    23, 10579,  9038,  1550,   116,     2,     2,   133, 31674, 19675,  1116,   627, \n",
       "    879, 31104,   354, 26037,  8598, 25784,   627, 17894, 31674,   463, 37782,  1116,   627,  9119,  4308,   293,   463,  8813,    29,     4,   133, 17894, 11557,   354, \n",
       "    627,  1570,    12,  6462,   133, 47648,   448,     4,   725,   293, 24035, 44855,     6,   175, 42711,   179, 44993,     6,  5488,   354,   627, 12347, 11557,   560, \n",
       "   3138,   627, 17894, 44443,  1116, 17788,     4,   133,  9289,  1116,   627, 31674,   354,   625, 39606,  5632,   627, 44051,  1116, 12116,   119,  9799, 23866,  1409, \n",
       "  42921, 26377,  1120,  2515,  2580,     4,   713,   119,  9799,   354, 34410,   352,  6421,   281,   113, 40121,  3955, 33877,   113, 13437,  1116,  2629,  4892,  1178, \n",
       "  45853,   560,  7199,   241,   495,  4344,  5320, 22131,   463, 33877,   108, 26852,  3340, 10852,   560, 19746,   627, 13033,   337,  1990,   102, 26673,  3955,     4, \n",
       "      2]),\n",
       " 'word_segment_ids': Tensor(shape=[145], dtype=Int32, value= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0bad1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_stack = ops.Stack()\n",
    "word_ids = op_stack([data_sample[\"word_ids\"], data_sample[\"word_ids\"]])\n",
    "word_segment_ids = op_stack([data_sample[\"word_segment_ids\"], data_sample[\"word_segment_ids\"]])\n",
    "word_attention_mask = op_stack([data_sample[\"word_attention_mask\"], data_sample[\"word_attention_mask\"]])\n",
    "entity_ids = op_stack([data_sample[\"entity_ids\"], data_sample[\"entity_ids\"]])\n",
    "entity_position_ids = op_stack([data_sample[\"entity_position_ids\"], data_sample[\"entity_position_ids\"]])\n",
    "entity_segment_ids = op_stack([data_sample[\"entity_segment_ids\"], data_sample[\"entity_segment_ids\"]])\n",
    "entity_attention_mask = op_stack([data_sample[\"entity_attention_mask\"], data_sample[\"entity_attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "493c6003",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 145, 768], dtype=Float32, value=\n",
       " [[[1.32461655e+000, 2.25216866e+000, 6.66976750e-001 ... -1.32662579e-001, -6.77926958e-001, -1.71238625e+000],\n",
       "   [9.78211164e-001, 1.53429389e+000, 3.03713500e-001 ... -1.17052667e-001, -1.19870579e+000, -1.11417317e+000],\n",
       "   [1.15689135e+000, 2.31475925e+000, 1.01121497e+000 ... 1.26311705e-001, -9.07421768e-001, -1.14169526e+000],\n",
       "   ...\n",
       "   [7.96258330e-001, 2.29012394e+000, 2.40670741e-001 ... -2.23583892e-001, -7.55770147e-001, -1.53572440e+000],\n",
       "   [8.36888134e-001, 1.83756924e+000, 8.01898897e-001 ... 7.48744383e-002, -8.31423879e-001, -1.00028861e+000],\n",
       "   [9.83971775e-001, 2.18472505e+000, 8.23230803e-001 ... -1.76605701e-001, -3.52684319e-001, -1.13514268e+000]],\n",
       "  [[1.32461655e+000, 2.25216866e+000, 6.66976750e-001 ... -1.32662579e-001, -6.77926958e-001, -1.71238625e+000],\n",
       "   [9.78211164e-001, 1.53429389e+000, 3.03713500e-001 ... -1.17052667e-001, -1.19870579e+000, -1.11417317e+000],\n",
       "   [1.15689135e+000, 2.31475925e+000, 1.01121497e+000 ... 1.26311705e-001, -9.07421768e-001, -1.14169526e+000],\n",
       "   ...\n",
       "   [7.96258330e-001, 2.29012394e+000, 2.40670741e-001 ... -2.23583892e-001, -7.55770147e-001, -1.53572440e+000],\n",
       "   [8.36888134e-001, 1.83756924e+000, 8.01898897e-001 ... 7.48744383e-002, -8.31423879e-001, -1.00028861e+000],\n",
       "   [9.83971775e-001, 2.18472505e+000, 8.23230803e-001 ... -1.76605701e-001, -3.52684319e-001, -1.13514268e+000]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[1.02263021e+000, 2.33524346e+000, 3.91031981e-001 ... -1.19930580e-001, -7.98104644e-001, -1.29697180e+000],\n",
       "   [1.02263021e+000, 2.33524346e+000, 3.91031981e-001 ... -1.19930580e-001, -7.98104644e-001, -1.29697180e+000]],\n",
       "  [[1.02263021e+000, 2.33524346e+000, 3.91031981e-001 ... -1.19930580e-001, -7.98104644e-001, -1.29697180e+000],\n",
       "   [1.02263021e+000, 2.33524346e+000, 3.91031981e-001 ... -1.19930580e-001, -7.98104644e-001, -1.29697180e+000]]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LukeEntityAwareAttentionModel__ = LukeEntityAwareAttentionModel(config)\n",
    "LukeEntityAwareAttentionModel__.construct(word_ids,\n",
    "                                          word_segment_ids,\n",
    "                                          word_attention_mask,\n",
    "                                          entity_ids,\n",
    "                                          entity_position_ids,\n",
    "                                          entity_segment_ids,\n",
    "                                          entity_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73663047",
   "metadata": {},
   "source": [
    "# LukeForReadingComprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cddef7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 145)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50fae1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops.shape(word_ids)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "058d58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LukeForReadingComprehension(LukeEntityAwareAttentionModel):\n",
    "    \"\"\"Luke for reading comprehension task\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LukeForReadingComprehension, self).__init__(config)\n",
    "        self.LukeEntityAwareAttentionModel = super(LukeForReadingComprehension, self)\n",
    "        self.qa_outputs = nn.Dense(self.config.hidden_size, 2)\n",
    "        self.split = ops.Split(-1, 2)\n",
    "        self.squeeze = ops.Squeeze(-1)\n",
    "\n",
    "    def construct(\n",
    "            self,\n",
    "            word_ids,\n",
    "            word_segment_ids,\n",
    "            word_attention_mask,\n",
    "            entity_ids,\n",
    "            entity_position_ids,\n",
    "            entity_segment_ids,\n",
    "            entity_attention_mask,\n",
    "    ):\n",
    "        \"\"\"LukeForReadingComprehension construct\"\"\"\n",
    "        encoder_outputs = self.LukeEntityAwareAttentionModel.construct(\n",
    "            word_ids,\n",
    "            word_segment_ids,\n",
    "            word_attention_mask,\n",
    "            entity_ids,\n",
    "            entity_position_ids,\n",
    "            entity_segment_ids,\n",
    "            entity_attention_mask,\n",
    "        )\n",
    "\n",
    "        word_hidden_states = encoder_outputs[0][:, : ops.shape(word_ids)[1], :]\n",
    "        logits = self.qa_outputs(word_hidden_states)\n",
    "        start_logits, end_logits = self.split(logits)\n",
    "        start_logits = self.squeeze(start_logits)\n",
    "        end_logits = self.squeeze(end_logits)\n",
    "        \n",
    "#                 if start_positions is not None and end_positions is not None:\n",
    "#             if len(start_positions.size()) > 1:\n",
    "#                 start_positions = start_positions.squeeze(-1)\n",
    "#             if len(end_positions.size()) > 1:\n",
    "#                 end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "#             ignored_index = start_logits.size(1)\n",
    "#             start_positions.clamp_(0, ignored_index)\n",
    "#             end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "#             loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "#             start_loss = loss_fct(start_logits, start_positions)\n",
    "#             end_loss = loss_fct(end_logits, end_positions)\n",
    "#             total_loss = (start_loss + end_loss) / 2\n",
    "#             outputs = (total_loss,)\n",
    "#         else:\n",
    "#             outputs = tuple()\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84f23630",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super() argument 1 must be type, not LukeForReadingComprehension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/464132910.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                       \u001b[0mentity_position_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                       \u001b[0mentity_segment_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                       entity_attention_mask)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/950801539.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self, word_ids, word_segment_ids, word_attention_mask, entity_ids, entity_position_ids, entity_segment_ids, entity_attention_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m     ):\n\u001b[0;32m     21\u001b[0m         \u001b[1;34m\"\"\"LukeForReadingComprehension construct\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         encoder_outputs = super(LukeForReadingComprehension, self).construct(\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mword_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mword_segment_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super() argument 1 must be type, not LukeForReadingComprehension"
     ]
    }
   ],
   "source": [
    "LukeForReadingComprehension = LukeForReadingComprehension(config)\n",
    "LukeForReadingComprehension.construct(word_ids,\n",
    "                                      word_segment_ids,\n",
    "                                      word_attention_mask,\n",
    "                                      entity_ids,\n",
    "                                      entity_position_ids,\n",
    "                                      entity_segment_ids,\n",
    "                                      entity_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124749d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
