{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfde629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.build_dataset import build_dataset\n",
    "from readingcomprehension.models.luke import LukeForReadingComprehensionWithLoss\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947df52",
   "metadata": {},
   "source": [
    "# Squad 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"./data/json_features.npy\"\n",
    "features = np.load(FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b71640",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "for item in features:\n",
    "    dict_temp = json.loads(item)\n",
    "    list_dict.append(dict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822df36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSRStatus.SUCCESS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQUAD_MINDRECORD_FILE = \"./data/squad_features.mindrecord\"\n",
    "\n",
    "if os.path.exists(SQUAD_MINDRECORD_FILE):\n",
    "    os.remove(SQUAD_MINDRECORD_FILE)\n",
    "    os.remove(SQUAD_MINDRECORD_FILE + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=SQUAD_MINDRECORD_FILE, shard_num=1)\n",
    "\n",
    "data_schema = {\n",
    "    \"word_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"word_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_position_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_segment_ids\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"entity_attention_mask\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"start_positions\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "    \"end_positions\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "}\n",
    "writer.add_schema(data_schema, \"it is a preprocessed squad dataset\")\n",
    "\n",
    "data = []\n",
    "i = 0\n",
    "for item in list_dict:\n",
    "    i += 1\n",
    "    sample = {\n",
    "        \"word_ids\": np.array(item[\"word_ids\"], dtype=np.int32),\n",
    "        \"word_segment_ids\": np.array(item[\"word_segment_ids\"], dtype=np.int32),\n",
    "        \"word_attention_mask\": np.array(item[\"word_attention_mask\"], dtype=np.int32),\n",
    "        \"entity_ids\": np.array(item[\"entity_ids\"], dtype=np.int32),\n",
    "        \"entity_position_ids\": np.array(item[\"entity_position_ids\"], dtype=np.int32),\n",
    "        \"entity_segment_ids\": np.array(item[\"entity_segment_ids\"], dtype=np.int32),\n",
    "        \"entity_attention_mask\": np.array(item[\"entity_attention_mask\"], dtype=np.int32),\n",
    "        \"start_positions\": np.array(item[\"start_positions\"], dtype=np.int32),\n",
    "        \"end_positions\": np.array(item[\"end_positions\"], dtype=np.int32),\n",
    "    }\n",
    "\n",
    "    data.append(sample)\n",
    "    #print(sample)\n",
    "    if i % 10 == 0:\n",
    "        writer.write_raw_data(data)\n",
    "        data = []\n",
    "\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee34fc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 269 samples\n"
     ]
    }
   ],
   "source": [
    "data_set = ds.MindDataset(dataset_file=SQUAD_MINDRECORD_FILE)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator():\n",
    "    #print(item)\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d955417",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e171d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readingcomprehension.models.luke import LukeForReadingComprehension\n",
    "import mindspore.common.dtype as mstype\n",
    "from model.bert_model import BertConfig\n",
    "from mindspore import context\n",
    "from model.luke import LukeModel, EntityAwareEncoder\n",
    "import numpy as np\n",
    "from mindspore import Tensor, context\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "from model.bert_model import BertOutput\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "import math\n",
    "from mindspore.ops import composite as C\n",
    "import mindspore\n",
    "from mindspore.ops import operations as P\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b36bef",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_positions': Tensor(shape=[1], dtype=Int32, value= [105]),\n",
       " 'entity_attention_mask': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_position_ids': Tensor(shape=[60], dtype=Int32, value= [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " 'entity_segment_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'start_positions': Tensor(shape=[1], dtype=Int32, value= [104]),\n",
       " 'word_attention_mask': Tensor(shape=[165], dtype=Int32, value= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'word_ids': Tensor(shape=[165], dtype=Int32, value= [    0,  1121,    99,    76,   222,     5,  1461, 14405,   179,  1490,  4326,  6919,   120,  4209,   116,     2,     2,   133,  9502, 28904, 13448,  7761,   627, 22313, \n",
       "  17341,  1584, 16230,   179,  1366,  3414,     4,   133,   879, 31104,  7325, 18793, 13833,  5632,  4651, 23411,  1033,   560,  1043,   175, 14377,   877,  4321, 26302, \n",
       "   4189,   463, 33492, 38431,     4,  3908, 37782,  4651, 10443,     6,  4651,  1043, 41551, 28644,    29, 17341,  1529,  3215,   463,  4651, 23411,  1033, 16353,   560, \n",
       "   1043,   175, 14377,   877, 35369,     4,   133, 34545, 31359, 37500, 16353,  1409,   104,   368,   179,  8987, 10669,   700,   271, 36040,  7325,   241, 12459,  1409, \n",
       "    102, 42710,   254,   113, 31359, 37500,   113,   179,  1366,  3506,     6,  5488,   298, 26281,   627,   879, 31104,    18, 31302,  8475,     6,  4684, 16943,     6, \n",
       "    463,   417,  8693,   405,  9023,     4, 46229,   179,  1366,  5352,     6,   102, 31674, 44443,  7325, 33437,  1409, 40589,   574, 34344,   282,   906,     4,  2765, \n",
       "   1366,  5220,   405, 12186, 20629,   560,  3869,   212, 37292, 13728, 19364,  6025, 17341,   298, 26281,   179,   627, 31359, 37500,     4,     2]),\n",
       " 'word_segment_ids': Tensor(shape=[165], dtype=Int32, value= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = next(data_set.create_dict_iterator())\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e5ff1",
   "metadata": {},
   "source": [
    "# RobertaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25cdacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEmbeddings(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                            config.hidden_size,\n",
    "                                            padding_idx=config.pad_token_id\n",
    "                                            )\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,\n",
    "                                                  config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm([config.hidden_size],\n",
    "                                      epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        # self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        # self.register_buffer(\"position_ids\", nn.Range(config.max_position_embeddings).expand((1, -1)))\n",
    "        # self.register_buffer(\"token_type_ids\",\n",
    "        #                      ops.Zeros(self.position_ids.size(), dtype=mstype.int64),  # dtype used to torch.long\n",
    "        #                      persistent=False)\n",
    "        # End copy\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size,\n",
    "                                                padding_idx=self.padding_idx)\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids=None,\n",
    "                  token_type_ids=None,\n",
    "                  position_ids=None,\n",
    "                  inputs_embeds=None,\n",
    "                  past_key_values_length=0):\n",
    "        if position_ids is None:\n",
    "            if input_ids is not None:\n",
    "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
    "            else:\n",
    "                position_ids = create_position_ids_from_input_ids(inputs_embeds)\n",
    "        #if input_ids is not None:\n",
    "        input_shape = input_ids.shape\n",
    "        seq_length = input_shape[1]\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.Zeros(input_shape, dtype=mstype.int64)\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "    Args:\n",
    "       x: torch.Tensor x:\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    pad_id = np.array(padding_idx)\n",
    "    mask = Tensor(1 * np.array(input_ids.asnumpy() != pad_id))\n",
    "    #mask = input_ids.ne(padding_idx).int()  # 可能有问题\n",
    "    cumsum = ops.CumSum()\n",
    "    incremental_indices = (cumsum(mask, 1) + past_key_values_length) * mask\n",
    "    return incremental_indices + padding_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226cefc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 165, 768], dtype=Float32, value=\n",
       "[[[-4.41752732e-001, -4.44808930e-001, 1.44897386e-001 ... 1.53909969e+000, 7.29057074e-001, -1.06630422e-001],\n",
       "  [5.97358465e-001, 7.25465938e-002, 9.16226804e-001 ... -3.36068273e-001, -1.52510810e+000, -3.72017056e-001],\n",
       "  [-2.41841629e-001, 5.47284521e-002, 1.64154339e+000 ... 2.51613528e-001, -8.57514024e-001, -1.07438004e+000],\n",
       "  ...\n",
       "  [-1.61258146e-001, -3.13901573e-001, 3.23314726e-001 ... 1.81788588e+000, -4.56612408e-001, 2.11932003e-001],\n",
       "  [3.16331476e-001, -4.44033027e-001, -1.08146501e+000 ... 2.19673419e+000, -1.20870686e+000, 1.40268490e-001],\n",
       "  [1.21113503e+000, -1.07970285e+000, -8.74109268e-001 ... 8.76560450e-001, -3.61373484e-001, 1.10155976e+000]],\n",
       " [[-4.41752732e-001, -4.44808930e-001, 1.44897386e-001 ... 1.53909969e+000, 7.29057074e-001, -1.06630422e-001],\n",
       "  [5.97358465e-001, 7.25465938e-002, 9.16226804e-001 ... -3.36068273e-001, -1.52510810e+000, -3.72017056e-001],\n",
       "  [-2.41841629e-001, 5.47284521e-002, 1.64154339e+000 ... 2.51613528e-001, -8.57514024e-001, -1.07438004e+000],\n",
       "  ...\n",
       "  [-1.61258146e-001, -3.13901573e-001, 3.23314726e-001 ... 1.81788588e+000, -4.56612408e-001, 2.11932003e-001],\n",
       "  [3.16331476e-001, -4.44033027e-001, -1.08146501e+000 ... 2.19673419e+000, -1.20870686e+000, 1.40268490e-001],\n",
       "  [1.21113503e+000, -1.07970285e+000, -8.74109268e-001 ... 8.76560450e-001, -3.61373484e-001, 1.10155976e+000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_stack = ops.Stack()\n",
    "word_ids = op_stack([data_sample[\"word_ids\"], data_sample[\"word_ids\"]])\n",
    "word_segment_ids = op_stack([data_sample[\"word_segment_ids\"], data_sample[\"word_segment_ids\"]])\n",
    "embeddings = RobertaEmbeddings(luke_net_cfg)\n",
    "word_embeddings = embeddings.construct(word_ids, word_segment_ids)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef05039",
   "metadata": {},
   "source": [
    "# EntityEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9f5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEmbeddings(nn.Cell):\n",
    "    \"\"\"entity embeddings for luke model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityEmbeddings, self).__init__()\n",
    "        self.config = config\n",
    "        #config.entity_vocab_size = 20\n",
    "        #config.entity_emb_size = config.hidden_size\n",
    "        #config.layer_norm_eps = 1e-6\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(config.entity_vocab_size, config.entity_emb_size, padding_idx=0)\n",
    "        \n",
    "        if config.entity_emb_size != config.hidden_size:\n",
    "            self.entity_embedding_dense = nn.Dense(config.entity_emb_size, config.hidden_size, has_bias=False)\n",
    "            \n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # TODO：[config.hidden_size] 和 torch有区别\n",
    "        self.layer_norm = nn.LayerNorm([config.hidden_size], epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.unsqueezee = ops.ExpandDims()\n",
    "\n",
    "    def construct(self, entity_ids, position_ids, token_type_ids=None):\n",
    "        \"\"\"EntityEmbeddings for luke\"\"\"\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.zeros_like(entity_ids)\n",
    "\n",
    "        entity_embeddings = self.entity_embeddings(entity_ids)\n",
    "        if self.config.entity_emb_size != self.config.hidden_size:\n",
    "            entity_embeddings = self.entity_embedding_dense(entity_embeddings)\n",
    "        entity_position_ids_int = clamp(position_ids)\n",
    "        entity_position_ids_int = Tensor(entity_position_ids_int.asnumpy().astype(np.int32))\n",
    "        position_embeddings = self.position_embeddings(entity_position_ids_int)\n",
    "        #position_embeddings = self.position_embeddings(position_ids)\n",
    "        position_embedding_mask = 1*self.unsqueezee((position_ids != -1), -1)\n",
    "        position_embeddings = position_embeddings * position_embedding_mask\n",
    "        position_embeddings = ops.reduce_sum(position_embeddings, -2)\n",
    "        position_embeddings = position_embeddings / clamp(ops.reduce_sum(position_embedding_mask, -2), minimum=1e-7)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = entity_embeddings + position_embeddings + token_type_embeddings\n",
    "        #embeddings = self.layer_norm(embeddings)\n",
    "        #embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def clamp(x, minimum=0.0):\n",
    "    mask = x > minimum\n",
    "    x = x * mask + minimum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c67c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       "[[[-7.71686435e-003, -9.18955076e-003, -2.75919517e-003 ... -1.93111412e-002, -1.82587970e-002, 7.03898724e-003],\n",
       "  [-7.71686435e-003, -9.18955076e-003, -2.75919517e-003 ... -1.93111412e-002, -1.82587970e-002, 7.03898724e-003]],\n",
       " [[-7.71686435e-003, -9.18955076e-003, -2.75919517e-003 ... -1.93111412e-002, -1.82587970e-002, 7.03898724e-003],\n",
       "  [-7.71686435e-003, -9.18955076e-003, -2.75919517e-003 ... -1.93111412e-002, -1.82587970e-002, 7.03898724e-003]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_EntityEmbeddings = EntityEmbeddings(luke_net_cfg)\n",
    "entity_ids = op_stack([data_sample[\"entity_ids\"],data_sample[\"entity_ids\"]])\n",
    "entity_position_ids = op_stack([data_sample[\"entity_position_ids\"],data_sample[\"entity_position_ids\"]])\n",
    "entity_segment_ids = op_stack([data_sample[\"entity_segment_ids\"],data_sample[\"entity_segment_ids\"]])\n",
    "eg_EntityEmbeddings = net_EntityEmbeddings.construct(entity_ids, entity_position_ids, entity_segment_ids)\n",
    "eg_EntityEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208ecfe",
   "metadata": {},
   "source": [
    "# attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f8ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_extended_attention_mask(word_attention_mask, entity_attention_mask):\n",
    "    attention_mask = word_attention_mask\n",
    "    if entity_attention_mask is not None:\n",
    "        op_Concat = ops.Concat(axis = 1)\n",
    "        attention_mask = op_Concat((attention_mask, entity_attention_mask))\n",
    "    unsqueezee = ops.ExpandDims()\n",
    "    extended_attention_mask = unsqueezee(unsqueezee(attention_mask, 1), 2)\n",
    "    extended_attention_mask = extended_attention_mask.astype(mstype.float32)\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "    return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d019e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 1, 1, 167], dtype=Float32, value=\n",
       "[[[[-0.00000000e+000, -0.00000000e+000, -0.00000000e+000 ... -0.00000000e+000, -1.00000000e+004, -1.00000000e+004]]],\n",
       " [[[-0.00000000e+000, -0.00000000e+000, -0.00000000e+000 ... -0.00000000e+000, -1.00000000e+004, -1.00000000e+004]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attention_mask = op_stack([data_sample[\"word_attention_mask\"],data_sample[\"word_attention_mask\"]])\n",
    "entity_attention_mask = op_stack([data_sample[\"entity_attention_mask\"],data_sample[\"entity_attention_mask\"]])\n",
    "attention_mask = _compute_extended_attention_mask(word_attention_mask, entity_attention_mask)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61005c",
   "metadata": {},
   "source": [
    "# self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "613c0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareSelfAttention(nn.Cell):\n",
    "    \"\"\"EntityAwareSelfAttention\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.w2e_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.e2w_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "        self.e2e_query = nn.Dense(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.concat = ops.Concat(1)\n",
    "        self.concat2 = ops.Concat(2)\n",
    "        self.concat3 = ops.Concat(3)\n",
    "        self.sotfmax = ops.Softmax()\n",
    "        self.shape = ops.Shape()\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "        self.softmax = ops.Softmax(axis = -1)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = ops.shape(x)[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        out = self.reshape(x, new_x_shape)\n",
    "        out = self.transpose(out, (0, 2, 1, 3))\n",
    "        return out\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        \"\"\"EntityAwareSelfAttention construct\"\"\"\n",
    "        word_size = self.shape(word_hidden_states)[1]\n",
    "        w2w_query_layer = self.transpose_for_scores(self.query(word_hidden_states))\n",
    "        w2e_query_layer = self.transpose_for_scores(self.w2e_query(word_hidden_states))\n",
    "        e2w_query_layer = self.transpose_for_scores(self.e2w_query(entity_hidden_states))\n",
    "        e2e_query_layer = self.transpose_for_scores(self.e2e_query(entity_hidden_states))\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(self.concat([word_hidden_states, entity_hidden_states])))\n",
    "\n",
    "        w2w_key_layer = key_layer[:, :, :word_size, :]\n",
    "        e2w_key_layer = key_layer[:, :, :word_size, :]\n",
    "        w2e_key_layer = key_layer[:, :, word_size:, :]\n",
    "        e2e_key_layer = key_layer[:, :, word_size:, :]\n",
    "\n",
    "        w2w_attention_scores = ops.matmul(w2w_query_layer, ops.transpose(w2w_key_layer, (0,1, 3, 2)))\n",
    "        w2e_attention_scores = ops.matmul(w2e_query_layer, ops.transpose(w2e_key_layer, (0,1, 3, 2)))\n",
    "        e2w_attention_scores = ops.matmul(e2w_query_layer, ops.transpose(e2w_key_layer, (0,1, 3, 2)))\n",
    "        e2e_attention_scores = ops.matmul(e2e_query_layer, ops.transpose(e2e_key_layer, (0,1, 3, 2)))\n",
    "\n",
    "        word_attention_scores = self.concat3([w2w_attention_scores, w2e_attention_scores])\n",
    "        entity_attention_scores = self.concat3([e2w_attention_scores, e2e_attention_scores])\n",
    "        attention_scores = self.concat2([word_attention_scores, entity_attention_scores])\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        value_layer = self.transpose_for_scores(\n",
    "            self.value(self.concat([word_hidden_states, entity_hidden_states]))\n",
    "        )\n",
    "        context_layer = ops.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = ops.transpose(context_layer, (0, 2, 1, 3))\n",
    "        new_context_layer_shape = ops.shape(context_layer)[:-2] + (self.all_head_size,)\n",
    "        context_layer = self.reshape(context_layer, new_context_layer_shape)\n",
    "\n",
    "        return context_layer[:, :word_size, :], context_layer[:, word_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcd3132f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 165, 768], dtype=Float32, value=\n",
       " [[[2.53133923e-001, -1.60536572e-001, 8.10929239e-002 ... -1.18726261e-001, 9.20061991e-002, 1.52696553e-003],\n",
       "   [2.52583355e-001, -1.61234885e-001, 7.85283968e-002 ... -1.19530343e-001, 8.81919041e-002, 3.52382869e-003],\n",
       "   [2.51733571e-001, -1.60819903e-001, 8.06009918e-002 ... -1.18606493e-001, 8.90120715e-002, 3.00622871e-003],\n",
       "   ...\n",
       "   [2.52933174e-001, -1.61249921e-001, 7.85813108e-002 ... -1.18925765e-001, 9.12281349e-002, 2.40319758e-003],\n",
       "   [2.52820879e-001, -1.61966741e-001, 7.76201263e-002 ... -1.16530523e-001, 8.90737846e-002, 5.29047986e-003],\n",
       "   [2.54186511e-001, -1.60900801e-001, 7.78411701e-002 ... -1.19053677e-001, 9.07880366e-002, 3.46294721e-003]],\n",
       "  [[2.53133923e-001, -1.60536572e-001, 8.10929239e-002 ... -1.18726261e-001, 9.20061991e-002, 1.52696553e-003],\n",
       "   [2.52583355e-001, -1.61234885e-001, 7.85283968e-002 ... -1.19530343e-001, 8.81919041e-002, 3.52382869e-003],\n",
       "   [2.51733571e-001, -1.60819903e-001, 8.06009918e-002 ... -1.18606493e-001, 8.90120715e-002, 3.00622871e-003],\n",
       "   ...\n",
       "   [2.52933174e-001, -1.61249921e-001, 7.85813108e-002 ... -1.18925765e-001, 9.12281349e-002, 2.40319758e-003],\n",
       "   [2.52820879e-001, -1.61966741e-001, 7.76201263e-002 ... -1.16530523e-001, 8.90737846e-002, 5.29047986e-003],\n",
       "   [2.54186511e-001, -1.60900801e-001, 7.78411701e-002 ... -1.19053677e-001, 9.07880366e-002, 3.46294721e-003]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[2.53137261e-001, -1.61272660e-001, 8.02488849e-002 ... -1.19216144e-001, 9.18444842e-002, 2.91784643e-003],\n",
       "   [2.53137261e-001, -1.61272660e-001, 8.02488849e-002 ... -1.19216144e-001, 9.18444842e-002, 2.91784643e-003]],\n",
       "  [[2.53137261e-001, -1.61272660e-001, 8.02488849e-002 ... -1.19216144e-001, 9.18444842e-002, 2.91784643e-003],\n",
       "   [2.53137261e-001, -1.61272660e-001, 8.02488849e-002 ... -1.19216144e-001, 9.18444842e-002, 2.91784643e-003]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = luke_net_cfg\n",
    "self_attention = EntityAwareSelfAttention(config)\n",
    "self_attention.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93d7bb",
   "metadata": {},
   "source": [
    "# EntityAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9bd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply a linear computation to hidden status and a residual computation to input.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Input channels.\n",
    "        out_channels (int): Output channels.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 initializer_range=0.02,\n",
    "                 dropout_prob=0.1,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Dense(in_channels, out_channels,\n",
    "                              weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.add = P.Add()\n",
    "        self.layernorm = nn.LayerNorm((out_channels,)).to_float(compute_type)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, hidden_status, input_tensor):\n",
    "        output = self.dense(hidden_status)\n",
    "        output = self.dropout(output)\n",
    "        output = self.add(input_tensor, output)\n",
    "        output = self.layernorm(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57adeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertSelfOutput(nn.Cell):\n",
    "    def __init__(self, config, compute_type=mstype.float32):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size,\n",
    "                             weight_init=TruncatedNormal(config.initializer_range)).to_float(compute_type)\n",
    "        self.LayerNorm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "        self.add = P.Add()\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.add(input_tensor, hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16dd5724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(config.hidden_dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33bd99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareAttention(nn.Cell):\n",
    "    \"\"\"EntityAwareAttention\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareAttention, self).__init__()\n",
    "        self.self_attention = EntityAwareSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        word_self_output, entity_self_output = self.self_attention.construct(word_hidden_states, entity_hidden_states, attention_mask)\n",
    "        hidden_states = self.concat([word_hidden_states, entity_hidden_states])\n",
    "        self_output = self.concat([word_self_output, entity_self_output])\n",
    "        out = self.output.construct(hidden_states, self_output)\n",
    "        out1 = out[:, : ops.shape(word_hidden_states)[1], :]\n",
    "        out2 = out[:, ops.shape(word_hidden_states)[1]:, :]\n",
    "        #return output[:, : ops.shape(word_hidden_states)[1], :], output[:, ops.shape(word_hidden_states)[1]:, :]\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e5d13ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AwareAttention = EntityAwareAttention(config)\n",
    "out1, out2 = AwareAttention.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f87d6c",
   "metadata": {},
   "source": [
    "# EntityAwareLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a644b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareLayer(nn.Cell):\n",
    "    \"\"\"EntityAwareLayer\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareLayer, self).__init__()\n",
    "\n",
    "        self.attention = EntityAwareAttention(config)\n",
    "        self.intermediate = nn.Dense(config.hidden_size, \n",
    "                                     config.intermediate_size,\n",
    "                                     activation=config.hidden_act,\n",
    "                                     weight_init=TruncatedNormal(config.initializer_range)).to_float(mstype.float32)\n",
    "        self.output = BertOutput(config.intermediate_size, config.hidden_size)\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        word_attention_output, entity_attention_output = self.attention.construct(\n",
    "            word_hidden_states, entity_hidden_states, attention_mask\n",
    "        )\n",
    "        attention_output = self.concat([word_attention_output, entity_attention_output])\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output.construct(intermediate_output, attention_output)\n",
    "\n",
    "        return layer_output[:, : ops.shape(word_hidden_states)[1], :], \\\n",
    "               layer_output[:, ops.shape(word_hidden_states)[1]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bf0c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EntityAwareLayer__ = EntityAwareLayer(config)\n",
    "out1, out2 = EntityAwareLayer__.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b443406",
   "metadata": {},
   "source": [
    "# EntityAwareEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957d6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityAwareEncoder(nn.Cell):\n",
    "    \"\"\"EntityAwareEncoder\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(EntityAwareEncoder, self).__init__()\n",
    "        #self.layer = EntityAwareLayer(config)\n",
    "        self.layer = nn.CellList([EntityAwareLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def construct(self, word_hidden_states, entity_hidden_states, attention_mask):\n",
    "        for layer_module in self.layer:\n",
    "            word_hidden_states, entity_hidden_states = layer_module.construct(\n",
    "                word_hidden_states, entity_hidden_states, attention_mask\n",
    "            )\n",
    "        return word_hidden_states, entity_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e50071d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 165, 768], dtype=Float32, value=\n",
       " [[[4.15360510e-001, -3.95976394e-001, -6.53767526e-001 ... -9.21365440e-001, 6.43256068e-001, -4.25003141e-001],\n",
       "   [-9.43257008e-003, -3.21244985e-001, -5.20340323e-001 ... -1.01637685e+000, 1.72726680e-002, -1.83341965e-001],\n",
       "   [-1.15032345e-001, -5.66492736e-001, -8.94666910e-001 ... -9.95096624e-001, 3.00080210e-001, -3.62655103e-001],\n",
       "   ...\n",
       "   [2.13470399e-001, -6.99665666e-001, -1.01645136e+000 ... -1.11550820e+000, 4.22653645e-001, -1.28931506e-002],\n",
       "   [-2.62470096e-001, -1.01830363e+000, -6.96891069e-001 ... -1.49254012e+000, 3.85883212e-001, -8.99146870e-002],\n",
       "   [1.96873173e-001, -8.99896383e-001, -7.28093684e-001 ... -1.01188862e+000, 6.34365797e-001, -9.88355130e-002]],\n",
       "  [[4.15360510e-001, -3.95976394e-001, -6.53767526e-001 ... -9.21365440e-001, 6.43256068e-001, -4.25003141e-001],\n",
       "   [-9.43257008e-003, -3.21244985e-001, -5.20340323e-001 ... -1.01637685e+000, 1.72726680e-002, -1.83341965e-001],\n",
       "   [-1.15032345e-001, -5.66492736e-001, -8.94666910e-001 ... -9.95096624e-001, 3.00080210e-001, -3.62655103e-001],\n",
       "   ...\n",
       "   [2.13470399e-001, -6.99665666e-001, -1.01645136e+000 ... -1.11550820e+000, 4.22653645e-001, -1.28931506e-002],\n",
       "   [-2.62470096e-001, -1.01830363e+000, -6.96891069e-001 ... -1.49254012e+000, 3.85883212e-001, -8.99146870e-002],\n",
       "   [1.96873173e-001, -8.99896383e-001, -7.28093684e-001 ... -1.01188862e+000, 6.34365797e-001, -9.88355130e-002]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[2.99076974e-001, -1.08714473e+000, -1.07654107e+000 ... -1.63852382e+000, 1.22375560e+000, 1.90842032e-001],\n",
       "   [2.99076974e-001, -1.08714473e+000, -1.07654107e+000 ... -1.63852382e+000, 1.22375560e+000, 1.90842032e-001]],\n",
       "  [[2.99076974e-001, -1.08714473e+000, -1.07654107e+000 ... -1.63852382e+000, 1.22375560e+000, 1.90842032e-001],\n",
       "   [2.99076974e-001, -1.08714473e+000, -1.07654107e+000 ... -1.63852382e+000, 1.22375560e+000, 1.90842032e-001]]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EntityAwareEncoder__ = EntityAwareEncoder(config)\n",
    "EntityAwareEncoder__.construct(word_embeddings, eg_EntityEmbeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7920058",
   "metadata": {},
   "source": [
    "# LukeEntityAwareAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a354dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LukeEntityAwareAttentionModel(LukeModel):\n",
    "    \"\"\"LukeEntityAwareAttentionModel\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LukeEntityAwareAttentionModel, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.encoder = EntityAwareEncoder(config)\n",
    "\n",
    "    def construct(self, word_ids, word_segment_ids, word_attention_mask, entity_ids,\n",
    "                  entity_position_ids, entity_segment_ids, entity_attention_mask):\n",
    "        word_embeddings = self.embeddings.construct(word_ids, word_segment_ids)\n",
    "        entity_embeddings = self.entity_embeddings.construct(entity_ids, entity_position_ids, entity_segment_ids)\n",
    "        attention_mask = self._compute_extended_attention_mask(word_attention_mask, entity_attention_mask)\n",
    "\n",
    "        return self.encoder.construct(word_embeddings, entity_embeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd842abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_positions': Tensor(shape=[1], dtype=Int32, value= [105]),\n",
       " 'entity_attention_mask': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'entity_position_ids': Tensor(shape=[60], dtype=Int32, value= [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n",
       "  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " 'entity_segment_ids': Tensor(shape=[2], dtype=Int32, value= [0, 0]),\n",
       " 'start_positions': Tensor(shape=[1], dtype=Int32, value= [104]),\n",
       " 'word_attention_mask': Tensor(shape=[165], dtype=Int32, value= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
       "  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'word_ids': Tensor(shape=[165], dtype=Int32, value= [    0,  1121,    99,    76,   222,     5,  1461, 14405,   179,  1490,  4326,  6919,   120,  4209,   116,     2,     2,   133,  9502, 28904, 13448,  7761,   627, 22313, \n",
       "  17341,  1584, 16230,   179,  1366,  3414,     4,   133,   879, 31104,  7325, 18793, 13833,  5632,  4651, 23411,  1033,   560,  1043,   175, 14377,   877,  4321, 26302, \n",
       "   4189,   463, 33492, 38431,     4,  3908, 37782,  4651, 10443,     6,  4651,  1043, 41551, 28644,    29, 17341,  1529,  3215,   463,  4651, 23411,  1033, 16353,   560, \n",
       "   1043,   175, 14377,   877, 35369,     4,   133, 34545, 31359, 37500, 16353,  1409,   104,   368,   179,  8987, 10669,   700,   271, 36040,  7325,   241, 12459,  1409, \n",
       "    102, 42710,   254,   113, 31359, 37500,   113,   179,  1366,  3506,     6,  5488,   298, 26281,   627,   879, 31104,    18, 31302,  8475,     6,  4684, 16943,     6, \n",
       "    463,   417,  8693,   405,  9023,     4, 46229,   179,  1366,  5352,     6,   102, 31674, 44443,  7325, 33437,  1409, 40589,   574, 34344,   282,   906,     4,  2765, \n",
       "   1366,  5220,   405, 12186, 20629,   560,  3869,   212, 37292, 13728, 19364,  6025, 17341,   298, 26281,   179,   627, 31359, 37500,     4,     2]),\n",
       " 'word_segment_ids': Tensor(shape=[165], dtype=Int32, value= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
       "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df81d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_stack = ops.Stack()\n",
    "word_ids = op_stack([data_sample[\"word_ids\"], data_sample[\"word_ids\"]])\n",
    "word_segment_ids = op_stack([data_sample[\"word_segment_ids\"], data_sample[\"word_segment_ids\"]])\n",
    "word_attention_mask = op_stack([data_sample[\"word_attention_mask\"], data_sample[\"word_attention_mask\"]])\n",
    "entity_ids = op_stack([data_sample[\"entity_ids\"], data_sample[\"entity_ids\"]])\n",
    "entity_position_ids = op_stack([data_sample[\"entity_position_ids\"], data_sample[\"entity_position_ids\"]])\n",
    "entity_segment_ids = op_stack([data_sample[\"entity_segment_ids\"], data_sample[\"entity_segment_ids\"]])\n",
    "entity_attention_mask = op_stack([data_sample[\"entity_attention_mask\"], data_sample[\"entity_attention_mask\"]])\n",
    "start_positions = op_stack([data_sample[\"start_positions\"], data_sample[\"start_positions\"]])\n",
    "end_positions = op_stack([data_sample[\"end_positions\"], data_sample[\"end_positions\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7406504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 165, 768], dtype=Float32, value=\n",
       " [[[2.49526367e-001, -7.46389508e-001, 3.81608248e-001 ... -5.47386035e-002, -1.27832508e+000, 6.00578904e-001],\n",
       "   [9.78615761e-001, -5.02685308e-001, 3.67616683e-001 ... -1.78594932e-001, -9.16401267e-001, 5.65950215e-001],\n",
       "   [6.86164558e-001, -5.35537004e-001, 5.53446352e-001 ... -7.36731440e-002, -1.02447784e+000, 4.10418957e-001],\n",
       "   ...\n",
       "   [5.61239362e-001, -6.25160217e-001, 6.52561709e-002 ... 3.27793539e-001, -1.26642430e+000, 7.57280111e-001],\n",
       "   [7.46241808e-001, -1.15103817e+000, 1.91354737e-001 ... 2.21213371e-001, -8.41529369e-001, 6.64219141e-001],\n",
       "   [8.04365695e-001, -7.37675309e-001, -1.86596975e-001 ... -3.32862735e-002, -1.07152200e+000, 2.90190727e-001]],\n",
       "  [[2.49526367e-001, -7.46389508e-001, 3.81608248e-001 ... -5.47386035e-002, -1.27832508e+000, 6.00578904e-001],\n",
       "   [9.78615761e-001, -5.02685308e-001, 3.67616683e-001 ... -1.78594932e-001, -9.16401267e-001, 5.65950215e-001],\n",
       "   [6.86164558e-001, -5.35537004e-001, 5.53446352e-001 ... -7.36731440e-002, -1.02447784e+000, 4.10418957e-001],\n",
       "   ...\n",
       "   [5.61239362e-001, -6.25160217e-001, 6.52561709e-002 ... 3.27793539e-001, -1.26642430e+000, 7.57280111e-001],\n",
       "   [7.46241808e-001, -1.15103817e+000, 1.91354737e-001 ... 2.21213371e-001, -8.41529369e-001, 6.64219141e-001],\n",
       "   [8.04365695e-001, -7.37675309e-001, -1.86596975e-001 ... -3.32862735e-002, -1.07152200e+000, 2.90190727e-001]]]),\n",
       " Tensor(shape=[2, 2, 768], dtype=Float32, value=\n",
       " [[[5.93393266e-001, -6.06372118e-001, 8.58129621e-001 ... 1.39611006e-001, -9.95455742e-001, 8.53921175e-001],\n",
       "   [5.93393266e-001, -6.06372118e-001, 8.58129621e-001 ... 1.39611006e-001, -9.95455742e-001, 8.53921175e-001]],\n",
       "  [[5.93393266e-001, -6.06372118e-001, 8.58129621e-001 ... 1.39611006e-001, -9.95455742e-001, 8.53921175e-001],\n",
       "   [5.93393266e-001, -6.06372118e-001, 8.58129621e-001 ... 1.39611006e-001, -9.95455742e-001, 8.53921175e-001]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LukeEntityAwareAttentionModel__ = LukeEntityAwareAttentionModel(config)\n",
    "LukeEntityAwareAttentionModel__.construct(word_ids,\n",
    "                                          word_segment_ids,\n",
    "                                          word_attention_mask,\n",
    "                                          entity_ids,\n",
    "                                          entity_position_ids,\n",
    "                                          entity_segment_ids,\n",
    "                                          entity_attention_mask\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db446815",
   "metadata": {},
   "source": [
    "# LukeForReadingComprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28c7fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 165)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bb60328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops.shape(word_ids)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84d7fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LukeForReadingComprehension(LukeEntityAwareAttentionModel):\n",
    "    \"\"\"Luke for reading comprehension task\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LukeForReadingComprehension, self).__init__(config)\n",
    "        self.LukeEntityAwareAttentionModel = super(LukeForReadingComprehension, self)\n",
    "        self.qa_outputs = nn.Dense(self.config.hidden_size, 2)\n",
    "        self.split = ops.Split(-1, 2)\n",
    "        self.squeeze = ops.Squeeze(-1)\n",
    "        self.shape = ops.Shape()\n",
    "\n",
    "    def construct(\n",
    "            self,\n",
    "            word_ids,\n",
    "            word_segment_ids,\n",
    "            word_attention_mask,\n",
    "            entity_ids,\n",
    "            entity_position_ids,\n",
    "            entity_segment_ids,\n",
    "            entity_attention_mask,\n",
    "            start_positions=None,\n",
    "            end_positions=None,\n",
    "    ):\n",
    "        \"\"\"LukeForReadingComprehension construct\"\"\"\n",
    "        encoder_outputs = self.LukeEntityAwareAttentionModel.construct(\n",
    "            word_ids,\n",
    "            word_segment_ids,\n",
    "            word_attention_mask,\n",
    "            entity_ids,\n",
    "            entity_position_ids,\n",
    "            entity_segment_ids,\n",
    "            entity_attention_mask,\n",
    "        )\n",
    "\n",
    "        word_hidden_states = encoder_outputs[0][:, : ops.shape(word_ids)[1], :]\n",
    "        logits = self.qa_outputs(word_hidden_states)\n",
    "        start_logits, end_logits = self.split(logits)\n",
    "        start_logits = self.squeeze(start_logits)\n",
    "        end_logits = self.squeeze(end_logits)\n",
    "        \n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            if len(self.shape(start_positions)) > 1:\n",
    "                start_positions = self.squeeze(start_positions)\n",
    "            if len(self.shape(end_positions)) > 1:\n",
    "                end_positions = self.squeeze(end_positions)\n",
    "\n",
    "            ignored_index = ops.shape(start_logits)[1]\n",
    "            start_positions = C.clip_by_value(start_positions, 0, ignored_index)\n",
    "            end_positions = C.clip_by_value(end_positions, 0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.SoftmaxCrossEntropyWithLogits(sparse = True)\n",
    "            #loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,)\n",
    "        else:\n",
    "            outputs = tuple()\n",
    "        return outputs + (start_logits, end_logits,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d8fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2], dtype=Float32, value= [5.13476753e+000, 5.13476753e+000]),\n",
       " Tensor(shape=[2, 165], dtype=Float32, value=\n",
       " [[2.52272874e-001, 3.20962906e-001, 1.44353583e-001 ... 1.16958171e-001, 1.70200512e-001, 2.72789359e-001],\n",
       "  [2.52272874e-001, 3.20962906e-001, 1.44353583e-001 ... 1.16958171e-001, 1.70200512e-001, 2.72789359e-001]]),\n",
       " Tensor(shape=[2, 165], dtype=Float32, value=\n",
       " [[2.52272874e-001, 3.20962906e-001, 1.44353583e-001 ... 1.16958171e-001, 1.70200512e-001, 2.72789359e-001],\n",
       "  [2.52272874e-001, 3.20962906e-001, 1.44353583e-001 ... 1.16958171e-001, 1.70200512e-001, 2.72789359e-001]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LukeForReadingComprehension__ = LukeForReadingComprehension(config)\n",
    "LukeForReadingComprehension__.construct(word_ids,\n",
    "                                        word_segment_ids,\n",
    "                                        word_attention_mask,\n",
    "                                        entity_ids,\n",
    "                                        entity_position_ids,\n",
    "                                        entity_segment_ids,\n",
    "                                        entity_attention_mask,\n",
    "                                        start_positions,\n",
    "                                        end_positions\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a961da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LukeForReadingComprehension<\n",
       "  (encoder): EntityAwareEncoder<\n",
       "    (layer): CellList<\n",
       "      (0): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.0.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.0.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.0.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.0.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (1): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.1.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.1.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.1.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.1.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (2): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.2.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.2.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.2.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.2.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (3): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.3.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.3.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.3.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.3.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (4): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.4.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.4.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.4.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.4.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (5): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.5.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.5.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.5.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.5.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (6): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.6.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.6.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.6.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.6.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (7): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.7.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.7.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.7.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.7.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (8): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.8.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.8.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.8.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.8.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (9): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.9.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.9.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.9.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.9.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (10): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.10.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.10.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.10.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.10.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (11): EntityAwareLayer<\n",
       "        (attention): EntityAwareAttention<\n",
       "          (self_attention): EntityAwareSelfAttention<\n",
       "            (query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (w2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2w_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (e2e_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=0.1>\n",
       "            >\n",
       "          (output): BertSelfOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.11.attention.output.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.11.attention.output.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            (dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=0.9>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=encoder.layer.11.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=encoder.layer.11.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      >\n",
       "    >\n",
       "  (pooler): Dense<\n",
       "    input_channels=768, output_channels=768, has_bias=True, activation=Tanh<>\n",
       "    (activation): Tanh<>\n",
       "    >\n",
       "  (embeddings): RobertaEmbeddings<\n",
       "    (word_embeddings): Embedding<vocab_size=50267, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=embeddings.word_embeddings.embedding_table, shape=(50267, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=1>\n",
       "    (position_embeddings): Embedding<vocab_size=514, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=embeddings.position_embeddings.embedding_table, shape=(514, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=1>\n",
       "    (token_type_embeddings): Embedding<vocab_size=16, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=embeddings.token_type_embeddings.embedding_table, shape=(16, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (LayerNorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=embeddings.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=embeddings.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "    (dropout): Dropout<keep_prob=0.1>\n",
       "    >\n",
       "  (entity_embeddings): EntityEmbeddings<\n",
       "    (entity_embeddings): Embedding<vocab_size=500000, embedding_size=256, use_one_hot=False, embedding_table=Parameter (name=entity_embeddings.entity_embeddings.embedding_table, shape=(500000, 256), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=0>\n",
       "    (entity_embedding_dense): Dense<input_channels=256, output_channels=768>\n",
       "    (position_embeddings): Embedding<vocab_size=514, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=entity_embeddings.position_embeddings.embedding_table, shape=(514, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (token_type_embeddings): Embedding<vocab_size=16, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=entity_embeddings.token_type_embeddings.embedding_table, shape=(16, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (layer_norm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=entity_embeddings.layer_norm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=entity_embeddings.layer_norm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "    (dropout): Dropout<keep_prob=0.1>\n",
       "    >\n",
       "  (qa_outputs): Dense<input_channels=768, output_channels=2, has_bias=True>\n",
       "  >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LukeForReadingComprehension__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
