{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/baicai/anaconda3/envs/mindspore1.3/lib/python3.7/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/baicai/anaconda3/envs/mindspore1.3/lib/python3.7/site-packages (from torch) (1.18.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layer.0.attention.self.query.weight',\n",
       "              tensor([[-0.0034,  0.0348,  0.0006,  ...,  0.0017,  0.0590, -0.0426],\n",
       "                      [-0.0249,  0.0531, -0.0148,  ..., -0.0304, -0.0145,  0.0117],\n",
       "                      [ 0.0062,  0.0708, -0.0332,  ...,  0.0803,  0.0118, -0.0132],\n",
       "                      ...,\n",
       "                      [-0.0585,  0.0210, -0.0427,  ..., -0.0298,  0.0051,  0.0694],\n",
       "                      [ 0.0424,  0.0232, -0.0613,  ..., -0.0552, -0.0157,  0.0171],\n",
       "                      [-0.0183, -0.0454, -0.0102,  ...,  0.0471,  0.0229, -0.0177]])),\n",
       "             ('encoder.layer.0.attention.self.query.bias',\n",
       "              tensor([ 0.3127,  0.0561, -0.0748,  ..., -0.0707, -0.0500, -0.0668])),\n",
       "             ('encoder.layer.0.attention.self.key.weight',\n",
       "              tensor([[-0.0044, -0.0183, -0.0138,  ..., -0.0041,  0.0094, -0.0148],\n",
       "                      [-0.0240, -0.0002,  0.0252,  ...,  0.0400,  0.0435, -0.0201],\n",
       "                      [-0.0266, -0.0528, -0.0121,  ..., -0.0364,  0.0073,  0.0146],\n",
       "                      ...,\n",
       "                      [-0.0705, -0.0267, -0.0192,  ..., -0.0191,  0.0093,  0.1013],\n",
       "                      [ 0.0151,  0.0078, -0.0170,  ..., -0.0034, -0.0085,  0.0441],\n",
       "                      [-0.0091, -0.0636,  0.0413,  ...,  0.0467,  0.0148, -0.0465]])),\n",
       "             ('encoder.layer.0.attention.self.key.bias',\n",
       "              tensor([-0.0048, -0.0028, -0.0003,  ...,  0.0012,  0.0018,  0.0012])),\n",
       "             ('encoder.layer.0.attention.self.value.weight',\n",
       "              tensor([[ 0.0301, -0.0005, -0.0243,  ..., -0.0180,  0.0026,  0.0220],\n",
       "                      [ 0.0566,  0.0431,  0.0014,  ..., -0.0154,  0.0925, -0.0200],\n",
       "                      [-0.0149, -0.0425,  0.0127,  ..., -0.0516,  0.0016,  0.0677],\n",
       "                      ...,\n",
       "                      [-0.0108,  0.0077, -0.0119,  ...,  0.0364,  0.0257,  0.0107],\n",
       "                      [-0.0035, -0.0135, -0.0510,  ...,  0.0386, -0.0337,  0.0285],\n",
       "                      [ 0.0042, -0.0091, -0.0134,  ..., -0.0255,  0.0878, -0.0182]])),\n",
       "             ('encoder.layer.0.attention.self.value.bias',\n",
       "              tensor([-0.0007,  0.0023, -0.0086,  ..., -0.0223, -0.0205, -0.0341])),\n",
       "             ('encoder.layer.0.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0025,  0.0355,  0.0011,  ...,  0.0028,  0.0597, -0.0425],\n",
       "                      [-0.0250,  0.0536, -0.0150,  ..., -0.0302, -0.0146,  0.0121],\n",
       "                      [ 0.0056,  0.0714, -0.0337,  ...,  0.0806,  0.0113, -0.0127],\n",
       "                      ...,\n",
       "                      [-0.0588,  0.0207, -0.0424,  ..., -0.0300,  0.0048,  0.0693],\n",
       "                      [ 0.0420,  0.0227, -0.0608,  ..., -0.0547, -0.0157,  0.0174],\n",
       "                      [-0.0183, -0.0458, -0.0104,  ...,  0.0469,  0.0234, -0.0180]])),\n",
       "             ('encoder.layer.0.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.3123,  0.0552, -0.0756,  ..., -0.0703, -0.0498, -0.0665])),\n",
       "             ('encoder.layer.0.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0029,  0.0351,  0.0011,  ...,  0.0024,  0.0605, -0.0435],\n",
       "                      [-0.0250,  0.0529, -0.0153,  ..., -0.0301, -0.0158,  0.0125],\n",
       "                      [ 0.0065,  0.0714, -0.0331,  ...,  0.0804,  0.0121, -0.0136],\n",
       "                      ...,\n",
       "                      [-0.0590,  0.0209, -0.0425,  ..., -0.0299,  0.0044,  0.0692],\n",
       "                      [ 0.0425,  0.0222, -0.0609,  ..., -0.0552, -0.0153,  0.0175],\n",
       "                      [-0.0189, -0.0457, -0.0107,  ...,  0.0480,  0.0223, -0.0180]])),\n",
       "             ('encoder.layer.0.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.3110,  0.0566, -0.0762,  ..., -0.0710, -0.0497, -0.0660])),\n",
       "             ('encoder.layer.0.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0031,  0.0351,  0.0007,  ...,  0.0025,  0.0601, -0.0429],\n",
       "                      [-0.0260,  0.0528, -0.0150,  ..., -0.0300, -0.0140,  0.0110],\n",
       "                      [ 0.0056,  0.0701, -0.0349,  ...,  0.0812,  0.0117, -0.0133],\n",
       "                      ...,\n",
       "                      [-0.0592,  0.0200, -0.0431,  ..., -0.0291,  0.0038,  0.0703],\n",
       "                      [ 0.0410,  0.0216, -0.0610,  ..., -0.0552, -0.0164,  0.0182],\n",
       "                      [-0.0193, -0.0460, -0.0109,  ...,  0.0482,  0.0220, -0.0185]])),\n",
       "             ('encoder.layer.0.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.3115,  0.0549, -0.0754,  ..., -0.0703, -0.0497, -0.0674])),\n",
       "             ('encoder.layer.0.attention.output.dense.weight',\n",
       "              tensor([[ 0.0019,  0.0402, -0.0172,  ..., -0.0144, -0.0331, -0.0170],\n",
       "                      [-0.0336,  0.0139, -0.0164,  ...,  0.0311, -0.0093,  0.0270],\n",
       "                      [ 0.0274, -0.0739,  0.0189,  ..., -0.0320, -0.0066,  0.0995],\n",
       "                      ...,\n",
       "                      [ 0.0281,  0.0011,  0.0216,  ...,  0.0128, -0.0107, -0.0363],\n",
       "                      [-0.0084,  0.0514, -0.0469,  ...,  0.0426,  0.0258, -0.0160],\n",
       "                      [ 0.0475,  0.0229,  0.0917,  ...,  0.0277,  0.0091,  0.0070]])),\n",
       "             ('encoder.layer.0.attention.output.dense.bias',\n",
       "              tensor([-0.0132,  0.0293,  0.0859,  ...,  0.0731, -0.0068,  0.0102])),\n",
       "             ('encoder.layer.0.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9795, 0.9897, 0.9736,  ..., 0.9834, 0.9902, 0.9971])),\n",
       "             ('encoder.layer.0.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.4302,  0.2761, -0.0064,  ...,  0.0112,  0.3301, -0.2979])),\n",
       "             ('encoder.layer.0.intermediate.dense.weight',\n",
       "              tensor([[ 5.8624e-02, -6.4453e-02, -9.3750e-02,  ...,  5.2834e-03,\n",
       "                        2.0187e-02, -1.5182e-02],\n",
       "                      [ 1.5289e-02, -2.7527e-02,  2.0676e-02,  ...,  1.5396e-02,\n",
       "                       -3.7750e-02,  1.2140e-01],\n",
       "                      [ 3.7201e-02, -6.6833e-02,  1.1522e-04,  ...,  2.9068e-02,\n",
       "                       -2.6688e-02, -2.1790e-02],\n",
       "                      ...,\n",
       "                      [ 1.6068e-02, -8.9294e-02,  4.4250e-03,  ...,  3.3112e-02,\n",
       "                       -5.1758e-02, -6.7329e-04],\n",
       "                      [ 1.3416e-01,  5.1971e-02, -1.3000e-01,  ..., -1.4868e-01,\n",
       "                       -3.1769e-02,  2.1835e-02],\n",
       "                      [ 7.1350e-02, -2.8976e-02, -6.0455e-02,  ...,  9.5901e-03,\n",
       "                       -5.7434e-02, -2.9419e-02]])),\n",
       "             ('encoder.layer.0.intermediate.dense.bias',\n",
       "              tensor([-0.0939, -0.0759, -0.0845,  ..., -0.1089, -0.0693, -0.0941])),\n",
       "             ('encoder.layer.0.output.dense.weight',\n",
       "              tensor([[ 0.0437,  0.1018,  0.0296,  ..., -0.0551,  0.0595,  0.0657],\n",
       "                      [ 0.0180,  0.0113, -0.0093,  ..., -0.0071,  0.0183, -0.0112],\n",
       "                      [ 0.0273, -0.0742,  0.0529,  ..., -0.0428, -0.0107,  0.0005],\n",
       "                      ...,\n",
       "                      [-0.0137, -0.0032,  0.0535,  ...,  0.0096, -0.0574,  0.0152],\n",
       "                      [-0.0496,  0.0331, -0.0703,  ...,  0.0590, -0.0355, -0.0052],\n",
       "                      [-0.0953, -0.0449, -0.0946,  ..., -0.0533,  0.0520, -0.0291]])),\n",
       "             ('encoder.layer.0.output.dense.bias',\n",
       "              tensor([ 0.0645, -0.0406,  0.0408,  ...,  0.0072, -0.0952,  0.0554])),\n",
       "             ('encoder.layer.0.output.LayerNorm.weight',\n",
       "              tensor([0.9688, 0.9604, 0.9673,  ..., 0.9727, 0.9697, 0.9487])),\n",
       "             ('encoder.layer.0.output.LayerNorm.bias',\n",
       "              tensor([ 0.3977, -0.1886,  0.0430,  ..., -0.0484, -0.2737,  0.2000])),\n",
       "             ('encoder.layer.1.attention.self.query.weight',\n",
       "              tensor([[ 0.0242, -0.1104,  0.0185,  ..., -0.0665, -0.0303, -0.0332],\n",
       "                      [ 0.0284, -0.0107,  0.0971,  ..., -0.0237,  0.0275,  0.1658],\n",
       "                      [ 0.0098, -0.0415,  0.0324,  ...,  0.0082, -0.0009, -0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0166,  0.0308,  0.0660,  ...,  0.0443, -0.0503,  0.1027],\n",
       "                      [-0.1144,  0.0697,  0.0682,  ..., -0.0142, -0.0743,  0.0615],\n",
       "                      [ 0.0299, -0.0684,  0.0246,  ...,  0.0627, -0.0016,  0.0035]])),\n",
       "             ('encoder.layer.1.attention.self.query.bias',\n",
       "              tensor([ 0.0731,  0.0507, -0.0681,  ...,  0.0828,  0.0448, -0.0767])),\n",
       "             ('encoder.layer.1.attention.self.key.weight',\n",
       "              tensor([[-0.0091,  0.0601, -0.0058,  ..., -0.0317,  0.0103, -0.0156],\n",
       "                      [-0.0205, -0.0876, -0.0567,  ...,  0.0548, -0.0381,  0.0180],\n",
       "                      [-0.0134,  0.0387,  0.0354,  ...,  0.0056,  0.0004, -0.0113],\n",
       "                      ...,\n",
       "                      [-0.0627, -0.0682,  0.0806,  ...,  0.0210,  0.0241,  0.0018],\n",
       "                      [ 0.0493, -0.0461,  0.0357,  ...,  0.0454,  0.0340, -0.0327],\n",
       "                      [ 0.0610,  0.0401,  0.0560,  ...,  0.0909,  0.0236, -0.0013]])),\n",
       "             ('encoder.layer.1.attention.self.key.bias',\n",
       "              tensor([ 1.8187e-03, -4.3774e-04, -1.1654e-03,  ...,  6.2823e-05,\n",
       "                       4.4751e-04,  2.1400e-03])),\n",
       "             ('encoder.layer.1.attention.self.value.weight',\n",
       "              tensor([[-0.0641,  0.0059, -0.0252,  ..., -0.0038,  0.0523,  0.0277],\n",
       "                      [-0.0176,  0.0365, -0.0351,  ..., -0.0132,  0.0315, -0.0209],\n",
       "                      [ 0.0033,  0.0356,  0.0023,  ...,  0.0255,  0.0857, -0.0169],\n",
       "                      ...,\n",
       "                      [-0.0051, -0.0835, -0.0303,  ..., -0.0220,  0.0368,  0.0246],\n",
       "                      [-0.0627,  0.0671, -0.0165,  ..., -0.0047,  0.0230, -0.0128],\n",
       "                      [ 0.0364,  0.0345,  0.0161,  ...,  0.0105,  0.0321, -0.0162]])),\n",
       "             ('encoder.layer.1.attention.self.value.bias',\n",
       "              tensor([ 0.0092,  0.0018,  0.0062,  ...,  0.0006, -0.0553, -0.0010])),\n",
       "             ('encoder.layer.1.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0244, -0.1104,  0.0182,  ..., -0.0667, -0.0310, -0.0332],\n",
       "                      [ 0.0290, -0.0110,  0.0972,  ..., -0.0223,  0.0279,  0.1655],\n",
       "                      [ 0.0110, -0.0412,  0.0322,  ...,  0.0071, -0.0015, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0175,  0.0304,  0.0660,  ...,  0.0444, -0.0501,  0.1025],\n",
       "                      [-0.1144,  0.0698,  0.0684,  ..., -0.0138, -0.0736,  0.0622],\n",
       "                      [ 0.0283, -0.0679,  0.0246,  ...,  0.0617, -0.0032,  0.0034]])),\n",
       "             ('encoder.layer.1.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0729,  0.0513, -0.0684,  ...,  0.0828,  0.0457, -0.0776])),\n",
       "             ('encoder.layer.1.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0252, -0.1111,  0.0186,  ..., -0.0667, -0.0309, -0.0335],\n",
       "                      [ 0.0295, -0.0111,  0.0970,  ..., -0.0231,  0.0260,  0.1663],\n",
       "                      [ 0.0099, -0.0415,  0.0315,  ...,  0.0077, -0.0009, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0171,  0.0314,  0.0657,  ...,  0.0442, -0.0500,  0.1033],\n",
       "                      [-0.1144,  0.0695,  0.0677,  ..., -0.0135, -0.0742,  0.0630],\n",
       "                      [ 0.0287, -0.0681,  0.0242,  ...,  0.0623, -0.0018,  0.0030]])),\n",
       "             ('encoder.layer.1.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0740,  0.0512, -0.0679,  ...,  0.0833,  0.0459, -0.0778])),\n",
       "             ('encoder.layer.1.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0251, -0.1100,  0.0191,  ..., -0.0665, -0.0299, -0.0339],\n",
       "                      [ 0.0288, -0.0110,  0.0966,  ..., -0.0225,  0.0267,  0.1661],\n",
       "                      [ 0.0100, -0.0417,  0.0317,  ...,  0.0079, -0.0011, -0.0034],\n",
       "                      ...,\n",
       "                      [ 0.0179,  0.0316,  0.0666,  ...,  0.0433, -0.0503,  0.1039],\n",
       "                      [-0.1141,  0.0702,  0.0680,  ..., -0.0132, -0.0740,  0.0627],\n",
       "                      [ 0.0287, -0.0687,  0.0240,  ...,  0.0619, -0.0021,  0.0033]])),\n",
       "             ('encoder.layer.1.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0734,  0.0510, -0.0681,  ...,  0.0840,  0.0453, -0.0772])),\n",
       "             ('encoder.layer.1.attention.output.dense.weight',\n",
       "              tensor([[-0.0353, -0.0124,  0.0090,  ..., -0.0709, -0.0297,  0.0190],\n",
       "                      [ 0.0014, -0.0534, -0.0028,  ..., -0.0754, -0.0400,  0.0075],\n",
       "                      [ 0.0417,  0.0133, -0.0243,  ..., -0.0331,  0.0178,  0.0327],\n",
       "                      ...,\n",
       "                      [-0.0061, -0.0166,  0.0351,  ...,  0.0248, -0.0249,  0.0453],\n",
       "                      [-0.0220,  0.0094,  0.0079,  ..., -0.0481, -0.0392, -0.0514],\n",
       "                      [ 0.0071, -0.0232,  0.0421,  ...,  0.0062, -0.0463,  0.0251]])),\n",
       "             ('encoder.layer.1.attention.output.dense.bias',\n",
       "              tensor([-0.2164,  0.0367, -0.0811,  ..., -0.0743,  0.2334,  0.0737])),\n",
       "             ('encoder.layer.1.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9849, 1.0000, 0.9570,  ..., 0.9790, 0.9893, 0.9712])),\n",
       "             ('encoder.layer.1.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.3284,  0.1815, -0.0329,  ..., -0.0721,  0.2505, -0.2284])),\n",
       "             ('encoder.layer.1.intermediate.dense.weight',\n",
       "              tensor([[ 0.0296, -0.0338,  0.0008,  ..., -0.0232,  0.0985,  0.0493],\n",
       "                      [-0.0059, -0.0673, -0.0325,  ...,  0.0357,  0.0019, -0.0533],\n",
       "                      [-0.0043,  0.0828,  0.0750,  ...,  0.0156,  0.0189, -0.0082],\n",
       "                      ...,\n",
       "                      [-0.0363, -0.0446,  0.1230,  ...,  0.0003,  0.0127, -0.0324],\n",
       "                      [ 0.0737, -0.0580, -0.1039,  ..., -0.0316,  0.0239, -0.0826],\n",
       "                      [ 0.0016,  0.0746,  0.0951,  ..., -0.0185, -0.0258,  0.0307]])),\n",
       "             ('encoder.layer.1.intermediate.dense.bias',\n",
       "              tensor([ 0.0947, -0.0511, -0.0545,  ..., -0.0864, -0.0854, -0.0851])),\n",
       "             ('encoder.layer.1.output.dense.weight',\n",
       "              tensor([[-0.0241, -0.0183,  0.0625,  ..., -0.0208, -0.0900,  0.0511],\n",
       "                      [ 0.0460, -0.0169, -0.0151,  ..., -0.0326, -0.0058, -0.0449],\n",
       "                      [ 0.0416,  0.0345,  0.0654,  ..., -0.0248, -0.0519,  0.0959],\n",
       "                      ...,\n",
       "                      [ 0.0425, -0.0576, -0.0737,  ..., -0.0384,  0.0234,  0.0344],\n",
       "                      [ 0.0103,  0.0833,  0.1078,  ...,  0.0645, -0.0420, -0.0219],\n",
       "                      [-0.0614,  0.0723,  0.0041,  ..., -0.0343,  0.0688,  0.0394]])),\n",
       "             ('encoder.layer.1.output.dense.bias',\n",
       "              tensor([ 0.0109,  0.0053,  0.0674,  ...,  0.0117, -0.0476,  0.0147])),\n",
       "             ('encoder.layer.1.output.LayerNorm.weight',\n",
       "              tensor([0.9609, 0.9941, 0.9404,  ..., 0.9702, 0.9683, 0.9365])),\n",
       "             ('encoder.layer.1.output.LayerNorm.bias',\n",
       "              tensor([ 0.1897, -0.1934, -0.0495,  ..., -0.0138, -0.2725,  0.1589])),\n",
       "             ('encoder.layer.2.attention.self.query.weight',\n",
       "              tensor([[ 0.0003, -0.0479,  0.0431,  ..., -0.0289, -0.0676, -0.0153],\n",
       "                      [-0.0823, -0.0898,  0.0355,  ..., -0.0230,  0.0029,  0.0134],\n",
       "                      [-0.0204, -0.0079, -0.0393,  ..., -0.0351, -0.0155, -0.0209],\n",
       "                      ...,\n",
       "                      [ 0.0219, -0.0164, -0.0113,  ..., -0.0356,  0.0807, -0.0701],\n",
       "                      [ 0.0315, -0.0955, -0.0500,  ..., -0.0432,  0.0002,  0.0130],\n",
       "                      [ 0.0215, -0.0232, -0.0687,  ...,  0.0747, -0.0058,  0.0457]])),\n",
       "             ('encoder.layer.2.attention.self.query.bias',\n",
       "              tensor([-0.0603,  0.1272,  0.1134,  ..., -0.1527,  0.0651, -0.1649])),\n",
       "             ('encoder.layer.2.attention.self.key.weight',\n",
       "              tensor([[ 0.0131,  0.1378,  0.0401,  ...,  0.0347,  0.0183, -0.0546],\n",
       "                      [ 0.0003,  0.0158, -0.0177,  ..., -0.0315, -0.0373,  0.0424],\n",
       "                      [ 0.0665, -0.0095,  0.0058,  ..., -0.0528,  0.0259, -0.0447],\n",
       "                      ...,\n",
       "                      [-0.0025, -0.0355, -0.0331,  ..., -0.0019, -0.0529, -0.0663],\n",
       "                      [-0.0566, -0.0352,  0.0327,  ...,  0.0011, -0.0503,  0.0195],\n",
       "                      [ 0.0336,  0.0129, -0.0254,  ...,  0.0652,  0.0098,  0.0503]])),\n",
       "             ('encoder.layer.2.attention.self.key.bias',\n",
       "              tensor([-0.0006, -0.0032, -0.0015,  ...,  0.0025, -0.0006, -0.0003])),\n",
       "             ('encoder.layer.2.attention.self.value.weight',\n",
       "              tensor([[-3.4370e-03,  4.9164e-02,  2.2186e-02,  ...,  2.5101e-02,\n",
       "                        2.7756e-02, -1.3781e-03],\n",
       "                      [-3.8624e-05,  8.9844e-02, -2.1042e-02,  ..., -4.3488e-03,\n",
       "                       -3.1525e-02, -6.6528e-03],\n",
       "                      [ 1.9547e-02,  4.7272e-02,  3.5248e-03,  ...,  3.5004e-02,\n",
       "                        2.9114e-02, -2.3575e-02],\n",
       "                      ...,\n",
       "                      [-1.2527e-02,  2.6398e-02, -1.1475e-02,  ..., -3.8879e-02,\n",
       "                       -2.4734e-02,  4.0863e-02],\n",
       "                      [ 6.2347e-02,  3.8513e-02, -4.0588e-02,  ..., -2.2827e-02,\n",
       "                        6.5613e-02,  2.9358e-02],\n",
       "                      [-2.3438e-02, -1.0963e-02,  1.7681e-03,  ..., -1.1505e-02,\n",
       "                        1.5343e-02,  5.3467e-02]])),\n",
       "             ('encoder.layer.2.attention.self.value.bias',\n",
       "              tensor([-0.0293,  0.0149,  0.0089,  ..., -0.0026,  0.0108, -0.0052])),\n",
       "             ('encoder.layer.2.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0005, -0.0482,  0.0431,  ..., -0.0290, -0.0692, -0.0151],\n",
       "                      [-0.0825, -0.0906,  0.0361,  ..., -0.0227,  0.0041,  0.0145],\n",
       "                      [-0.0208, -0.0072, -0.0388,  ..., -0.0344, -0.0140, -0.0209],\n",
       "                      ...,\n",
       "                      [ 0.0211, -0.0169, -0.0120,  ..., -0.0348,  0.0814, -0.0695],\n",
       "                      [ 0.0319, -0.0948, -0.0502,  ..., -0.0431, -0.0003,  0.0119],\n",
       "                      [ 0.0211, -0.0241, -0.0688,  ...,  0.0755, -0.0056,  0.0462]])),\n",
       "             ('encoder.layer.2.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0599,  0.1266,  0.1125,  ..., -0.1526,  0.0652, -0.1647])),\n",
       "             ('encoder.layer.2.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0009, -0.0472,  0.0435,  ..., -0.0284, -0.0676, -0.0151],\n",
       "                      [-0.0815, -0.0903,  0.0362,  ..., -0.0241,  0.0045,  0.0135],\n",
       "                      [-0.0208, -0.0076, -0.0380,  ..., -0.0348, -0.0153, -0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0223, -0.0159, -0.0119,  ..., -0.0353,  0.0815, -0.0695],\n",
       "                      [ 0.0318, -0.0950, -0.0498,  ..., -0.0430,  0.0009,  0.0121],\n",
       "                      [ 0.0218, -0.0240, -0.0690,  ...,  0.0752, -0.0059,  0.0461]])),\n",
       "             ('encoder.layer.2.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0608,  0.1270,  0.1131,  ..., -0.1528,  0.0649, -0.1647])),\n",
       "             ('encoder.layer.2.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0004, -0.0480,  0.0432,  ..., -0.0277, -0.0678, -0.0151],\n",
       "                      [-0.0819, -0.0909,  0.0357,  ..., -0.0236,  0.0036,  0.0141],\n",
       "                      [-0.0198, -0.0079, -0.0386,  ..., -0.0352, -0.0154, -0.0207],\n",
       "                      ...,\n",
       "                      [ 0.0224, -0.0163, -0.0113,  ..., -0.0359,  0.0810, -0.0691],\n",
       "                      [ 0.0317, -0.0945, -0.0504,  ..., -0.0425,  0.0005,  0.0116],\n",
       "                      [ 0.0216, -0.0240, -0.0682,  ...,  0.0743, -0.0058,  0.0463]])),\n",
       "             ('encoder.layer.2.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0608,  0.1270,  0.1135,  ..., -0.1521,  0.0647, -0.1643])),\n",
       "             ('encoder.layer.2.attention.output.dense.weight',\n",
       "              tensor([[-0.0134, -0.0185,  0.0227,  ..., -0.0042, -0.0105,  0.0215],\n",
       "                      [ 0.0053,  0.0161, -0.0151,  ..., -0.0476, -0.0120, -0.0026],\n",
       "                      [ 0.0063,  0.0410, -0.0690,  ..., -0.0133, -0.0132, -0.0135],\n",
       "                      ...,\n",
       "                      [-0.0316, -0.0073,  0.0503,  ..., -0.0046,  0.0113, -0.0107],\n",
       "                      [-0.0788,  0.0022,  0.0044,  ...,  0.0443,  0.0207, -0.0086],\n",
       "                      [ 0.0565,  0.0484,  0.0016,  ...,  0.0045,  0.0565,  0.0270]])),\n",
       "             ('encoder.layer.2.attention.output.dense.bias',\n",
       "              tensor([ 0.0612, -0.0575,  0.0323,  ...,  0.0818, -0.0381,  0.0750])),\n",
       "             ('encoder.layer.2.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9844, 0.9844, 0.9839,  ..., 0.9639, 0.9561, 0.9717])),\n",
       "             ('encoder.layer.2.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0937, -0.0035, -0.3413,  ..., -0.0270, -0.0707,  0.2170])),\n",
       "             ('encoder.layer.2.intermediate.dense.weight',\n",
       "              tensor([[ 2.1637e-02,  8.6243e-02,  6.9214e-02,  ..., -4.6906e-02,\n",
       "                       -1.3237e-02, -4.2328e-02],\n",
       "                      [-2.5146e-02, -6.8176e-02,  9.2712e-02,  ..., -7.4585e-02,\n",
       "                        5.3528e-02, -2.3117e-03],\n",
       "                      [-7.1655e-02,  4.0398e-03,  9.1797e-02,  ...,  1.4557e-02,\n",
       "                        2.1866e-02,  2.6855e-03],\n",
       "                      ...,\n",
       "                      [-3.8422e-02, -1.5549e-02,  5.8327e-03,  ...,  9.1858e-03,\n",
       "                       -1.1559e-03,  1.9073e-02],\n",
       "                      [-6.4209e-02,  1.2459e-02, -1.4587e-02,  ..., -1.8072e-03,\n",
       "                        3.4302e-02, -9.1980e-02],\n",
       "                      [-1.7285e-05,  1.8631e-02,  1.5747e-02,  ..., -1.3870e-02,\n",
       "                       -5.0537e-02, -1.0089e-01]])),\n",
       "             ('encoder.layer.2.intermediate.dense.bias',\n",
       "              tensor([-0.0231, -0.0798, -0.0697,  ...,  0.0595, -0.0860, -0.0901])),\n",
       "             ('encoder.layer.2.output.dense.weight',\n",
       "              tensor([[-0.0228,  0.0848,  0.0173,  ..., -0.0385, -0.1061,  0.0026],\n",
       "                      [-0.0032,  0.0186, -0.0611,  ...,  0.0482, -0.0289, -0.0113],\n",
       "                      [ 0.0523, -0.0566,  0.0584,  ...,  0.0111, -0.0482, -0.0760],\n",
       "                      ...,\n",
       "                      [ 0.0022, -0.0454,  0.0661,  ...,  0.0137,  0.1047, -0.0017],\n",
       "                      [ 0.0481, -0.0090, -0.0307,  ...,  0.0111, -0.0938, -0.0097],\n",
       "                      [ 0.0302, -0.0505, -0.0229,  ..., -0.0407,  0.0136,  0.0024]])),\n",
       "             ('encoder.layer.2.output.dense.bias',\n",
       "              tensor([-0.0081, -0.0427,  0.0117,  ...,  0.0311,  0.0149,  0.0456])),\n",
       "             ('encoder.layer.2.output.LayerNorm.weight',\n",
       "              tensor([0.9658, 0.9678, 0.9517,  ..., 0.9761, 0.9775, 0.9585])),\n",
       "             ('encoder.layer.2.output.LayerNorm.bias',\n",
       "              tensor([-0.0324, -0.1088,  0.2081,  ..., -0.0497, -0.0271, -0.2030])),\n",
       "             ('encoder.layer.3.attention.self.query.weight',\n",
       "              tensor([[-0.0692,  0.0113,  0.0363,  ...,  0.0251,  0.0511, -0.0131],\n",
       "                      [-0.0192, -0.0787, -0.0413,  ...,  0.0364, -0.0144,  0.0318],\n",
       "                      [-0.0204, -0.0166,  0.0804,  ...,  0.0341, -0.0174,  0.0057],\n",
       "                      ...,\n",
       "                      [-0.0573, -0.0114, -0.0103,  ...,  0.0396,  0.0895, -0.0156],\n",
       "                      [ 0.0670, -0.0320,  0.0414,  ..., -0.0896,  0.0230, -0.0345],\n",
       "                      [ 0.0327,  0.0645, -0.0220,  ..., -0.0438, -0.0320,  0.0217]])),\n",
       "             ('encoder.layer.3.attention.self.query.bias',\n",
       "              tensor([-0.0252, -0.0515,  0.0316,  ...,  0.0130,  0.1174, -0.0814])),\n",
       "             ('encoder.layer.3.attention.self.key.weight',\n",
       "              tensor([[-0.1348, -0.0401, -0.0014,  ...,  0.0218,  0.0508, -0.0275],\n",
       "                      [ 0.0268,  0.0994,  0.0907,  ..., -0.0239,  0.0826, -0.0440],\n",
       "                      [ 0.0050, -0.0410,  0.0916,  ..., -0.0214,  0.0171, -0.0213],\n",
       "                      ...,\n",
       "                      [-0.0250, -0.0044, -0.0051,  ...,  0.0714, -0.0627, -0.0399],\n",
       "                      [ 0.0044, -0.0239, -0.0045,  ...,  0.0388, -0.0312,  0.0059],\n",
       "                      [ 0.0296,  0.0116,  0.0230,  ...,  0.0174, -0.0285, -0.0182]])),\n",
       "             ('encoder.layer.3.attention.self.key.bias',\n",
       "              tensor([-0.0005,  0.0015,  0.0006,  ...,  0.0003, -0.0021,  0.0001])),\n",
       "             ('encoder.layer.3.attention.self.value.weight',\n",
       "              tensor([[ 0.0106, -0.0296, -0.0731,  ...,  0.0305,  0.0952,  0.0224],\n",
       "                      [-0.0164,  0.0392,  0.0460,  ..., -0.0300,  0.0101, -0.0068],\n",
       "                      [ 0.0403,  0.0391, -0.0698,  ..., -0.0504,  0.0274,  0.0202],\n",
       "                      ...,\n",
       "                      [ 0.0709,  0.0027,  0.0122,  ...,  0.0197,  0.0324,  0.0446],\n",
       "                      [-0.0240,  0.0493, -0.0339,  ..., -0.0035, -0.1051, -0.0111],\n",
       "                      [ 0.0273, -0.0094, -0.0410,  ..., -0.0299, -0.0014, -0.0493]])),\n",
       "             ('encoder.layer.3.attention.self.value.bias',\n",
       "              tensor([-0.0189, -0.0092, -0.0087,  ...,  0.0056,  0.0059, -0.0009])),\n",
       "             ('encoder.layer.3.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0703,  0.0098,  0.0364,  ...,  0.0260,  0.0518, -0.0126],\n",
       "                      [-0.0194, -0.0788, -0.0412,  ...,  0.0366, -0.0142,  0.0311],\n",
       "                      [-0.0192, -0.0169,  0.0803,  ...,  0.0333, -0.0176,  0.0063],\n",
       "                      ...,\n",
       "                      [-0.0580, -0.0111, -0.0101,  ...,  0.0396,  0.0898, -0.0157],\n",
       "                      [ 0.0663, -0.0331,  0.0411,  ..., -0.0894,  0.0228, -0.0347],\n",
       "                      [ 0.0323,  0.0638, -0.0221,  ..., -0.0440, -0.0322,  0.0216]])),\n",
       "             ('encoder.layer.3.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0252, -0.0518,  0.0317,  ...,  0.0138,  0.1171, -0.0805])),\n",
       "             ('encoder.layer.3.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0699,  0.0112,  0.0369,  ...,  0.0258,  0.0514, -0.0131],\n",
       "                      [-0.0192, -0.0793, -0.0417,  ...,  0.0370, -0.0143,  0.0318],\n",
       "                      [-0.0200, -0.0164,  0.0809,  ...,  0.0338, -0.0174,  0.0055],\n",
       "                      ...,\n",
       "                      [-0.0570, -0.0107, -0.0104,  ...,  0.0396,  0.0901, -0.0163],\n",
       "                      [ 0.0662, -0.0330,  0.0413,  ..., -0.0895,  0.0231, -0.0351],\n",
       "                      [ 0.0325,  0.0639, -0.0229,  ..., -0.0439, -0.0329,  0.0218]])),\n",
       "             ('encoder.layer.3.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0255, -0.0517,  0.0313,  ...,  0.0121,  0.1171, -0.0812])),\n",
       "             ('encoder.layer.3.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0692,  0.0112,  0.0373,  ...,  0.0253,  0.0515, -0.0131],\n",
       "                      [-0.0185, -0.0792, -0.0412,  ...,  0.0368, -0.0144,  0.0317],\n",
       "                      [-0.0198, -0.0167,  0.0807,  ...,  0.0334, -0.0174,  0.0056],\n",
       "                      ...,\n",
       "                      [-0.0574, -0.0109, -0.0101,  ...,  0.0394,  0.0898, -0.0153],\n",
       "                      [ 0.0667, -0.0328,  0.0418,  ..., -0.0901,  0.0233, -0.0348],\n",
       "                      [ 0.0317,  0.0633, -0.0221,  ..., -0.0433, -0.0322,  0.0220]])),\n",
       "             ('encoder.layer.3.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0257, -0.0520,  0.0314,  ...,  0.0132,  0.1173, -0.0809])),\n",
       "             ('encoder.layer.3.attention.output.dense.weight',\n",
       "              tensor([[-0.0011,  0.0025,  0.0338,  ...,  0.0006, -0.0271, -0.0290],\n",
       "                      [ 0.0108, -0.0077,  0.0414,  ...,  0.0160, -0.0203,  0.0140],\n",
       "                      [-0.0812,  0.0118, -0.0060,  ..., -0.0130,  0.0566, -0.0046],\n",
       "                      ...,\n",
       "                      [ 0.0232, -0.0010, -0.0213,  ...,  0.0211, -0.0293,  0.0163],\n",
       "                      [ 0.0115,  0.0443, -0.0003,  ..., -0.0078, -0.0108, -0.0085],\n",
       "                      [-0.0278, -0.0149,  0.0179,  ..., -0.0085,  0.0174,  0.0353]])),\n",
       "             ('encoder.layer.3.attention.output.dense.bias',\n",
       "              tensor([ 0.0416, -0.0173, -0.0270,  ..., -0.0223,  0.0339, -0.0195])),\n",
       "             ('encoder.layer.3.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9902, 0.9795, 0.9771,  ..., 0.9888, 0.9780, 0.9644])),\n",
       "             ('encoder.layer.3.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1794, -0.1175, -0.3018,  ..., -0.0448,  0.1127,  0.0366])),\n",
       "             ('encoder.layer.3.intermediate.dense.weight',\n",
       "              tensor([[ 0.0111,  0.0399, -0.0327,  ..., -0.0500, -0.0241, -0.0334],\n",
       "                      [ 0.0363,  0.0330,  0.0163,  ...,  0.0264, -0.0450, -0.0160],\n",
       "                      [ 0.0340, -0.0640, -0.0149,  ...,  0.0647, -0.1825,  0.0071],\n",
       "                      ...,\n",
       "                      [-0.0294, -0.0330, -0.0367,  ...,  0.0329, -0.0259, -0.0535],\n",
       "                      [ 0.0302, -0.0571, -0.0364,  ..., -0.0058,  0.0171, -0.0274],\n",
       "                      [ 0.0004,  0.0036, -0.0480,  ...,  0.0114, -0.0651, -0.0437]])),\n",
       "             ('encoder.layer.3.intermediate.dense.bias',\n",
       "              tensor([-0.0912, -0.1099, -0.0568,  ..., -0.1026, -0.0178, -0.0859])),\n",
       "             ('encoder.layer.3.output.dense.weight',\n",
       "              tensor([[-0.0192, -0.0285, -0.0228,  ...,  0.0220, -0.0685, -0.0088],\n",
       "                      [ 0.0233, -0.0093, -0.0246,  ..., -0.0055, -0.0484, -0.0692],\n",
       "                      [-0.0765, -0.0780,  0.0205,  ...,  0.0548, -0.0286, -0.0031],\n",
       "                      ...,\n",
       "                      [ 0.0070,  0.0266,  0.0748,  ..., -0.0541, -0.0733,  0.0016],\n",
       "                      [-0.0949, -0.0780, -0.0582,  ..., -0.0207, -0.0069, -0.1099],\n",
       "                      [ 0.1377,  0.0521, -0.0617,  ...,  0.0497, -0.0419,  0.0295]])),\n",
       "             ('encoder.layer.3.output.dense.bias',\n",
       "              tensor([-0.0872,  0.0336,  0.0646,  ...,  0.0421, -0.0818,  0.2703])),\n",
       "             ('encoder.layer.3.output.LayerNorm.weight',\n",
       "              tensor([0.9751, 0.9888, 0.9673,  ..., 0.9712, 0.9663, 0.9473])),\n",
       "             ('encoder.layer.3.output.LayerNorm.bias',\n",
       "              tensor([ 0.0133, -0.0067,  0.1970,  ..., -0.0244, -0.1177, -0.0815])),\n",
       "             ('encoder.layer.4.attention.self.query.weight',\n",
       "              tensor([[-0.0048, -0.0116, -0.0579,  ..., -0.0100, -0.0352, -0.0083],\n",
       "                      [-0.0043,  0.0023,  0.0457,  ..., -0.0598, -0.0140,  0.0690],\n",
       "                      [-0.0833,  0.0049,  0.0118,  ..., -0.0637, -0.0124,  0.0270],\n",
       "                      ...,\n",
       "                      [ 0.0159,  0.0046, -0.0128,  ...,  0.0219,  0.0028,  0.0016],\n",
       "                      [ 0.0182, -0.0273,  0.0395,  ..., -0.0105, -0.0259,  0.0172],\n",
       "                      [ 0.0342, -0.1028, -0.0173,  ..., -0.0395, -0.0361, -0.1447]])),\n",
       "             ('encoder.layer.4.attention.self.query.bias',\n",
       "              tensor([ 0.2111,  0.0043,  0.2280,  ..., -0.2534, -0.1899,  0.2002])),\n",
       "             ('encoder.layer.4.attention.self.key.weight',\n",
       "              tensor([[-0.0172, -0.0750, -0.0478,  ..., -0.0066, -0.0432,  0.0080],\n",
       "                      [-0.0535, -0.0123,  0.0208,  ...,  0.0051, -0.0211,  0.0093],\n",
       "                      [ 0.0136,  0.0290,  0.0385,  ...,  0.0328,  0.0643, -0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0373,  0.0782,  0.0371,  ...,  0.0547,  0.0094,  0.0895],\n",
       "                      [-0.0156, -0.0241, -0.0285,  ..., -0.0204, -0.0425,  0.0641],\n",
       "                      [ 0.0037,  0.0272, -0.0062,  ...,  0.0233, -0.0327,  0.0215]])),\n",
       "             ('encoder.layer.4.attention.self.key.bias',\n",
       "              tensor([-0.0004, -0.0006,  0.0002,  ..., -0.0005,  0.0003, -0.0001])),\n",
       "             ('encoder.layer.4.attention.self.value.weight',\n",
       "              tensor([[-0.0097, -0.0005,  0.0115,  ...,  0.0188,  0.0033,  0.0482],\n",
       "                      [ 0.0213,  0.0596, -0.0212,  ...,  0.0630, -0.0178, -0.0739],\n",
       "                      [ 0.0363,  0.0133,  0.0228,  ...,  0.0497, -0.0365,  0.0641],\n",
       "                      ...,\n",
       "                      [ 0.0101,  0.0669,  0.0218,  ...,  0.0451, -0.0022,  0.0248],\n",
       "                      [ 0.0252,  0.0064,  0.0164,  ...,  0.0272, -0.0034,  0.0626],\n",
       "                      [ 0.0095, -0.1021, -0.0005,  ..., -0.0421,  0.0341,  0.0093]])),\n",
       "             ('encoder.layer.4.attention.self.value.bias',\n",
       "              tensor([-0.0099, -0.0035,  0.0078,  ..., -0.0069,  0.0027, -0.0015])),\n",
       "             ('encoder.layer.4.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0046, -0.0114, -0.0574,  ..., -0.0105, -0.0349, -0.0074],\n",
       "                      [-0.0037,  0.0028,  0.0458,  ..., -0.0598, -0.0141,  0.0685],\n",
       "                      [-0.0831,  0.0045,  0.0123,  ..., -0.0638, -0.0127,  0.0271],\n",
       "                      ...,\n",
       "                      [ 0.0158,  0.0051, -0.0132,  ...,  0.0220,  0.0034,  0.0025],\n",
       "                      [ 0.0187, -0.0276,  0.0391,  ..., -0.0100, -0.0264,  0.0174],\n",
       "                      [ 0.0352, -0.1023, -0.0165,  ..., -0.0395, -0.0354, -0.1445]])),\n",
       "             ('encoder.layer.4.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.2120,  0.0037,  0.2285,  ..., -0.2539, -0.1892,  0.2001])),\n",
       "             ('encoder.layer.4.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0049, -0.0119, -0.0577,  ..., -0.0100, -0.0351, -0.0086],\n",
       "                      [-0.0031,  0.0035,  0.0457,  ..., -0.0600, -0.0143,  0.0695],\n",
       "                      [-0.0836,  0.0054,  0.0124,  ..., -0.0640, -0.0126,  0.0263],\n",
       "                      ...,\n",
       "                      [ 0.0156,  0.0045, -0.0125,  ...,  0.0225,  0.0034,  0.0020],\n",
       "                      [ 0.0185, -0.0269,  0.0385,  ..., -0.0101, -0.0267,  0.0173],\n",
       "                      [ 0.0345, -0.1022, -0.0175,  ..., -0.0390, -0.0352, -0.1449]])),\n",
       "             ('encoder.layer.4.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.2107,  0.0043,  0.2275,  ..., -0.2539, -0.1892,  0.2004])),\n",
       "             ('encoder.layer.4.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0046, -0.0117, -0.0580,  ..., -0.0105, -0.0351, -0.0084],\n",
       "                      [-0.0035,  0.0028,  0.0461,  ..., -0.0598, -0.0145,  0.0692],\n",
       "                      [-0.0834,  0.0047,  0.0119,  ..., -0.0635, -0.0125,  0.0263],\n",
       "                      ...,\n",
       "                      [ 0.0154,  0.0047, -0.0130,  ...,  0.0220,  0.0033,  0.0020],\n",
       "                      [ 0.0184, -0.0277,  0.0391,  ..., -0.0100, -0.0261,  0.0172],\n",
       "                      [ 0.0345, -0.1024, -0.0173,  ..., -0.0391, -0.0355, -0.1445]])),\n",
       "             ('encoder.layer.4.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.2107,  0.0046,  0.2273,  ..., -0.2542, -0.1895,  0.2004])),\n",
       "             ('encoder.layer.4.attention.output.dense.weight',\n",
       "              tensor([[-0.0355, -0.0723, -0.0189,  ..., -0.0075, -0.0106,  0.0657],\n",
       "                      [-0.0439, -0.0138, -0.0081,  ...,  0.0479,  0.0296,  0.0063],\n",
       "                      [ 0.0077,  0.0093, -0.0097,  ..., -0.0410,  0.0453, -0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0056,  0.0078,  0.0048,  ...,  0.0102,  0.0030, -0.0156],\n",
       "                      [-0.0010,  0.0143, -0.0287,  ...,  0.0525,  0.0054, -0.0255],\n",
       "                      [ 0.0171,  0.0429, -0.0140,  ..., -0.0104, -0.0246,  0.0065]])),\n",
       "             ('encoder.layer.4.attention.output.dense.bias',\n",
       "              tensor([-0.0068, -0.0116, -0.0084,  ...,  0.0171, -0.0643, -0.0034])),\n",
       "             ('encoder.layer.4.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9912, 0.9941, 0.9873,  ..., 0.9858, 0.9966, 0.9575])),\n",
       "             ('encoder.layer.4.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1671,  0.0793, -0.2969,  ...,  0.0615,  0.0174,  0.2063])),\n",
       "             ('encoder.layer.4.intermediate.dense.weight',\n",
       "              tensor([[ 0.0023,  0.0547, -0.0293,  ..., -0.0751,  0.0538,  0.0414],\n",
       "                      [-0.0141, -0.0166, -0.0067,  ..., -0.0164, -0.0176, -0.0872],\n",
       "                      [-0.0312,  0.0116,  0.0497,  ..., -0.0912,  0.0204, -0.0461],\n",
       "                      ...,\n",
       "                      [-0.0573, -0.0226,  0.0654,  ...,  0.1235,  0.0429, -0.0715],\n",
       "                      [-0.0049, -0.0798, -0.0870,  ..., -0.0537, -0.0265,  0.0606],\n",
       "                      [-0.0364, -0.0251,  0.0061,  ...,  0.0237,  0.0355, -0.0443]])),\n",
       "             ('encoder.layer.4.intermediate.dense.bias',\n",
       "              tensor([-0.0908, -0.0793, -0.0723,  ..., -0.0704, -0.0524, -0.0552])),\n",
       "             ('encoder.layer.4.output.dense.weight',\n",
       "              tensor([[-0.0193,  0.0604,  0.0076,  ...,  0.0364, -0.0123, -0.0303],\n",
       "                      [-0.0456, -0.0289, -0.0543,  ...,  0.0342, -0.0569,  0.0027],\n",
       "                      [ 0.0162,  0.0829,  0.1042,  ...,  0.0024,  0.0531, -0.0298],\n",
       "                      ...,\n",
       "                      [-0.0370, -0.0462, -0.0655,  ...,  0.1377, -0.0663,  0.0141],\n",
       "                      [ 0.0360, -0.0242,  0.0308,  ...,  0.1028, -0.0171,  0.0116],\n",
       "                      [ 0.0049, -0.0453, -0.0970,  ..., -0.0390,  0.0637, -0.0401]])),\n",
       "             ('encoder.layer.4.output.dense.bias',\n",
       "              tensor([-0.0836, -0.0169,  0.1398,  ...,  0.0107, -0.0988,  0.1644])),\n",
       "             ('encoder.layer.4.output.LayerNorm.weight',\n",
       "              tensor([0.9722, 0.9922, 0.9746,  ..., 0.9766, 0.9956, 0.9648])),\n",
       "             ('encoder.layer.4.output.LayerNorm.bias',\n",
       "              tensor([ 0.0117, -0.1006,  0.2795,  ..., -0.1052, -0.1005, -0.1656])),\n",
       "             ('encoder.layer.5.attention.self.query.weight',\n",
       "              tensor([[ 6.9336e-02, -1.7319e-02,  2.5299e-02,  ..., -1.4160e-02,\n",
       "                        2.2125e-02,  3.6163e-02],\n",
       "                      [ 5.4535e-02, -1.0048e-02,  6.8130e-03,  ...,  1.9932e-03,\n",
       "                        4.3854e-02, -1.3782e-01],\n",
       "                      [-5.3284e-02, -6.5002e-03, -1.8433e-02,  ...,  2.3331e-02,\n",
       "                       -4.6936e-02, -5.9175e-04],\n",
       "                      ...,\n",
       "                      [-1.6708e-02, -1.9165e-02, -7.5195e-02,  ..., -3.3081e-02,\n",
       "                        1.2833e-02,  9.7930e-05],\n",
       "                      [ 4.6753e-02, -8.4045e-02,  2.2797e-02,  ...,  3.2623e-02,\n",
       "                        2.3682e-02, -3.1311e-02],\n",
       "                      [-1.1513e-02,  2.3529e-02,  5.9814e-02,  ...,  1.0101e-02,\n",
       "                        6.6162e-02,  5.5450e-02]])),\n",
       "             ('encoder.layer.5.attention.self.query.bias',\n",
       "              tensor([ 0.0316, -0.0279, -0.0237,  ...,  0.0286,  0.1267, -0.1344])),\n",
       "             ('encoder.layer.5.attention.self.key.weight',\n",
       "              tensor([[ 1.6451e-03,  6.4026e-02, -7.0534e-03,  ..., -3.9032e-02,\n",
       "                        4.5166e-03, -1.3634e-02],\n",
       "                      [ 3.9093e-02, -1.9409e-02, -3.4882e-02,  ...,  3.9185e-02,\n",
       "                        2.8961e-02, -3.7811e-02],\n",
       "                      [ 3.3539e-02, -1.5053e-02, -3.2196e-02,  ..., -1.0950e-01,\n",
       "                        4.4159e-02,  3.1494e-02],\n",
       "                      ...,\n",
       "                      [ 2.9617e-02, -5.1641e-04, -5.7983e-03,  ...,  2.4765e-02,\n",
       "                        6.5552e-02,  4.0741e-02],\n",
       "                      [ 1.7715e-02,  5.5847e-03, -1.2016e-02,  ...,  1.3283e-02,\n",
       "                       -1.6815e-02,  8.7204e-03],\n",
       "                      [-7.2510e-02,  3.5763e-07,  5.9723e-02,  ..., -5.2887e-02,\n",
       "                        1.7258e-02,  3.1490e-03]])),\n",
       "             ('encoder.layer.5.attention.self.key.bias',\n",
       "              tensor([ 0.0017, -0.0008,  0.0012,  ...,  0.0007,  0.0037, -0.0018])),\n",
       "             ('encoder.layer.5.attention.self.value.weight',\n",
       "              tensor([[-0.0194,  0.0536,  0.0140,  ..., -0.0014, -0.0059,  0.0480],\n",
       "                      [-0.0663, -0.0649, -0.0117,  ...,  0.0232, -0.0415, -0.0605],\n",
       "                      [ 0.0382, -0.0432, -0.0295,  ..., -0.0011, -0.0129,  0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0306, -0.0235, -0.0108,  ..., -0.0295, -0.0084, -0.0315],\n",
       "                      [ 0.0569,  0.0293, -0.0966,  ...,  0.0110, -0.0084,  0.0479],\n",
       "                      [-0.0338,  0.0692, -0.0239,  ..., -0.0207, -0.0203,  0.0236]])),\n",
       "             ('encoder.layer.5.attention.self.value.bias',\n",
       "              tensor([ 0.0026, -0.0055, -0.0020,  ...,  0.0086, -0.0052, -0.0094])),\n",
       "             ('encoder.layer.5.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0698, -0.0164,  0.0256,  ..., -0.0146,  0.0225,  0.0367],\n",
       "                      [ 0.0541, -0.0096,  0.0067,  ...,  0.0008,  0.0434, -0.1379],\n",
       "                      [-0.0540, -0.0060, -0.0182,  ...,  0.0228, -0.0476, -0.0009],\n",
       "                      ...,\n",
       "                      [-0.0161, -0.0184, -0.0744,  ..., -0.0338,  0.0129,  0.0002],\n",
       "                      [ 0.0460, -0.0852,  0.0220,  ...,  0.0331,  0.0237, -0.0324],\n",
       "                      [-0.0123,  0.0236,  0.0591,  ...,  0.0096,  0.0658,  0.0553]])),\n",
       "             ('encoder.layer.5.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0321, -0.0285, -0.0243,  ...,  0.0287,  0.1265, -0.1344])),\n",
       "             ('encoder.layer.5.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0698, -0.0167,  0.0247,  ..., -0.0140,  0.0228,  0.0353],\n",
       "                      [ 0.0543, -0.0093,  0.0080,  ...,  0.0004,  0.0429, -0.1367],\n",
       "                      [-0.0536, -0.0062, -0.0196,  ...,  0.0234, -0.0473, -0.0024],\n",
       "                      ...,\n",
       "                      [-0.0169, -0.0192, -0.0752,  ..., -0.0327,  0.0127, -0.0007],\n",
       "                      [ 0.0470, -0.0844,  0.0227,  ...,  0.0327,  0.0240, -0.0320],\n",
       "                      [-0.0116,  0.0242,  0.0592,  ...,  0.0096,  0.0667,  0.0551]])),\n",
       "             ('encoder.layer.5.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0303, -0.0265, -0.0259,  ...,  0.0283,  0.1266, -0.1342])),\n",
       "             ('encoder.layer.5.attention.self.e2e_query.weight',\n",
       "              tensor([[ 6.9214e-02, -1.5823e-02,  2.6535e-02,  ..., -1.5587e-02,\n",
       "                        2.2232e-02,  3.7933e-02],\n",
       "                      [ 5.5023e-02, -9.9258e-03,  6.1760e-03,  ...,  2.0332e-03,\n",
       "                        4.3732e-02, -1.3892e-01],\n",
       "                      [-5.4260e-02, -5.6839e-03, -1.7563e-02,  ...,  2.1820e-02,\n",
       "                       -4.8035e-02, -4.6909e-05],\n",
       "                      ...,\n",
       "                      [-1.6327e-02, -1.8692e-02, -7.4951e-02,  ..., -3.2776e-02,\n",
       "                        1.3390e-02, -1.2636e-03],\n",
       "                      [ 4.6265e-02, -8.5083e-02,  2.1912e-02,  ...,  3.2837e-02,\n",
       "                        2.3361e-02, -3.1342e-02],\n",
       "                      [-1.2390e-02,  2.3590e-02,  5.8899e-02,  ...,  9.2926e-03,\n",
       "                        6.6223e-02,  5.5603e-02]])),\n",
       "             ('encoder.layer.5.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0330, -0.0289, -0.0235,  ...,  0.0284,  0.1261, -0.1342])),\n",
       "             ('encoder.layer.5.attention.output.dense.weight',\n",
       "              tensor([[ 0.0294, -0.0096, -0.0186,  ...,  0.0224,  0.0250, -0.0238],\n",
       "                      [-0.0995,  0.0264,  0.0084,  ..., -0.0531, -0.0200,  0.0451],\n",
       "                      [ 0.0625, -0.0075,  0.0410,  ..., -0.0177, -0.0416, -0.0313],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0521, -0.0145,  ..., -0.0068,  0.0221, -0.0332],\n",
       "                      [ 0.0024,  0.0141,  0.0371,  ...,  0.0058, -0.0707, -0.0075],\n",
       "                      [ 0.0035,  0.0249,  0.0193,  ...,  0.0319, -0.0072, -0.0026]])),\n",
       "             ('encoder.layer.5.attention.output.dense.bias',\n",
       "              tensor([ 0.0069,  0.0101, -0.0751,  ..., -0.0281, -0.0622, -0.0142])),\n",
       "             ('encoder.layer.5.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9907, 0.9985, 0.9985,  ..., 0.9795, 0.9922, 0.9692])),\n",
       "             ('encoder.layer.5.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1210,  0.0566, -0.2688,  ...,  0.0004, -0.0929,  0.0938])),\n",
       "             ('encoder.layer.5.intermediate.dense.weight',\n",
       "              tensor([[-0.0039, -0.0371, -0.0339,  ..., -0.0102, -0.0074, -0.0216],\n",
       "                      [-0.0056, -0.0417, -0.0221,  ..., -0.0152,  0.0682, -0.0400],\n",
       "                      [ 0.0634,  0.0468,  0.0047,  ..., -0.0110, -0.0377,  0.0522],\n",
       "                      ...,\n",
       "                      [-0.0170, -0.0531, -0.0112,  ...,  0.0620, -0.0022, -0.0717],\n",
       "                      [-0.0215, -0.1219, -0.0007,  ...,  0.0276,  0.0666, -0.0353],\n",
       "                      [-0.0626,  0.0396,  0.0501,  ...,  0.0248,  0.0035,  0.0488]])),\n",
       "             ('encoder.layer.5.intermediate.dense.bias',\n",
       "              tensor([-0.0723, -0.1092, -0.1085,  ..., -0.0520, -0.1166, -0.1045])),\n",
       "             ('encoder.layer.5.output.dense.weight',\n",
       "              tensor([[-0.0298,  0.0577, -0.0004,  ..., -0.0181, -0.0363,  0.0270],\n",
       "                      [ 0.0432, -0.0573,  0.0622,  ..., -0.0422, -0.0212, -0.0703],\n",
       "                      [ 0.0057,  0.0140, -0.0124,  ..., -0.0307, -0.0123, -0.0762],\n",
       "                      ...,\n",
       "                      [-0.0681, -0.0643, -0.0325,  ...,  0.0006,  0.0983, -0.0229],\n",
       "                      [-0.0088,  0.0202, -0.0761,  ..., -0.0248, -0.0096,  0.0071],\n",
       "                      [ 0.0018, -0.0226,  0.0839,  ..., -0.0544,  0.0298, -0.0390]])),\n",
       "             ('encoder.layer.5.output.dense.bias',\n",
       "              tensor([-0.1064,  0.0096,  0.1367,  ..., -0.0008, -0.0582,  0.1449])),\n",
       "             ('encoder.layer.5.output.LayerNorm.weight',\n",
       "              tensor([0.9736, 0.9941, 0.9902,  ..., 0.9771, 0.9893, 0.9561])),\n",
       "             ('encoder.layer.5.output.LayerNorm.bias',\n",
       "              tensor([-0.0168, -0.1134,  0.2703,  ..., -0.1185, -0.0455, -0.1478])),\n",
       "             ('encoder.layer.6.attention.self.query.weight',\n",
       "              tensor([[ 0.0191, -0.0274,  0.0555,  ...,  0.0440,  0.0507,  0.0070],\n",
       "                      [-0.0064, -0.0346, -0.0075,  ..., -0.0074, -0.0504, -0.0337],\n",
       "                      [ 0.0385,  0.1499,  0.0865,  ...,  0.0527,  0.0464,  0.0359],\n",
       "                      ...,\n",
       "                      [ 0.0106, -0.0030, -0.1033,  ...,  0.0304,  0.0675,  0.0625],\n",
       "                      [ 0.0259,  0.0175,  0.0193,  ...,  0.0042,  0.0257,  0.0856],\n",
       "                      [-0.0403, -0.0035,  0.0387,  ..., -0.0431,  0.0334, -0.0291]])),\n",
       "             ('encoder.layer.6.attention.self.query.bias',\n",
       "              tensor([-0.3086,  0.0538, -0.3079,  ..., -0.0919,  0.0284,  0.0072])),\n",
       "             ('encoder.layer.6.attention.self.key.weight',\n",
       "              tensor([[ 0.0276, -0.0291,  0.0681,  ..., -0.0124, -0.0306, -0.0020],\n",
       "                      [-0.0172,  0.0290,  0.0191,  ...,  0.0995,  0.0760,  0.0158],\n",
       "                      [ 0.0530,  0.0023,  0.0164,  ..., -0.0137, -0.0324, -0.0408],\n",
       "                      ...,\n",
       "                      [-0.0184,  0.0137,  0.0039,  ..., -0.0318,  0.0146, -0.0016],\n",
       "                      [-0.0743, -0.0652,  0.0002,  ...,  0.0444,  0.0355, -0.0076],\n",
       "                      [ 0.0126,  0.0371,  0.0011,  ...,  0.0569,  0.0309, -0.0071]])),\n",
       "             ('encoder.layer.6.attention.self.key.bias',\n",
       "              tensor([-0.0023,  0.0019,  0.0023,  ...,  0.0041, -0.0017,  0.0008])),\n",
       "             ('encoder.layer.6.attention.self.value.weight',\n",
       "              tensor([[ 0.0092,  0.0158, -0.0072,  ..., -0.0324, -0.0140, -0.0007],\n",
       "                      [-0.0025,  0.0403,  0.0201,  ..., -0.0583,  0.0352,  0.0144],\n",
       "                      [-0.0168, -0.0579,  0.0550,  ...,  0.0447, -0.0038,  0.0248],\n",
       "                      ...,\n",
       "                      [ 0.0170,  0.0394,  0.0008,  ...,  0.0479, -0.0569, -0.0010],\n",
       "                      [ 0.0117, -0.0156, -0.0036,  ..., -0.0306,  0.0201, -0.0155],\n",
       "                      [ 0.0127,  0.0234,  0.0216,  ..., -0.0321,  0.1088,  0.0267]])),\n",
       "             ('encoder.layer.6.attention.self.value.bias',\n",
       "              tensor([-0.0066, -0.0081, -0.0019,  ..., -0.0177, -0.0101, -0.0023])),\n",
       "             ('encoder.layer.6.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0187, -0.0283,  0.0552,  ...,  0.0433,  0.0501,  0.0066],\n",
       "                      [-0.0063, -0.0342, -0.0072,  ..., -0.0065, -0.0499, -0.0332],\n",
       "                      [ 0.0383,  0.1493,  0.0866,  ...,  0.0523,  0.0459,  0.0362],\n",
       "                      ...,\n",
       "                      [ 0.0102, -0.0024, -0.1021,  ...,  0.0304,  0.0673,  0.0632],\n",
       "                      [ 0.0265,  0.0177,  0.0188,  ...,  0.0048,  0.0254,  0.0850],\n",
       "                      [-0.0403, -0.0042,  0.0378,  ..., -0.0421,  0.0338, -0.0295]])),\n",
       "             ('encoder.layer.6.attention.self.w2e_query.bias',\n",
       "              tensor([-0.3079,  0.0541, -0.3071,  ..., -0.0925,  0.0281,  0.0066])),\n",
       "             ('encoder.layer.6.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0200, -0.0279,  0.0560,  ...,  0.0439,  0.0508,  0.0067],\n",
       "                      [-0.0070, -0.0348, -0.0069,  ..., -0.0075, -0.0508, -0.0339],\n",
       "                      [ 0.0385,  0.1497,  0.0869,  ...,  0.0523,  0.0457,  0.0362],\n",
       "                      ...,\n",
       "                      [ 0.0104, -0.0024, -0.1028,  ...,  0.0312,  0.0673,  0.0628],\n",
       "                      [ 0.0265,  0.0187,  0.0196,  ...,  0.0038,  0.0251,  0.0859],\n",
       "                      [-0.0408, -0.0044,  0.0373,  ..., -0.0422,  0.0334, -0.0291]])),\n",
       "             ('encoder.layer.6.attention.self.e2w_query.bias',\n",
       "              tensor([-0.3076,  0.0539, -0.3076,  ..., -0.0927,  0.0282,  0.0061])),\n",
       "             ('encoder.layer.6.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0187, -0.0281,  0.0553,  ...,  0.0435,  0.0511,  0.0070],\n",
       "                      [-0.0067, -0.0346, -0.0072,  ..., -0.0069, -0.0507, -0.0339],\n",
       "                      [ 0.0381,  0.1495,  0.0863,  ...,  0.0523,  0.0461,  0.0363],\n",
       "                      ...,\n",
       "                      [ 0.0111, -0.0024, -0.1033,  ...,  0.0308,  0.0671,  0.0633],\n",
       "                      [ 0.0267,  0.0179,  0.0181,  ...,  0.0049,  0.0250,  0.0853],\n",
       "                      [-0.0414, -0.0045,  0.0383,  ..., -0.0421,  0.0331, -0.0298]])),\n",
       "             ('encoder.layer.6.attention.self.e2e_query.bias',\n",
       "              tensor([-0.3079,  0.0538, -0.3076,  ..., -0.0924,  0.0275,  0.0057])),\n",
       "             ('encoder.layer.6.attention.output.dense.weight',\n",
       "              tensor([[-0.0381,  0.0060, -0.0181,  ...,  0.0412, -0.0340, -0.0060],\n",
       "                      [ 0.0182, -0.0333, -0.0387,  ...,  0.0184,  0.0386, -0.0211],\n",
       "                      [ 0.0490, -0.0558, -0.0347,  ...,  0.0259,  0.0654, -0.0326],\n",
       "                      ...,\n",
       "                      [-0.0152,  0.0487,  0.0014,  ..., -0.0396,  0.0066,  0.0203],\n",
       "                      [-0.0274,  0.0246,  0.0402,  ...,  0.0244, -0.0206, -0.0407],\n",
       "                      [-0.0220,  0.0468,  0.0235,  ...,  0.0573,  0.0059,  0.0195]])),\n",
       "             ('encoder.layer.6.attention.output.dense.bias',\n",
       "              tensor([ 0.0074, -0.0103,  0.0320,  ..., -0.0007, -0.0668,  0.0093])),\n",
       "             ('encoder.layer.6.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9863, 1.0010, 1.0000,  ..., 0.9839, 0.9785, 0.9507])),\n",
       "             ('encoder.layer.6.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1212,  0.0573, -0.2832,  ..., -0.0249, -0.1036,  0.1007])),\n",
       "             ('encoder.layer.6.intermediate.dense.weight',\n",
       "              tensor([[-0.0072, -0.0098, -0.0129,  ..., -0.0032,  0.0383,  0.0657],\n",
       "                      [ 0.0070,  0.0061,  0.0020,  ..., -0.0295,  0.0241, -0.0394],\n",
       "                      [-0.0615, -0.0760, -0.0501,  ...,  0.0775, -0.0170, -0.0793],\n",
       "                      ...,\n",
       "                      [-0.0487, -0.0450,  0.0055,  ..., -0.0157,  0.0779, -0.0472],\n",
       "                      [ 0.0014, -0.0007,  0.0093,  ...,  0.0117,  0.0122,  0.0071],\n",
       "                      [-0.0139, -0.0630,  0.0478,  ..., -0.0173,  0.0205, -0.1285]])),\n",
       "             ('encoder.layer.6.intermediate.dense.bias',\n",
       "              tensor([-0.0801, -0.0938, -0.1008,  ..., -0.0772, -0.0706, -0.1740])),\n",
       "             ('encoder.layer.6.output.dense.weight',\n",
       "              tensor([[ 0.0255, -0.0316, -0.0169,  ...,  0.0354,  0.0260, -0.0037],\n",
       "                      [ 0.0099, -0.0424, -0.0053,  ..., -0.0090,  0.0345,  0.0071],\n",
       "                      [ 0.0258, -0.0111, -0.0872,  ...,  0.0444, -0.0430,  0.0403],\n",
       "                      ...,\n",
       "                      [ 0.0117, -0.0619,  0.0111,  ..., -0.0165,  0.0546, -0.0435],\n",
       "                      [ 0.0295, -0.0011, -0.1118,  ..., -0.0197, -0.0283,  0.0630],\n",
       "                      [ 0.0385,  0.0103,  0.0164,  ..., -0.0274,  0.0252, -0.0165]])),\n",
       "             ('encoder.layer.6.output.dense.bias',\n",
       "              tensor([-0.0682,  0.0310,  0.0649,  ..., -0.0076, -0.0544,  0.0703])),\n",
       "             ('encoder.layer.6.output.LayerNorm.weight',\n",
       "              tensor([0.9810, 0.9917, 1.0010,  ..., 0.9829, 0.9756, 0.9580])),\n",
       "             ('encoder.layer.6.output.LayerNorm.bias',\n",
       "              tensor([-0.0244, -0.1123,  0.1108,  ..., -0.0813, -0.0277, -0.1342])),\n",
       "             ('encoder.layer.7.attention.self.query.weight',\n",
       "              tensor([[ 0.0180, -0.0184,  0.0033,  ...,  0.0238, -0.0409, -0.1068],\n",
       "                      [ 0.0537, -0.0863, -0.0176,  ...,  0.0195, -0.0401,  0.0211],\n",
       "                      [-0.1298,  0.0182,  0.0236,  ...,  0.0137,  0.0829,  0.0031],\n",
       "                      ...,\n",
       "                      [-0.0289,  0.1951,  0.0442,  ...,  0.0558,  0.0096,  0.1268],\n",
       "                      [ 0.0082,  0.0291, -0.0292,  ...,  0.0123,  0.0730,  0.1010],\n",
       "                      [ 0.0155,  0.0116,  0.0266,  ...,  0.0961,  0.1191,  0.0444]])),\n",
       "             ('encoder.layer.7.attention.self.query.bias',\n",
       "              tensor([ 0.0167,  0.0146, -0.0549,  ..., -0.2546,  0.1248, -0.2620])),\n",
       "             ('encoder.layer.7.attention.self.key.weight',\n",
       "              tensor([[ 0.0047, -0.0028, -0.0563,  ..., -0.0108, -0.0041,  0.0657],\n",
       "                      [ 0.0112,  0.0036,  0.0254,  ...,  0.0251, -0.0430,  0.0037],\n",
       "                      [-0.0118, -0.0464, -0.0176,  ...,  0.0202, -0.0370,  0.0291],\n",
       "                      ...,\n",
       "                      [-0.0215,  0.0020,  0.0296,  ..., -0.0529, -0.0782, -0.0426],\n",
       "                      [-0.0684,  0.0409,  0.0163,  ..., -0.0070, -0.0227,  0.0784],\n",
       "                      [ 0.0212,  0.0218,  0.0275,  ...,  0.0087, -0.0120, -0.0377]])),\n",
       "             ('encoder.layer.7.attention.self.key.bias',\n",
       "              tensor([ 0.0003,  0.0002,  0.0002,  ..., -0.0026, -0.0019, -0.0017])),\n",
       "             ('encoder.layer.7.attention.self.value.weight',\n",
       "              tensor([[ 0.0063, -0.0164,  0.0039,  ...,  0.0194, -0.0032, -0.0408],\n",
       "                      [-0.0115, -0.0283, -0.0230,  ..., -0.0106, -0.0347, -0.0465],\n",
       "                      [ 0.0265, -0.0222,  0.0264,  ..., -0.0151,  0.0234,  0.0066],\n",
       "                      ...,\n",
       "                      [-0.0027, -0.0408,  0.0154,  ...,  0.0225,  0.0395,  0.0111],\n",
       "                      [-0.0538, -0.0531, -0.0034,  ...,  0.0465,  0.0203,  0.0214],\n",
       "                      [ 0.0334,  0.0006, -0.0258,  ...,  0.0323, -0.0358, -0.0147]])),\n",
       "             ('encoder.layer.7.attention.self.value.bias',\n",
       "              tensor([ 0.0009,  0.0338, -0.0517,  ...,  0.0139,  0.0031,  0.0053])),\n",
       "             ('encoder.layer.7.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0179, -0.0180,  0.0035,  ...,  0.0231, -0.0400, -0.1065],\n",
       "                      [ 0.0537, -0.0859, -0.0164,  ...,  0.0199, -0.0406,  0.0197],\n",
       "                      [-0.1305,  0.0170,  0.0233,  ...,  0.0154,  0.0837,  0.0026],\n",
       "                      ...,\n",
       "                      [-0.0289,  0.1953,  0.0440,  ...,  0.0560,  0.0093,  0.1260],\n",
       "                      [ 0.0084,  0.0286, -0.0303,  ...,  0.0115,  0.0728,  0.0999],\n",
       "                      [ 0.0158,  0.0118,  0.0263,  ...,  0.0967,  0.1193,  0.0443]])),\n",
       "             ('encoder.layer.7.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0170,  0.0140, -0.0550,  ..., -0.2544,  0.1249, -0.2620])),\n",
       "             ('encoder.layer.7.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0188, -0.0178,  0.0029,  ...,  0.0226, -0.0392, -0.1075],\n",
       "                      [ 0.0550, -0.0868, -0.0160,  ...,  0.0200, -0.0403,  0.0206],\n",
       "                      [-0.1294,  0.0161,  0.0234,  ...,  0.0143,  0.0825,  0.0041],\n",
       "                      ...,\n",
       "                      [-0.0287,  0.1959,  0.0454,  ...,  0.0556,  0.0086,  0.1277],\n",
       "                      [ 0.0082,  0.0293, -0.0297,  ...,  0.0117,  0.0729,  0.1014],\n",
       "                      [ 0.0154,  0.0123,  0.0269,  ...,  0.0966,  0.1184,  0.0452]])),\n",
       "             ('encoder.layer.7.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0174,  0.0142, -0.0547,  ..., -0.2537,  0.1254, -0.2617])),\n",
       "             ('encoder.layer.7.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0184, -0.0181,  0.0037,  ...,  0.0231, -0.0403, -0.1058],\n",
       "                      [ 0.0539, -0.0854, -0.0169,  ...,  0.0185, -0.0410,  0.0210],\n",
       "                      [-0.1307,  0.0161,  0.0227,  ...,  0.0145,  0.0834,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0289,  0.1947,  0.0442,  ...,  0.0558,  0.0093,  0.1274],\n",
       "                      [ 0.0081,  0.0287, -0.0302,  ...,  0.0118,  0.0726,  0.1011],\n",
       "                      [ 0.0153,  0.0117,  0.0261,  ...,  0.0967,  0.1193,  0.0449]])),\n",
       "             ('encoder.layer.7.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0172,  0.0154, -0.0546,  ..., -0.2542,  0.1249, -0.2622])),\n",
       "             ('encoder.layer.7.attention.output.dense.weight',\n",
       "              tensor([[ 0.0017, -0.0088,  0.0260,  ...,  0.0087,  0.0618, -0.0324],\n",
       "                      [-0.0372,  0.0020,  0.0134,  ...,  0.0394, -0.0769,  0.0254],\n",
       "                      [-0.0338, -0.0224,  0.0125,  ...,  0.0452,  0.0380,  0.0360],\n",
       "                      ...,\n",
       "                      [ 0.0468,  0.0038,  0.0503,  ...,  0.0088, -0.0650, -0.0061],\n",
       "                      [ 0.0116,  0.0146,  0.0085,  ..., -0.0078,  0.0175,  0.0344],\n",
       "                      [-0.0111, -0.0271, -0.0085,  ..., -0.0511, -0.0149,  0.0452]])),\n",
       "             ('encoder.layer.7.attention.output.dense.bias',\n",
       "              tensor([-0.0160,  0.0177, -0.0382,  ..., -0.0472, -0.0036,  0.0279])),\n",
       "             ('encoder.layer.7.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9893, 0.9858, 1.0000,  ..., 0.9824, 0.9600, 0.9736])),\n",
       "             ('encoder.layer.7.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0914,  0.0115, -0.4255,  ..., -0.0746, -0.1012, -0.0005])),\n",
       "             ('encoder.layer.7.intermediate.dense.weight',\n",
       "              tensor([[-0.0624,  0.0346,  0.0146,  ...,  0.0605,  0.0029,  0.0340],\n",
       "                      [ 0.0616,  0.0391, -0.0108,  ...,  0.0022, -0.0381, -0.0384],\n",
       "                      [-0.0389, -0.0217, -0.0027,  ...,  0.0152,  0.0168,  0.0369],\n",
       "                      ...,\n",
       "                      [ 0.0085,  0.0289,  0.0034,  ..., -0.0396, -0.0216,  0.0088],\n",
       "                      [-0.0290, -0.1046,  0.0171,  ..., -0.0875, -0.0072, -0.0133],\n",
       "                      [-0.0510, -0.0491,  0.0022,  ...,  0.0431, -0.0265,  0.0333]])),\n",
       "             ('encoder.layer.7.intermediate.dense.bias',\n",
       "              tensor([-0.0927, -0.0911, -0.0093,  ...,  0.0066, -0.0867, -0.0317])),\n",
       "             ('encoder.layer.7.output.dense.weight',\n",
       "              tensor([[-0.0152,  0.0691, -0.0159,  ...,  0.0145, -0.1045, -0.0387],\n",
       "                      [ 0.0349,  0.0783,  0.0063,  ..., -0.0378, -0.0623,  0.0322],\n",
       "                      [-0.0170, -0.0065, -0.0098,  ...,  0.0288,  0.0146,  0.0065],\n",
       "                      ...,\n",
       "                      [-0.0588,  0.0388, -0.0263,  ...,  0.0265, -0.0131,  0.0303],\n",
       "                      [-0.0053, -0.0009, -0.0233,  ...,  0.0274,  0.0177,  0.0128],\n",
       "                      [ 0.0675, -0.0200,  0.0125,  ..., -0.0291,  0.0172, -0.0451]])),\n",
       "             ('encoder.layer.7.output.dense.bias',\n",
       "              tensor([-0.1936,  0.0498,  0.1147,  ..., -0.0159, -0.0419,  0.0591])),\n",
       "             ('encoder.layer.7.output.LayerNorm.weight',\n",
       "              tensor([0.9858, 0.9907, 1.0010,  ..., 0.9839, 0.9771, 0.9512])),\n",
       "             ('encoder.layer.7.output.LayerNorm.bias',\n",
       "              tensor([-0.0314, -0.0963, -0.0043,  ..., -0.0470, -0.0411, -0.1100])),\n",
       "             ('encoder.layer.8.attention.self.query.weight',\n",
       "              tensor([[ 0.0361,  0.0367,  0.0308,  ...,  0.0186,  0.0145,  0.0127],\n",
       "                      [-0.0941, -0.0731,  0.0662,  ..., -0.0357,  0.0025,  0.0132],\n",
       "                      [-0.0665,  0.1041, -0.0329,  ...,  0.0449,  0.0667, -0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0604,  0.0410, -0.0004,  ..., -0.0049,  0.0847, -0.0043],\n",
       "                      [ 0.0089,  0.0823, -0.0327,  ..., -0.1254,  0.0414,  0.0260],\n",
       "                      [-0.0284, -0.0781,  0.0005,  ...,  0.0033, -0.0180, -0.0443]])),\n",
       "             ('encoder.layer.8.attention.self.query.bias',\n",
       "              tensor([-0.0941,  0.1290,  0.0201,  ..., -0.1393, -0.0542,  0.0567])),\n",
       "             ('encoder.layer.8.attention.self.key.weight',\n",
       "              tensor([[-0.0735,  0.0458,  0.0021,  ..., -0.1582,  0.0519, -0.0136],\n",
       "                      [-0.0551, -0.0018,  0.1166,  ...,  0.0133,  0.0207, -0.0320],\n",
       "                      [ 0.0406, -0.0359, -0.0614,  ..., -0.0282,  0.1107, -0.0296],\n",
       "                      ...,\n",
       "                      [-0.0065,  0.0531, -0.0414,  ...,  0.0585, -0.0011,  0.0284],\n",
       "                      [-0.0804,  0.0314, -0.0479,  ...,  0.0148,  0.0390, -0.0273],\n",
       "                      [-0.0110, -0.0840,  0.0368,  ...,  0.0139,  0.0094, -0.0631]])),\n",
       "             ('encoder.layer.8.attention.self.key.bias',\n",
       "              tensor([-0.0003,  0.0035,  0.0031,  ...,  0.0004,  0.0002,  0.0007])),\n",
       "             ('encoder.layer.8.attention.self.value.weight',\n",
       "              tensor([[ 4.3335e-02, -8.9598e-04, -1.5854e-02,  ..., -4.4830e-02,\n",
       "                        3.0090e-02,  3.4088e-02],\n",
       "                      [-5.0110e-02, -3.3875e-02,  1.3647e-03,  ...,  6.5857e-02,\n",
       "                       -8.3399e-04,  4.9255e-02],\n",
       "                      [ 3.1769e-05, -5.4359e-03,  5.1460e-03,  ...,  2.6245e-03,\n",
       "                       -6.6719e-03, -6.8848e-02],\n",
       "                      ...,\n",
       "                      [ 3.9177e-03,  3.1143e-02,  1.5732e-02,  ..., -2.3788e-02,\n",
       "                       -3.9886e-02, -3.3875e-02],\n",
       "                      [ 5.3619e-02, -5.0323e-02,  1.0042e-03,  ..., -2.3956e-03,\n",
       "                       -1.7090e-02, -1.4877e-02],\n",
       "                      [-2.6505e-02, -1.7395e-02,  2.9469e-03,  ...,  3.9978e-02,\n",
       "                       -1.6434e-02, -4.2999e-02]])),\n",
       "             ('encoder.layer.8.attention.self.value.bias',\n",
       "              tensor([-0.0063, -0.0156,  0.0096,  ..., -0.0151, -0.0058, -0.0037])),\n",
       "             ('encoder.layer.8.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0349,  0.0369,  0.0303,  ...,  0.0185,  0.0151,  0.0129],\n",
       "                      [-0.0925, -0.0733,  0.0660,  ..., -0.0357,  0.0026,  0.0125],\n",
       "                      [-0.0667,  0.1038, -0.0334,  ...,  0.0461,  0.0682, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0608,  0.0403, -0.0012,  ..., -0.0059,  0.0852, -0.0041],\n",
       "                      [ 0.0090,  0.0820, -0.0321,  ..., -0.1252,  0.0417,  0.0261],\n",
       "                      [-0.0283, -0.0793,  0.0004,  ...,  0.0032, -0.0191, -0.0436]])),\n",
       "             ('encoder.layer.8.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0931,  0.1290,  0.0206,  ..., -0.1388, -0.0547,  0.0562])),\n",
       "             ('encoder.layer.8.attention.self.e2w_query.weight',\n",
       "              tensor([[ 3.4790e-02,  3.7354e-02,  3.0487e-02,  ...,  1.8051e-02,\n",
       "                        1.4717e-02,  1.2337e-02],\n",
       "                      [-9.3262e-02, -7.2876e-02,  6.6711e-02,  ..., -3.5675e-02,\n",
       "                        2.2869e-03,  1.3359e-02],\n",
       "                      [-6.7200e-02,  1.0388e-01, -3.1891e-02,  ...,  4.6021e-02,\n",
       "                        6.7017e-02, -2.1420e-03],\n",
       "                      ...,\n",
       "                      [ 6.0760e-02,  4.0100e-02, -7.1430e-04,  ..., -4.8409e-03,\n",
       "                        8.5510e-02, -4.3182e-03],\n",
       "                      [ 9.1553e-03,  8.1848e-02, -3.2440e-02,  ..., -1.2549e-01,\n",
       "                        4.2175e-02,  2.6062e-02],\n",
       "                      [-2.8427e-02, -7.9163e-02,  5.0545e-05,  ...,  4.5433e-03,\n",
       "                       -1.9073e-02, -4.3976e-02]])),\n",
       "             ('encoder.layer.8.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0938,  0.1283,  0.0197,  ..., -0.1394, -0.0540,  0.0552])),\n",
       "             ('encoder.layer.8.attention.self.e2e_query.weight',\n",
       "              tensor([[ 3.5217e-02,  3.7323e-02,  3.1204e-02,  ...,  1.7212e-02,\n",
       "                        1.5091e-02,  1.2726e-02],\n",
       "                      [-9.3201e-02, -7.2754e-02,  6.6345e-02,  ..., -3.6377e-02,\n",
       "                        2.1038e-03,  1.2650e-02],\n",
       "                      [-6.6284e-02,  1.0449e-01, -3.2288e-02,  ...,  4.4647e-02,\n",
       "                        6.7871e-02, -2.3384e-03],\n",
       "                      ...,\n",
       "                      [ 6.1554e-02,  4.1412e-02,  8.6606e-05,  ..., -6.2523e-03,\n",
       "                        8.5266e-02, -3.9482e-03],\n",
       "                      [ 9.3842e-03,  8.0994e-02, -3.3417e-02,  ..., -1.2512e-01,\n",
       "                        4.2206e-02,  2.6840e-02],\n",
       "                      [-2.7847e-02, -7.8064e-02,  1.8768e-03,  ...,  3.0098e-03,\n",
       "                       -1.9272e-02, -4.3823e-02]])),\n",
       "             ('encoder.layer.8.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0930,  0.1287,  0.0204,  ..., -0.1381, -0.0539,  0.0567])),\n",
       "             ('encoder.layer.8.attention.output.dense.weight',\n",
       "              tensor([[ 0.0301, -0.0038, -0.0123,  ..., -0.0034,  0.0063, -0.0015],\n",
       "                      [ 0.0007, -0.0154,  0.0310,  ..., -0.0021,  0.0381,  0.0576],\n",
       "                      [ 0.0495, -0.0363,  0.0229,  ...,  0.0416, -0.0346,  0.0092],\n",
       "                      ...,\n",
       "                      [-0.0308,  0.0274, -0.0140,  ...,  0.0361, -0.0330, -0.0280],\n",
       "                      [-0.0482,  0.0053, -0.0289,  ...,  0.0051,  0.0046,  0.0027],\n",
       "                      [ 0.0277, -0.0154,  0.0312,  ...,  0.0210,  0.0024,  0.0466]])),\n",
       "             ('encoder.layer.8.attention.output.dense.bias',\n",
       "              tensor([-0.0499,  0.0063,  0.0057,  ...,  0.0091, -0.0970,  0.0553])),\n",
       "             ('encoder.layer.8.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9834, 0.9985, 1.0029,  ..., 0.9917, 0.9951, 0.9751])),\n",
       "             ('encoder.layer.8.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1709,  0.0128, -0.5000,  ..., -0.0110, -0.1101, -0.0140])),\n",
       "             ('encoder.layer.8.intermediate.dense.weight',\n",
       "              tensor([[-0.0096, -0.0001,  0.0202,  ..., -0.0604,  0.0108, -0.0806],\n",
       "                      [ 0.0285, -0.0735,  0.0402,  ..., -0.1035,  0.0449, -0.0235],\n",
       "                      [ 0.0339,  0.0986,  0.0405,  ...,  0.0236,  0.0101,  0.0095],\n",
       "                      ...,\n",
       "                      [ 0.0440, -0.0218,  0.0360,  ..., -0.0203, -0.0192,  0.0359],\n",
       "                      [ 0.0903,  0.0180,  0.0093,  ...,  0.0139,  0.0549, -0.0726],\n",
       "                      [ 0.0332,  0.0066,  0.0176,  ..., -0.0273, -0.0696, -0.0132]])),\n",
       "             ('encoder.layer.8.intermediate.dense.bias',\n",
       "              tensor([-0.0611, -0.0988, -0.0522,  ..., -0.0769, -0.0559, -0.0340])),\n",
       "             ('encoder.layer.8.output.dense.weight',\n",
       "              tensor([[ 0.0076,  0.0093,  0.0085,  ..., -0.0269,  0.0693,  0.0105],\n",
       "                      [-0.0330,  0.0174,  0.0649,  ...,  0.0183,  0.0104,  0.0277],\n",
       "                      [ 0.0359, -0.0177,  0.0201,  ...,  0.0377, -0.0028, -0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0735, -0.0575,  0.0309,  ...,  0.0532, -0.0142,  0.0227],\n",
       "                      [-0.0140, -0.0083,  0.0144,  ..., -0.0607, -0.0089, -0.0031],\n",
       "                      [ 0.0225,  0.0212,  0.0240,  ...,  0.0245,  0.0058,  0.0251]])),\n",
       "             ('encoder.layer.8.output.dense.bias',\n",
       "              tensor([-1.6809e-01, -6.2525e-05,  6.0669e-02,  ..., -4.1077e-02,\n",
       "                      -5.0415e-02, -5.7251e-02])),\n",
       "             ('encoder.layer.8.output.LayerNorm.weight',\n",
       "              tensor([0.9722, 0.9976, 1.0029,  ..., 0.9771, 0.9775, 0.9531])),\n",
       "             ('encoder.layer.8.output.LayerNorm.bias',\n",
       "              tensor([ 0.0209, -0.1112, -0.0228,  ..., -0.0834, -0.0246, -0.0840])),\n",
       "             ('encoder.layer.9.attention.self.query.weight',\n",
       "              tensor([[ 0.0047, -0.0861, -0.0393,  ..., -0.0264,  0.0690, -0.0676],\n",
       "                      [ 0.0204, -0.0374, -0.0083,  ..., -0.0327,  0.0463,  0.0606],\n",
       "                      [ 0.0182,  0.0224,  0.0157,  ..., -0.0356,  0.0154, -0.0265],\n",
       "                      ...,\n",
       "                      [ 0.0576, -0.0253, -0.0017,  ..., -0.0114, -0.0648, -0.0132],\n",
       "                      [ 0.0480,  0.0103, -0.0787,  ...,  0.0330,  0.0260,  0.0237],\n",
       "                      [-0.0188, -0.0179, -0.0397,  ..., -0.0514, -0.0354, -0.0428]])),\n",
       "             ('encoder.layer.9.attention.self.query.bias',\n",
       "              tensor([-0.0111, -0.0300, -0.0628,  ...,  0.0026, -0.0579,  0.0747])),\n",
       "             ('encoder.layer.9.attention.self.key.weight',\n",
       "              tensor([[-0.0364,  0.0486,  0.0022,  ..., -0.0111, -0.0300, -0.0414],\n",
       "                      [ 0.0354, -0.0101, -0.0049,  ..., -0.0300, -0.0209, -0.0419],\n",
       "                      [ 0.0357, -0.0505,  0.0702,  ..., -0.0667, -0.0244,  0.0155],\n",
       "                      ...,\n",
       "                      [-0.0154,  0.0128,  0.0031,  ...,  0.0080,  0.0555,  0.0066],\n",
       "                      [ 0.0428,  0.0311, -0.0116,  ...,  0.0165,  0.0322,  0.0061],\n",
       "                      [-0.0377,  0.0069, -0.0308,  ...,  0.0875, -0.0026,  0.0983]])),\n",
       "             ('encoder.layer.9.attention.self.key.bias',\n",
       "              tensor([-0.0005, -0.0013, -0.0023,  ..., -0.0035, -0.0022,  0.0011])),\n",
       "             ('encoder.layer.9.attention.self.value.weight',\n",
       "              tensor([[-2.4017e-02,  1.8806e-03,  1.0345e-02,  ..., -2.1011e-02,\n",
       "                       -5.4016e-02,  2.7237e-02],\n",
       "                      [-2.2629e-02, -3.0624e-02, -8.0299e-04,  ..., -5.2979e-02,\n",
       "                       -6.3438e-03,  5.5695e-02],\n",
       "                      [ 7.9870e-06,  5.6213e-02,  1.1536e-02,  ...,  2.3621e-02,\n",
       "                        2.5482e-02,  1.1681e-02],\n",
       "                      ...,\n",
       "                      [-5.1910e-02,  3.4698e-02,  1.3283e-02,  ...,  5.7434e-02,\n",
       "                        5.5298e-02, -7.7133e-03],\n",
       "                      [ 5.4138e-02,  1.6342e-02, -1.1238e-02,  ..., -3.4210e-02,\n",
       "                       -3.8757e-02, -3.1525e-02],\n",
       "                      [-3.3844e-02,  1.5198e-02,  8.9569e-03,  ...,  2.3193e-02,\n",
       "                        5.0964e-03, -2.0767e-02]])),\n",
       "             ('encoder.layer.9.attention.self.value.bias',\n",
       "              tensor([ 0.0007, -0.0072,  0.0018,  ...,  0.0329, -0.0018,  0.0011])),\n",
       "             ('encoder.layer.9.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0045, -0.0848, -0.0388,  ..., -0.0278,  0.0671, -0.0679],\n",
       "                      [ 0.0199, -0.0372, -0.0087,  ..., -0.0325,  0.0450,  0.0597],\n",
       "                      [ 0.0174,  0.0214,  0.0147,  ..., -0.0370,  0.0122, -0.0272],\n",
       "                      ...,\n",
       "                      [ 0.0580, -0.0251, -0.0015,  ..., -0.0116, -0.0648, -0.0137],\n",
       "                      [ 0.0475,  0.0108, -0.0788,  ...,  0.0320,  0.0252,  0.0227],\n",
       "                      [-0.0195, -0.0175, -0.0387,  ..., -0.0520, -0.0356, -0.0430]])),\n",
       "             ('encoder.layer.9.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0121, -0.0300, -0.0616,  ...,  0.0025, -0.0583,  0.0737])),\n",
       "             ('encoder.layer.9.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0052, -0.0845, -0.0396,  ..., -0.0283,  0.0671, -0.0679],\n",
       "                      [ 0.0202, -0.0378, -0.0086,  ..., -0.0321,  0.0452,  0.0594],\n",
       "                      [ 0.0189,  0.0217,  0.0151,  ..., -0.0365,  0.0130, -0.0270],\n",
       "                      ...,\n",
       "                      [ 0.0579, -0.0245, -0.0014,  ..., -0.0116, -0.0649, -0.0139],\n",
       "                      [ 0.0478,  0.0097, -0.0780,  ...,  0.0320,  0.0245,  0.0234],\n",
       "                      [-0.0189, -0.0184, -0.0388,  ..., -0.0521, -0.0356, -0.0436]])),\n",
       "             ('encoder.layer.9.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0106, -0.0292, -0.0615,  ...,  0.0026, -0.0576,  0.0732])),\n",
       "             ('encoder.layer.9.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0046, -0.0849, -0.0386,  ..., -0.0268,  0.0687, -0.0682],\n",
       "                      [ 0.0202, -0.0375, -0.0084,  ..., -0.0314,  0.0467,  0.0596],\n",
       "                      [ 0.0174,  0.0221,  0.0153,  ..., -0.0359,  0.0133, -0.0269],\n",
       "                      ...,\n",
       "                      [ 0.0572, -0.0249, -0.0010,  ..., -0.0111, -0.0641, -0.0143],\n",
       "                      [ 0.0482,  0.0104, -0.0789,  ...,  0.0316,  0.0247,  0.0224],\n",
       "                      [-0.0185, -0.0171, -0.0389,  ..., -0.0527, -0.0363, -0.0433]])),\n",
       "             ('encoder.layer.9.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0122, -0.0296, -0.0621,  ...,  0.0024, -0.0570,  0.0737])),\n",
       "             ('encoder.layer.9.attention.output.dense.weight',\n",
       "              tensor([[-0.0206, -0.0256,  0.0105,  ...,  0.0207, -0.0112, -0.0178],\n",
       "                      [ 0.0020, -0.0390,  0.0025,  ..., -0.0450,  0.0156,  0.0210],\n",
       "                      [ 0.0354, -0.0695, -0.0001,  ..., -0.0131,  0.0377, -0.0039],\n",
       "                      ...,\n",
       "                      [ 0.0089,  0.0145,  0.0290,  ..., -0.0184,  0.0031,  0.0006],\n",
       "                      [-0.0133, -0.0091, -0.0081,  ..., -0.0784, -0.0182,  0.0555],\n",
       "                      [ 0.0278,  0.0114,  0.0279,  ..., -0.0142, -0.0116,  0.0005]])),\n",
       "             ('encoder.layer.9.attention.output.dense.bias',\n",
       "              tensor([ 0.0467,  0.0035, -0.1740,  ...,  0.0528, -0.0490,  0.0050])),\n",
       "             ('encoder.layer.9.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9824, 0.9932, 1.0029,  ..., 0.9824, 0.9902, 0.9868])),\n",
       "             ('encoder.layer.9.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1801, -0.0290, -0.4668,  ..., -0.0483, -0.1160, -0.0282])),\n",
       "             ('encoder.layer.9.intermediate.dense.weight',\n",
       "              tensor([[ 0.0259,  0.0051, -0.0152,  ...,  0.0247,  0.0222,  0.0019],\n",
       "                      [ 0.0083, -0.0007,  0.0180,  ...,  0.0797,  0.0246,  0.0166],\n",
       "                      [ 0.0482, -0.0523,  0.0129,  ...,  0.0628, -0.0767,  0.0656],\n",
       "                      ...,\n",
       "                      [ 0.0072, -0.0344,  0.0137,  ...,  0.0684,  0.0027, -0.0212],\n",
       "                      [ 0.0132, -0.0570,  0.0078,  ...,  0.0019, -0.0058, -0.0106],\n",
       "                      [-0.0213, -0.0305,  0.0117,  ...,  0.0126,  0.0490,  0.0235]])),\n",
       "             ('encoder.layer.9.intermediate.dense.bias',\n",
       "              tensor([-0.0534, -0.0788, -0.0242,  ..., -0.0482, -0.0435, -0.1004])),\n",
       "             ('encoder.layer.9.output.dense.weight',\n",
       "              tensor([[-0.0023,  0.0223, -0.0161,  ..., -0.0073,  0.0211,  0.0042],\n",
       "                      [ 0.0326,  0.0782, -0.0155,  ..., -0.0213, -0.0086, -0.0095],\n",
       "                      [ 0.0139,  0.0043,  0.0180,  ...,  0.0142, -0.0035, -0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0117,  0.0909, -0.0475,  ..., -0.0516,  0.0205,  0.0145],\n",
       "                      [ 0.0332,  0.0456, -0.0470,  ...,  0.0441,  0.0305, -0.0034],\n",
       "                      [-0.0382, -0.0254, -0.0489,  ..., -0.0116,  0.0197,  0.0037]])),\n",
       "             ('encoder.layer.9.output.dense.bias',\n",
       "              tensor([-0.1371,  0.0667, -0.0892,  ..., -0.0439, -0.0769, -0.0242])),\n",
       "             ('encoder.layer.9.output.LayerNorm.weight',\n",
       "              tensor([0.9731, 0.9897, 1.0029,  ..., 0.9692, 0.9814, 0.9736])),\n",
       "             ('encoder.layer.9.output.LayerNorm.bias',\n",
       "              tensor([ 0.0326, -0.0622, -0.2124,  ..., -0.0582, -0.0128, -0.0451])),\n",
       "             ('encoder.layer.10.attention.self.query.weight',\n",
       "              tensor([[ 0.0644,  0.0335, -0.0018,  ..., -0.1428,  0.0443,  0.0173],\n",
       "                      [ 0.0671, -0.1069, -0.0096,  ...,  0.0784,  0.0021, -0.0284],\n",
       "                      [-0.0356, -0.0140, -0.0359,  ...,  0.0768,  0.0930, -0.0367],\n",
       "                      ...,\n",
       "                      [ 0.0334,  0.0389,  0.2369,  ...,  0.0899, -0.0646,  0.0598],\n",
       "                      [ 0.0208,  0.0157,  0.0611,  ...,  0.1124,  0.0530, -0.0475],\n",
       "                      [ 0.0268, -0.0213,  0.0464,  ...,  0.0500,  0.0579, -0.0198]])),\n",
       "             ('encoder.layer.10.attention.self.query.bias',\n",
       "              tensor([ 0.0239,  0.0311, -0.0158,  ...,  0.0281,  0.0562,  0.0006])),\n",
       "             ('encoder.layer.10.attention.self.key.weight',\n",
       "              tensor([[-0.0143,  0.0388,  0.0549,  ..., -0.0165, -0.0266, -0.0473],\n",
       "                      [-0.0984, -0.0994,  0.0235,  ..., -0.0699,  0.0006, -0.0057],\n",
       "                      [-0.0342,  0.0716, -0.0076,  ..., -0.1048, -0.0014, -0.0561],\n",
       "                      ...,\n",
       "                      [ 0.0174, -0.0100,  0.2573,  ..., -0.0773, -0.0313,  0.0265],\n",
       "                      [ 0.0088, -0.0074,  0.0808,  ..., -0.0076,  0.0013, -0.0124],\n",
       "                      [-0.0298, -0.0457,  0.0265,  ...,  0.0643, -0.0070, -0.0244]])),\n",
       "             ('encoder.layer.10.attention.self.key.bias',\n",
       "              tensor([-0.0005, -0.0061, -0.0002,  ...,  0.0015,  0.0097,  0.0025])),\n",
       "             ('encoder.layer.10.attention.self.value.weight',\n",
       "              tensor([[ 0.0090,  0.0418,  0.0110,  ..., -0.0018, -0.0263, -0.0288],\n",
       "                      [ 0.0221, -0.0057, -0.0106,  ..., -0.0128, -0.0042, -0.0188],\n",
       "                      [ 0.0357,  0.0496,  0.0039,  ..., -0.0046,  0.0283, -0.0185],\n",
       "                      ...,\n",
       "                      [-0.0145,  0.0077, -0.0007,  ..., -0.0036,  0.0077, -0.0378],\n",
       "                      [ 0.0142,  0.0152, -0.0091,  ...,  0.0172, -0.0399,  0.0161],\n",
       "                      [-0.0256, -0.0177, -0.0148,  ..., -0.0241,  0.0066, -0.0161]])),\n",
       "             ('encoder.layer.10.attention.self.value.bias',\n",
       "              tensor([ 0.0215,  0.0107, -0.0329,  ...,  0.0200, -0.0135,  0.0496])),\n",
       "             ('encoder.layer.10.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0638,  0.0338, -0.0017,  ..., -0.1443,  0.0432,  0.0177],\n",
       "                      [ 0.0681, -0.1075, -0.0097,  ...,  0.0785,  0.0022, -0.0283],\n",
       "                      [-0.0346, -0.0132, -0.0355,  ...,  0.0769,  0.0928, -0.0376],\n",
       "                      ...,\n",
       "                      [ 0.0326,  0.0385,  0.2368,  ...,  0.0893, -0.0630,  0.0597],\n",
       "                      [ 0.0210,  0.0160,  0.0619,  ...,  0.1121,  0.0530, -0.0469],\n",
       "                      [ 0.0266, -0.0204,  0.0468,  ...,  0.0490,  0.0565, -0.0208]])),\n",
       "             ('encoder.layer.10.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0237,  0.0307, -0.0164,  ...,  0.0283,  0.0554,  0.0008])),\n",
       "             ('encoder.layer.10.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0642,  0.0331, -0.0017,  ..., -0.1437,  0.0441,  0.0170],\n",
       "                      [ 0.0676, -0.1065, -0.0102,  ...,  0.0773,  0.0013, -0.0279],\n",
       "                      [-0.0352, -0.0128, -0.0363,  ...,  0.0768,  0.0924, -0.0371],\n",
       "                      ...,\n",
       "                      [ 0.0329,  0.0383,  0.2355,  ...,  0.0895, -0.0639,  0.0596],\n",
       "                      [ 0.0210,  0.0167,  0.0614,  ...,  0.1125,  0.0521, -0.0467],\n",
       "                      [ 0.0260, -0.0203,  0.0476,  ...,  0.0497,  0.0566, -0.0208]])),\n",
       "             ('encoder.layer.10.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0233,  0.0316, -0.0161,  ...,  0.0293,  0.0561, -0.0003])),\n",
       "             ('encoder.layer.10.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0647,  0.0336, -0.0015,  ..., -0.1436,  0.0435,  0.0172],\n",
       "                      [ 0.0673, -0.1072, -0.0100,  ...,  0.0780,  0.0012, -0.0278],\n",
       "                      [-0.0352, -0.0132, -0.0357,  ...,  0.0768,  0.0923, -0.0375],\n",
       "                      ...,\n",
       "                      [ 0.0326,  0.0376,  0.2365,  ...,  0.0903, -0.0630,  0.0592],\n",
       "                      [ 0.0203,  0.0158,  0.0618,  ...,  0.1125,  0.0530, -0.0471],\n",
       "                      [ 0.0268, -0.0204,  0.0471,  ...,  0.0483,  0.0559, -0.0202]])),\n",
       "             ('encoder.layer.10.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0235,  0.0310, -0.0162,  ...,  0.0284,  0.0551,  0.0006])),\n",
       "             ('encoder.layer.10.attention.output.dense.weight',\n",
       "              tensor([[ 0.0050,  0.0538, -0.0093,  ...,  0.0128,  0.0360,  0.0190],\n",
       "                      [-0.0212, -0.0050, -0.0149,  ..., -0.0418,  0.0244, -0.0164],\n",
       "                      [ 0.0311,  0.0218, -0.1362,  ..., -0.0697, -0.0639, -0.0096],\n",
       "                      ...,\n",
       "                      [ 0.0049, -0.0076, -0.0358,  ..., -0.0119, -0.0052, -0.0197],\n",
       "                      [ 0.0321, -0.0038,  0.0465,  ..., -0.0152,  0.0022,  0.0145],\n",
       "                      [ 0.0531, -0.0061,  0.0050,  ..., -0.0076, -0.0168, -0.0287]])),\n",
       "             ('encoder.layer.10.attention.output.dense.bias',\n",
       "              tensor([-0.0041,  0.0633,  0.0535,  ...,  0.0352, -0.0610,  0.0742])),\n",
       "             ('encoder.layer.10.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9707, 0.9912, 1.0039,  ..., 0.9863, 0.9888, 0.9849])),\n",
       "             ('encoder.layer.10.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1819, -0.0036, -0.4734,  ..., -0.0690, -0.1519, -0.0396])),\n",
       "             ('encoder.layer.10.intermediate.dense.weight',\n",
       "              tensor([[-0.0186, -0.0082,  0.0063,  ..., -0.0164, -0.0109, -0.0724],\n",
       "                      [ 0.0105,  0.0210,  0.0364,  ...,  0.0605,  0.0287, -0.0074],\n",
       "                      [ 0.0016, -0.0150,  0.0281,  ...,  0.0309,  0.0354,  0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0212,  0.0146,  0.0211,  ...,  0.0746, -0.0360,  0.0043],\n",
       "                      [ 0.0523, -0.0187,  0.0048,  ...,  0.0298,  0.0580, -0.0283],\n",
       "                      [-0.0114, -0.0151, -0.0091,  ...,  0.0195, -0.0005, -0.0273]])),\n",
       "             ('encoder.layer.10.intermediate.dense.bias',\n",
       "              tensor([-0.0578, -0.0499, -0.0149,  ..., -0.0923, -0.0911, -0.0138])),\n",
       "             ('encoder.layer.10.output.dense.weight',\n",
       "              tensor([[-0.0207,  0.0032, -0.0034,  ...,  0.0599,  0.0145, -0.0527],\n",
       "                      [-0.0373,  0.0092,  0.0012,  ...,  0.0115, -0.0237, -0.0471],\n",
       "                      [ 0.0045, -0.0083,  0.0103,  ...,  0.0019, -0.0099, -0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0113, -0.0172, -0.0156,  ...,  0.0204, -0.0020, -0.0076],\n",
       "                      [-0.0198, -0.0941, -0.0469,  ...,  0.0211,  0.0086, -0.0479],\n",
       "                      [-0.0275, -0.0004, -0.0414,  ...,  0.0021,  0.0263,  0.0101]])),\n",
       "             ('encoder.layer.10.output.dense.bias',\n",
       "              tensor([-0.2124,  0.0537,  0.0872,  ..., -0.0431,  0.0078, -0.1136])),\n",
       "             ('encoder.layer.10.output.LayerNorm.weight',\n",
       "              tensor([0.9766, 0.9888, 1.0049,  ..., 0.9775, 0.9741, 0.9829])),\n",
       "             ('encoder.layer.10.output.LayerNorm.bias',\n",
       "              tensor([ 0.0439, -0.0737, -0.2052,  ..., -0.0423,  0.0371, -0.0467])),\n",
       "             ('encoder.layer.11.attention.self.query.weight',\n",
       "              tensor([[ 0.0356, -0.0126,  0.0254,  ...,  0.0174,  0.0880,  0.0403],\n",
       "                      [ 0.0723, -0.0446, -0.0037,  ..., -0.0252,  0.0167,  0.0327],\n",
       "                      [ 0.0517, -0.0481, -0.2144,  ...,  0.0500,  0.0699, -0.0316],\n",
       "                      ...,\n",
       "                      [ 0.0937, -0.0227,  0.0844,  ..., -0.0406,  0.0629, -0.0690],\n",
       "                      [-0.0662, -0.0240, -0.0636,  ..., -0.0201, -0.0359, -0.0837],\n",
       "                      [ 0.0381,  0.0006, -0.0354,  ..., -0.0559, -0.0938, -0.0453]])),\n",
       "             ('encoder.layer.11.attention.self.query.bias',\n",
       "              tensor([0.0094, 0.0302, 0.0473,  ..., 0.1013, 0.0177, 0.0239])),\n",
       "             ('encoder.layer.11.attention.self.key.weight',\n",
       "              tensor([[-0.0916, -0.0083,  0.0753,  ...,  0.0046, -0.0249, -0.0119],\n",
       "                      [-0.0618, -0.0132, -0.0291,  ...,  0.0228, -0.0328,  0.0745],\n",
       "                      [-0.1138,  0.0077, -0.2325,  ..., -0.0596, -0.0071,  0.0070],\n",
       "                      ...,\n",
       "                      [-0.0389, -0.0297,  0.1083,  ..., -0.0745, -0.0312, -0.0233],\n",
       "                      [-0.0141,  0.0063, -0.0655,  ..., -0.1009,  0.0087,  0.0084],\n",
       "                      [ 0.0117, -0.0765, -0.0356,  ..., -0.0415, -0.0187, -0.0171]])),\n",
       "             ('encoder.layer.11.attention.self.key.bias',\n",
       "              tensor([ 7.4625e-05, -4.0674e-04, -6.3019e-03,  ..., -1.1024e-03,\n",
       "                      -6.7425e-04,  4.9324e-03])),\n",
       "             ('encoder.layer.11.attention.self.value.weight',\n",
       "              tensor([[ 0.0106, -0.0299,  0.0033,  ...,  0.0638,  0.0018, -0.0023],\n",
       "                      [ 0.0240, -0.0329, -0.0044,  ..., -0.0777, -0.0322,  0.0486],\n",
       "                      [-0.0456, -0.0125,  0.0268,  ..., -0.0859, -0.0298,  0.0203],\n",
       "                      ...,\n",
       "                      [-0.0046,  0.0456, -0.0011,  ..., -0.0005, -0.0474, -0.0057],\n",
       "                      [ 0.0433,  0.0277,  0.0012,  ...,  0.0469,  0.0358, -0.0773],\n",
       "                      [-0.0119, -0.0010,  0.0015,  ..., -0.0457,  0.0319, -0.0246]])),\n",
       "             ('encoder.layer.11.attention.self.value.bias',\n",
       "              tensor([-0.0104,  0.0126,  0.0059,  ..., -0.0277,  0.0114, -0.0346])),\n",
       "             ('encoder.layer.11.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0359, -0.0121,  0.0254,  ...,  0.0178,  0.0880,  0.0397],\n",
       "                      [ 0.0717, -0.0450, -0.0035,  ..., -0.0246,  0.0176,  0.0327],\n",
       "                      [ 0.0521, -0.0477, -0.2135,  ...,  0.0506,  0.0692, -0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0936, -0.0231,  0.0848,  ..., -0.0409,  0.0634, -0.0677],\n",
       "                      [-0.0669, -0.0238, -0.0641,  ..., -0.0196, -0.0348, -0.0831],\n",
       "                      [ 0.0378,  0.0003, -0.0349,  ..., -0.0559, -0.0935, -0.0464]])),\n",
       "             ('encoder.layer.11.attention.self.w2e_query.bias',\n",
       "              tensor([0.0098, 0.0298, 0.0466,  ..., 0.1000, 0.0183, 0.0238])),\n",
       "             ('encoder.layer.11.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0358, -0.0116,  0.0256,  ...,  0.0172,  0.0873,  0.0394],\n",
       "                      [ 0.0724, -0.0452, -0.0031,  ..., -0.0241,  0.0184,  0.0326],\n",
       "                      [ 0.0527, -0.0477, -0.2147,  ...,  0.0501,  0.0696, -0.0308],\n",
       "                      ...,\n",
       "                      [ 0.0942, -0.0238,  0.0838,  ..., -0.0404,  0.0634, -0.0683],\n",
       "                      [-0.0665, -0.0238, -0.0635,  ..., -0.0198, -0.0349, -0.0836],\n",
       "                      [ 0.0370,  0.0003, -0.0353,  ..., -0.0559, -0.0942, -0.0464]])),\n",
       "             ('encoder.layer.11.attention.self.e2w_query.bias',\n",
       "              tensor([0.0101, 0.0292, 0.0473,  ..., 0.1016, 0.0172, 0.0239])),\n",
       "             ('encoder.layer.11.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0356, -0.0126,  0.0254,  ...,  0.0177,  0.0877,  0.0398],\n",
       "                      [ 0.0726, -0.0440, -0.0030,  ..., -0.0251,  0.0172,  0.0331],\n",
       "                      [ 0.0518, -0.0483, -0.2144,  ...,  0.0499,  0.0697, -0.0311],\n",
       "                      ...,\n",
       "                      [ 0.0936, -0.0242,  0.0839,  ..., -0.0399,  0.0635, -0.0689],\n",
       "                      [-0.0664, -0.0243, -0.0637,  ..., -0.0202, -0.0356, -0.0830],\n",
       "                      [ 0.0378,  0.0004, -0.0357,  ..., -0.0564, -0.0938, -0.0462]])),\n",
       "             ('encoder.layer.11.attention.self.e2e_query.bias',\n",
       "              tensor([0.0096, 0.0302, 0.0469,  ..., 0.1010, 0.0174, 0.0241])),\n",
       "             ('encoder.layer.11.attention.output.dense.weight',\n",
       "              tensor([[-0.0051, -0.0264,  0.0219,  ...,  0.0088,  0.0077, -0.0275],\n",
       "                      [ 0.0444,  0.0070,  0.0266,  ...,  0.0099, -0.0033,  0.0344],\n",
       "                      [-0.0247,  0.0090, -0.0312,  ...,  0.0065,  0.0184, -0.0219],\n",
       "                      ...,\n",
       "                      [-0.0105, -0.0004,  0.0625,  ...,  0.0244,  0.0271, -0.0177],\n",
       "                      [ 0.0043,  0.0189,  0.0361,  ...,  0.0089,  0.0311, -0.0044],\n",
       "                      [ 0.0236, -0.0564, -0.0013,  ..., -0.0180, -0.0252, -0.0247]])),\n",
       "             ('encoder.layer.11.attention.output.dense.bias',\n",
       "              tensor([-0.0502,  0.1439, -0.1538,  ...,  0.0583, -0.0470,  0.0363])),\n",
       "             ('encoder.layer.11.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9780, 0.9946, 1.0039,  ..., 0.9917, 0.9917, 0.9766])),\n",
       "             ('encoder.layer.11.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.2131,  0.0078, -0.3499,  ..., -0.0626, -0.0710, -0.0352])),\n",
       "             ('encoder.layer.11.intermediate.dense.weight',\n",
       "              tensor([[ 0.0027, -0.0360, -0.0068,  ...,  0.0521,  0.0257,  0.0382],\n",
       "                      [ 0.1037,  0.0596,  0.0043,  ...,  0.0592,  0.0305, -0.0858],\n",
       "                      [ 0.0021,  0.0421, -0.0072,  ...,  0.0370,  0.0071, -0.0342],\n",
       "                      ...,\n",
       "                      [ 0.0575,  0.0113,  0.0083,  ..., -0.0046,  0.0500, -0.1034],\n",
       "                      [ 0.0515,  0.0054,  0.0134,  ...,  0.0610, -0.0278,  0.0374],\n",
       "                      [ 0.0683,  0.0210,  0.0369,  ...,  0.0178,  0.0008,  0.0075]])),\n",
       "             ('encoder.layer.11.intermediate.dense.bias',\n",
       "              tensor([ 0.0251, -0.0461, -0.0562,  ..., -0.0541, -0.0627,  0.0369])),\n",
       "             ('encoder.layer.11.output.dense.weight',\n",
       "              tensor([[ 0.0096,  0.0332, -0.0019,  ...,  0.0087,  0.0273, -0.0528],\n",
       "                      [ 0.0209,  0.0415,  0.0043,  ..., -0.0194,  0.0442,  0.0100],\n",
       "                      [-0.0147,  0.0217, -0.0069,  ..., -0.0033, -0.0069, -0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0475, -0.0575,  0.0631,  ..., -0.0210, -0.0167, -0.0103],\n",
       "                      [ 0.0137,  0.0289, -0.0425,  ...,  0.0094, -0.0359,  0.0132],\n",
       "                      [ 0.0227, -0.0406, -0.0009,  ..., -0.0292, -0.0610, -0.0126]])),\n",
       "             ('encoder.layer.11.output.dense.bias',\n",
       "              tensor([-0.1759, -0.0102, -0.0060,  ..., -0.0144, -0.0833, -0.1052])),\n",
       "             ('encoder.layer.11.output.LayerNorm.weight',\n",
       "              tensor([0.9844, 0.9932, 1.0049,  ..., 0.9756, 0.9800, 0.9897])),\n",
       "             ('encoder.layer.11.output.LayerNorm.bias',\n",
       "              tensor([ 0.0851, -0.0881,  0.1752,  ..., -0.0378, -0.0191, -0.0720])),\n",
       "             ('encoder.layer.12.attention.self.query.weight',\n",
       "              tensor([[-0.0488, -0.0256,  0.0858,  ..., -0.0010,  0.0500,  0.0536],\n",
       "                      [-0.0159,  0.0054,  0.0135,  ..., -0.0769, -0.0668,  0.0492],\n",
       "                      [-0.0540,  0.0361, -0.1340,  ...,  0.0570,  0.0264,  0.0520],\n",
       "                      ...,\n",
       "                      [ 0.0245,  0.0216,  0.2898,  ..., -0.0100,  0.0930, -0.0194],\n",
       "                      [-0.0828,  0.0131, -0.1093,  ..., -0.0341,  0.0127, -0.0220],\n",
       "                      [-0.0060, -0.0068, -0.0304,  ..., -0.0873, -0.0026, -0.0832]])),\n",
       "             ('encoder.layer.12.attention.self.query.bias',\n",
       "              tensor([ 0.0054, -0.0075,  0.0036,  ..., -0.2410, -0.0197, -0.0044])),\n",
       "             ('encoder.layer.12.attention.self.key.weight',\n",
       "              tensor([[-2.8244e-02, -1.2457e-01,  7.5012e-02,  ...,  3.4943e-02,\n",
       "                       -1.9699e-02,  6.4453e-02],\n",
       "                      [ 1.1896e-01,  2.5253e-02, -7.4158e-02,  ..., -7.4482e-04,\n",
       "                       -2.5940e-02, -1.8616e-02],\n",
       "                      [-4.1351e-02,  3.2043e-02, -1.0724e-01,  ..., -7.5134e-02,\n",
       "                       -4.4678e-02, -1.8969e-03],\n",
       "                      ...,\n",
       "                      [-6.1989e-03,  1.4053e-02, -3.7012e-01,  ...,  5.9113e-02,\n",
       "                       -5.2299e-03, -8.4534e-03],\n",
       "                      [ 2.6352e-02, -9.7942e-04, -1.9275e-01,  ..., -2.7145e-02,\n",
       "                        5.5962e-03,  5.4016e-02],\n",
       "                      [ 1.5402e-04, -4.5929e-02, -9.6680e-02,  ...,  4.7791e-02,\n",
       "                        8.3847e-03,  9.2545e-03]])),\n",
       "             ('encoder.layer.12.attention.self.key.bias',\n",
       "              tensor([ 0.0002, -0.0009,  0.0004,  ..., -0.0111,  0.0027, -0.0042])),\n",
       "             ('encoder.layer.12.attention.self.value.weight',\n",
       "              tensor([[-0.0885, -0.0014,  0.0082,  ...,  0.0435, -0.0056, -0.0083],\n",
       "                      [ 0.0298,  0.1000, -0.0121,  ..., -0.0118,  0.0301,  0.0829],\n",
       "                      [ 0.0188,  0.0054,  0.0002,  ..., -0.0400, -0.0425,  0.0327],\n",
       "                      ...,\n",
       "                      [-0.0253, -0.0458, -0.0151,  ..., -0.0051, -0.0123,  0.0576],\n",
       "                      [-0.0400, -0.0420,  0.0236,  ..., -0.0110, -0.0389,  0.0612],\n",
       "                      [ 0.0019, -0.0249, -0.0008,  ..., -0.0346,  0.0318,  0.0284]])),\n",
       "             ('encoder.layer.12.attention.self.value.bias',\n",
       "              tensor([ 0.0054, -0.0072,  0.0003,  ...,  0.0293,  0.0158,  0.0316])),\n",
       "             ('encoder.layer.12.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0478, -0.0254,  0.0855,  ..., -0.0012,  0.0502,  0.0533],\n",
       "                      [-0.0180,  0.0039,  0.0115,  ..., -0.0787, -0.0656,  0.0495],\n",
       "                      [-0.0523,  0.0366, -0.1329,  ...,  0.0573,  0.0259,  0.0522],\n",
       "                      ...,\n",
       "                      [ 0.0242,  0.0214,  0.2903,  ..., -0.0090,  0.0942, -0.0187],\n",
       "                      [-0.0834,  0.0134, -0.1102,  ..., -0.0337,  0.0124, -0.0226],\n",
       "                      [-0.0067, -0.0064, -0.0318,  ..., -0.0869, -0.0018, -0.0840]])),\n",
       "             ('encoder.layer.12.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0056, -0.0051,  0.0025,  ..., -0.2412, -0.0190, -0.0032])),\n",
       "             ('encoder.layer.12.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0475, -0.0250,  0.0862,  ..., -0.0003,  0.0501,  0.0543],\n",
       "                      [-0.0178,  0.0040,  0.0118,  ..., -0.0781, -0.0657,  0.0494],\n",
       "                      [-0.0534,  0.0364, -0.1332,  ...,  0.0569,  0.0260,  0.0521],\n",
       "                      ...,\n",
       "                      [ 0.0247,  0.0208,  0.2898,  ..., -0.0093,  0.0936, -0.0187],\n",
       "                      [-0.0831,  0.0134, -0.1100,  ..., -0.0339,  0.0130, -0.0224],\n",
       "                      [-0.0057, -0.0067, -0.0310,  ..., -0.0865, -0.0017, -0.0830]])),\n",
       "             ('encoder.layer.12.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0048, -0.0055,  0.0026,  ..., -0.2407, -0.0190, -0.0041])),\n",
       "             ('encoder.layer.12.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0465, -0.0239,  0.0851,  ..., -0.0013,  0.0496,  0.0545],\n",
       "                      [-0.0178,  0.0037,  0.0127,  ..., -0.0770, -0.0658,  0.0490],\n",
       "                      [-0.0526,  0.0370, -0.1343,  ...,  0.0560,  0.0267,  0.0525],\n",
       "                      ...,\n",
       "                      [ 0.0249,  0.0210,  0.2903,  ..., -0.0099,  0.0939, -0.0187],\n",
       "                      [-0.0829,  0.0136, -0.1091,  ..., -0.0335,  0.0126, -0.0226],\n",
       "                      [-0.0058, -0.0059, -0.0309,  ..., -0.0875, -0.0020, -0.0832]])),\n",
       "             ('encoder.layer.12.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0061, -0.0066,  0.0036,  ..., -0.2408, -0.0195, -0.0038])),\n",
       "             ('encoder.layer.12.attention.output.dense.weight',\n",
       "              tensor([[-0.0414, -0.0168, -0.0210,  ...,  0.0183, -0.0035, -0.0064],\n",
       "                      [-0.0469,  0.0126, -0.0432,  ..., -0.0307,  0.0024,  0.0154],\n",
       "                      [-0.0412, -0.0203,  0.0276,  ...,  0.0149,  0.0392,  0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0165, -0.0113, -0.0403,  ..., -0.0097,  0.0156,  0.0301],\n",
       "                      [-0.0012, -0.0291,  0.0198,  ..., -0.0248,  0.0183, -0.0164],\n",
       "                      [ 0.0039,  0.0388,  0.0415,  ..., -0.0073, -0.0340, -0.0304]])),\n",
       "             ('encoder.layer.12.attention.output.dense.bias',\n",
       "              tensor([ 0.0298, -0.0024,  0.0632,  ..., -0.0140, -0.0419, -0.0074])),\n",
       "             ('encoder.layer.12.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9917, 0.9814, 1.0029,  ..., 0.9839, 0.9805, 0.9849])),\n",
       "             ('encoder.layer.12.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1652,  0.0027, -0.3047,  ..., -0.0290, -0.1437, -0.0361])),\n",
       "             ('encoder.layer.12.intermediate.dense.weight',\n",
       "              tensor([[ 0.0739, -0.0299,  0.0165,  ..., -0.0100, -0.0059,  0.0089],\n",
       "                      [-0.0154, -0.0288,  0.0178,  ..., -0.0423, -0.0396, -0.0234],\n",
       "                      [-0.0438, -0.0429,  0.0853,  ...,  0.0035, -0.0200, -0.0239],\n",
       "                      ...,\n",
       "                      [ 0.0181, -0.0465,  0.0263,  ..., -0.1221, -0.0165, -0.0053],\n",
       "                      [ 0.0189, -0.0502,  0.0145,  ..., -0.0496, -0.0219,  0.0486],\n",
       "                      [ 0.0233,  0.0119, -0.0056,  ...,  0.0166, -0.0277, -0.0094]])),\n",
       "             ('encoder.layer.12.intermediate.dense.bias',\n",
       "              tensor([-0.1082, -0.0402, -0.0091,  ..., -0.1224, -0.0850, -0.1054])),\n",
       "             ('encoder.layer.12.output.dense.weight',\n",
       "              tensor([[ 0.0305, -0.0023, -0.0702,  ...,  0.0267,  0.0273, -0.0189],\n",
       "                      [-0.0123,  0.0209,  0.0146,  ..., -0.0280,  0.0058, -0.0346],\n",
       "                      [ 0.0021,  0.0007, -0.0149,  ..., -0.0040, -0.0253,  0.0004],\n",
       "                      ...,\n",
       "                      [-0.0104, -0.0108,  0.0165,  ..., -0.0426, -0.0157, -0.0237],\n",
       "                      [-0.0203, -0.0099, -0.0200,  ..., -0.0121, -0.0292,  0.0208],\n",
       "                      [-0.0514, -0.0340,  0.0322,  ...,  0.0074, -0.0035, -0.0122]])),\n",
       "             ('encoder.layer.12.output.dense.bias',\n",
       "              tensor([-0.2185,  0.1660, -0.0793,  ..., -0.0086, -0.0798, -0.1241])),\n",
       "             ('encoder.layer.12.output.LayerNorm.weight',\n",
       "              tensor([0.9863, 0.9927, 1.0010,  ..., 0.9873, 0.9834, 0.9907])),\n",
       "             ('encoder.layer.12.output.LayerNorm.bias',\n",
       "              tensor([ 0.0081, -0.0793,  0.0837,  ..., -0.0700, -0.0122, -0.0617])),\n",
       "             ('encoder.layer.13.attention.self.query.weight',\n",
       "              tensor([[ 0.0467,  0.0499, -0.0475,  ..., -0.0418,  0.0445, -0.0552],\n",
       "                      [-0.0258, -0.0089,  0.0503,  ...,  0.0328,  0.0287,  0.0623],\n",
       "                      [-0.0029, -0.0252,  0.0006,  ..., -0.0360, -0.0779, -0.0103],\n",
       "                      ...,\n",
       "                      [ 0.0665, -0.0066,  0.0292,  ...,  0.0056, -0.0182, -0.0072],\n",
       "                      [-0.0050, -0.0266, -0.1427,  ...,  0.0441,  0.0207, -0.0274],\n",
       "                      [ 0.0504,  0.0510, -0.1576,  ...,  0.0230,  0.0385,  0.0675]])),\n",
       "             ('encoder.layer.13.attention.self.query.bias',\n",
       "              tensor([-0.0495,  0.0435,  0.0070,  ...,  0.0418,  0.1788,  0.0484])),\n",
       "             ('encoder.layer.13.attention.self.key.weight',\n",
       "              tensor([[ 0.0242,  0.0640, -0.0420,  ..., -0.0147,  0.0274, -0.0693],\n",
       "                      [ 0.0562, -0.0043,  0.0737,  ..., -0.0331, -0.0074, -0.0347],\n",
       "                      [ 0.0183,  0.0652, -0.0408,  ...,  0.1559,  0.0287, -0.0793],\n",
       "                      ...,\n",
       "                      [-0.0481, -0.0498, -0.0121,  ..., -0.0222,  0.0134, -0.0264],\n",
       "                      [ 0.0221,  0.0304, -0.2097,  ..., -0.0201, -0.0186,  0.0012],\n",
       "                      [-0.0395,  0.0450, -0.1318,  ...,  0.0207,  0.0107,  0.0306]])),\n",
       "             ('encoder.layer.13.attention.self.key.bias',\n",
       "              tensor([ 0.0067, -0.0009,  0.0048,  ...,  0.0031,  0.0032,  0.0026])),\n",
       "             ('encoder.layer.13.attention.self.value.weight',\n",
       "              tensor([[ 0.0549,  0.0644, -0.0161,  ...,  0.0346,  0.0051,  0.0380],\n",
       "                      [ 0.0181,  0.0548,  0.0066,  ..., -0.0371, -0.0209,  0.0256],\n",
       "                      [-0.0051,  0.0461, -0.0021,  ..., -0.0210, -0.0573,  0.0381],\n",
       "                      ...,\n",
       "                      [ 0.0427, -0.0724,  0.0181,  ...,  0.0004, -0.0155, -0.0235],\n",
       "                      [-0.0018, -0.0005, -0.0391,  ...,  0.0100, -0.1066, -0.0103],\n",
       "                      [-0.0410, -0.0927,  0.0247,  ...,  0.0010, -0.0072, -0.0175]])),\n",
       "             ('encoder.layer.13.attention.self.value.bias',\n",
       "              tensor([ 0.0113,  0.0083,  0.0070,  ..., -0.0064, -0.0073, -0.0009])),\n",
       "             ('encoder.layer.13.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0460,  0.0500, -0.0478,  ..., -0.0423,  0.0454, -0.0554],\n",
       "                      [-0.0252, -0.0087,  0.0506,  ...,  0.0336,  0.0300,  0.0627],\n",
       "                      [-0.0026, -0.0251,  0.0008,  ..., -0.0368, -0.0778, -0.0104],\n",
       "                      ...,\n",
       "                      [ 0.0670, -0.0080,  0.0292,  ...,  0.0054, -0.0174, -0.0077],\n",
       "                      [-0.0058, -0.0260, -0.1434,  ...,  0.0430,  0.0199, -0.0277],\n",
       "                      [ 0.0502,  0.0508, -0.1583,  ...,  0.0242,  0.0382,  0.0669]])),\n",
       "             ('encoder.layer.13.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0492,  0.0434,  0.0062,  ...,  0.0423,  0.1794,  0.0481])),\n",
       "             ('encoder.layer.13.attention.self.e2w_query.weight',\n",
       "              tensor([[ 4.5685e-02,  4.8981e-02, -4.7729e-02,  ..., -4.1077e-02,\n",
       "                        4.5044e-02, -5.5939e-02],\n",
       "                      [-2.5772e-02, -8.7662e-03,  4.9744e-02,  ...,  3.2806e-02,\n",
       "                        2.9465e-02,  6.2927e-02],\n",
       "                      [-3.0117e-03, -2.5070e-02, -1.3733e-04,  ..., -3.6774e-02,\n",
       "                       -7.7759e-02, -1.0719e-02],\n",
       "                      ...,\n",
       "                      [ 6.7078e-02, -6.6223e-03,  2.8183e-02,  ...,  4.0321e-03,\n",
       "                       -1.7349e-02, -6.1226e-03],\n",
       "                      [-5.7716e-03, -2.5436e-02, -1.4307e-01,  ...,  4.3976e-02,\n",
       "                        1.9882e-02, -2.7832e-02],\n",
       "                      [ 4.9652e-02,  4.9957e-02, -1.5698e-01,  ...,  2.4918e-02,\n",
       "                        3.8544e-02,  6.6162e-02]])),\n",
       "             ('encoder.layer.13.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0494,  0.0443,  0.0076,  ...,  0.0436,  0.1790,  0.0469])),\n",
       "             ('encoder.layer.13.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0460,  0.0492, -0.0488,  ..., -0.0426,  0.0457, -0.0557],\n",
       "                      [-0.0256, -0.0091,  0.0501,  ...,  0.0329,  0.0296,  0.0629],\n",
       "                      [-0.0033, -0.0258, -0.0003,  ..., -0.0367, -0.0768, -0.0107],\n",
       "                      ...,\n",
       "                      [ 0.0668, -0.0073,  0.0286,  ...,  0.0046, -0.0173, -0.0069],\n",
       "                      [-0.0053, -0.0256, -0.1431,  ...,  0.0431,  0.0197, -0.0277],\n",
       "                      [ 0.0495,  0.0505, -0.1582,  ...,  0.0240,  0.0382,  0.0664]])),\n",
       "             ('encoder.layer.13.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0486,  0.0440,  0.0073,  ...,  0.0427,  0.1793,  0.0479])),\n",
       "             ('encoder.layer.13.attention.output.dense.weight',\n",
       "              tensor([[ 0.0012, -0.0389,  0.0300,  ..., -0.0499,  0.0024,  0.1019],\n",
       "                      [ 0.0078,  0.0095, -0.0061,  ...,  0.0063, -0.0690,  0.0446],\n",
       "                      [-0.0035, -0.0109, -0.0179,  ...,  0.0160, -0.0183, -0.0027],\n",
       "                      ...,\n",
       "                      [-0.0393, -0.0031, -0.0454,  ...,  0.0007, -0.0422,  0.0185],\n",
       "                      [-0.0412,  0.0609, -0.0032,  ...,  0.0147,  0.0768, -0.0290],\n",
       "                      [ 0.0017,  0.0061,  0.0103,  ...,  0.0196,  0.0349,  0.0488]])),\n",
       "             ('encoder.layer.13.attention.output.dense.bias',\n",
       "              tensor([-0.0429,  0.0439, -0.0901,  ...,  0.0370, -0.0334,  0.0341])),\n",
       "             ('encoder.layer.13.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9775, 0.9893, 1.0020,  ..., 0.9888, 0.9824, 0.9766])),\n",
       "             ('encoder.layer.13.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1793,  0.0703, -0.2717,  ..., -0.0598, -0.1188, -0.0613])),\n",
       "             ('encoder.layer.13.intermediate.dense.weight',\n",
       "              tensor([[ 0.0217, -0.0255,  0.0184,  ..., -0.0300, -0.0561, -0.0472],\n",
       "                      [ 0.0326,  0.0015,  0.0090,  ...,  0.0497,  0.0244,  0.0014],\n",
       "                      [ 0.0583, -0.0423,  0.0534,  ...,  0.1165, -0.0111, -0.0607],\n",
       "                      ...,\n",
       "                      [ 0.0292, -0.0145, -0.0065,  ..., -0.0116,  0.0372,  0.0076],\n",
       "                      [ 0.0141,  0.0399,  0.0949,  ...,  0.0481,  0.0398, -0.0367],\n",
       "                      [ 0.0364, -0.1292,  0.0403,  ...,  0.0048, -0.0009, -0.0759]])),\n",
       "             ('encoder.layer.13.intermediate.dense.bias',\n",
       "              tensor([-0.1367, -0.0517, -0.0891,  ..., -0.1030, -0.0341, -0.0382])),\n",
       "             ('encoder.layer.13.output.dense.weight',\n",
       "              tensor([[ 6.0638e-02, -3.4729e-02, -4.3060e-02,  ..., -3.4851e-02,\n",
       "                        1.4038e-02, -8.1482e-03],\n",
       "                      [-1.1391e-02,  5.9814e-03, -5.6061e-02,  ...,  8.0719e-03,\n",
       "                        2.1622e-02, -2.4231e-02],\n",
       "                      [ 1.2413e-02,  3.7689e-03,  1.7185e-03,  ..., -2.6054e-03,\n",
       "                        6.1264e-03, -6.1569e-03],\n",
       "                      ...,\n",
       "                      [-2.8534e-02,  3.8483e-02,  4.7531e-03,  ..., -3.3844e-02,\n",
       "                        1.5282e-02,  3.3081e-02],\n",
       "                      [-8.7402e-02, -2.7878e-02,  3.1113e-02,  ..., -1.9165e-02,\n",
       "                        5.8624e-02,  1.4473e-02],\n",
       "                      [-2.8519e-02, -3.8481e-04, -2.4811e-02,  ..., -2.4536e-02,\n",
       "                       -1.4365e-05, -4.0710e-02]])),\n",
       "             ('encoder.layer.13.output.dense.bias',\n",
       "              tensor([-0.1071,  0.0568, -0.0720,  ..., -0.0243, -0.0503, -0.0881])),\n",
       "             ('encoder.layer.13.output.LayerNorm.weight',\n",
       "              tensor([0.9849, 0.9912, 1.0010,  ..., 0.9883, 0.9873, 0.9922])),\n",
       "             ('encoder.layer.13.output.LayerNorm.bias',\n",
       "              tensor([ 0.0677, -0.1175,  0.2385,  ..., -0.0541, -0.0039, -0.0446])),\n",
       "             ('encoder.layer.14.attention.self.query.weight',\n",
       "              tensor([[-0.0440, -0.0606,  0.0087,  ..., -0.1205, -0.0219, -0.1600],\n",
       "                      [-0.0602,  0.0294, -0.0967,  ...,  0.0375, -0.0198, -0.0383],\n",
       "                      [-0.0041, -0.0288,  0.1078,  ..., -0.0109, -0.0606,  0.0385],\n",
       "                      ...,\n",
       "                      [ 0.0633,  0.0006,  0.0213,  ..., -0.0382, -0.0679,  0.0398],\n",
       "                      [-0.0173,  0.0390, -0.0182,  ..., -0.0922, -0.0248,  0.0566],\n",
       "                      [-0.0659,  0.0229,  0.0090,  ...,  0.0541,  0.0317,  0.0381]])),\n",
       "             ('encoder.layer.14.attention.self.query.bias',\n",
       "              tensor([ 0.1919,  0.1414, -0.1239,  ...,  0.2340, -0.1186,  0.0177])),\n",
       "             ('encoder.layer.14.attention.self.key.weight',\n",
       "              tensor([[ 0.0352,  0.0811,  0.0710,  ...,  0.0091,  0.0045, -0.0096],\n",
       "                      [-0.0663, -0.0061, -0.1246,  ..., -0.0135,  0.0071,  0.0470],\n",
       "                      [-0.0232,  0.0269,  0.0476,  ...,  0.0193, -0.1163,  0.0616],\n",
       "                      ...,\n",
       "                      [ 0.0784, -0.0635,  0.0656,  ..., -0.0961, -0.0451, -0.0300],\n",
       "                      [ 0.0467,  0.0869,  0.0437,  ..., -0.0759,  0.0657,  0.0388],\n",
       "                      [-0.0688, -0.0052,  0.0558,  ..., -0.0034, -0.0491, -0.0246]])),\n",
       "             ('encoder.layer.14.attention.self.key.bias',\n",
       "              tensor([-0.0012,  0.0015, -0.0064,  ...,  0.0006, -0.0010,  0.0037])),\n",
       "             ('encoder.layer.14.attention.self.value.weight',\n",
       "              tensor([[-4.3427e-02,  5.7340e-05,  4.2542e-02,  ..., -4.4189e-02,\n",
       "                       -5.5237e-03, -7.8583e-03],\n",
       "                      [-2.8248e-03, -6.8665e-02, -8.7128e-03,  ...,  6.2622e-02,\n",
       "                        6.3416e-02, -5.0537e-02],\n",
       "                      [ 2.4673e-02, -3.5919e-02, -2.5497e-02,  ..., -6.7383e-02,\n",
       "                        1.3374e-02,  2.2842e-02],\n",
       "                      ...,\n",
       "                      [ 8.0872e-02,  2.5452e-02, -1.7334e-02,  ...,  1.9119e-02,\n",
       "                       -2.5955e-02, -3.3264e-02],\n",
       "                      [ 7.1045e-02,  1.5244e-02, -1.3084e-02,  ..., -5.0774e-03,\n",
       "                       -2.0508e-02,  2.7557e-02],\n",
       "                      [-7.5623e-02, -4.0192e-02, -2.3071e-02,  ..., -2.8782e-03,\n",
       "                        4.7791e-02, -1.3074e-01]])),\n",
       "             ('encoder.layer.14.attention.self.value.bias',\n",
       "              tensor([-3.5119e-04,  6.4240e-03, -1.1986e-02,  ..., -9.7513e-05,\n",
       "                       5.2490e-03,  1.3374e-02])),\n",
       "             ('encoder.layer.14.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0436, -0.0602,  0.0091,  ..., -0.1191, -0.0216, -0.1595],\n",
       "                      [-0.0607,  0.0292, -0.0957,  ...,  0.0371, -0.0190, -0.0381],\n",
       "                      [-0.0033, -0.0289,  0.1082,  ..., -0.0097, -0.0602,  0.0385],\n",
       "                      ...,\n",
       "                      [ 0.0625,  0.0003,  0.0221,  ..., -0.0385, -0.0680,  0.0405],\n",
       "                      [-0.0182,  0.0391, -0.0182,  ..., -0.0934, -0.0236,  0.0570],\n",
       "                      [-0.0662,  0.0227,  0.0080,  ...,  0.0538,  0.0329,  0.0378]])),\n",
       "             ('encoder.layer.14.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.1924,  0.1399, -0.1243,  ...,  0.2332, -0.1191,  0.0183])),\n",
       "             ('encoder.layer.14.attention.self.e2w_query.weight',\n",
       "              tensor([[-4.4159e-02, -6.0425e-02,  8.8730e-03,  ..., -1.1963e-01,\n",
       "                       -2.2507e-02, -1.5979e-01],\n",
       "                      [-6.0547e-02,  3.0106e-02, -9.6985e-02,  ...,  3.6621e-02,\n",
       "                       -1.9363e-02, -3.7842e-02],\n",
       "                      [-3.3741e-03, -2.8809e-02,  1.0767e-01,  ..., -1.0811e-02,\n",
       "                       -5.9998e-02,  3.8269e-02],\n",
       "                      ...,\n",
       "                      [ 6.2927e-02,  5.3346e-05,  2.1423e-02,  ..., -3.7872e-02,\n",
       "                       -6.6284e-02,  3.9673e-02],\n",
       "                      [-1.8723e-02,  3.8635e-02, -1.7502e-02,  ..., -9.2834e-02,\n",
       "                       -2.3926e-02,  5.6305e-02],\n",
       "                      [-6.7017e-02,  2.2842e-02,  7.7782e-03,  ...,  5.2460e-02,\n",
       "                        3.1525e-02,  3.8422e-02]])),\n",
       "             ('encoder.layer.14.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.1925,  0.1407, -0.1238,  ...,  0.2334, -0.1198,  0.0184])),\n",
       "             ('encoder.layer.14.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0433, -0.0596,  0.0081,  ..., -0.1199, -0.0218, -0.1594],\n",
       "                      [-0.0608,  0.0301, -0.0968,  ...,  0.0370, -0.0194, -0.0379],\n",
       "                      [-0.0034, -0.0291,  0.1073,  ..., -0.0106, -0.0601,  0.0381],\n",
       "                      ...,\n",
       "                      [ 0.0622, -0.0006,  0.0224,  ..., -0.0382, -0.0673,  0.0399],\n",
       "                      [-0.0180,  0.0392, -0.0190,  ..., -0.0939, -0.0236,  0.0574],\n",
       "                      [-0.0664,  0.0233,  0.0077,  ...,  0.0534,  0.0324,  0.0381]])),\n",
       "             ('encoder.layer.14.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.1932,  0.1409, -0.1236,  ...,  0.2324, -0.1180,  0.0186])),\n",
       "             ('encoder.layer.14.attention.output.dense.weight',\n",
       "              tensor([[-0.0056, -0.0224,  0.0443,  ..., -0.0428, -0.0007,  0.0840],\n",
       "                      [ 0.0128, -0.0439,  0.0451,  ..., -0.0732, -0.0457, -0.0054],\n",
       "                      [ 0.0380, -0.0344,  0.0119,  ...,  0.0033,  0.0141, -0.0202],\n",
       "                      ...,\n",
       "                      [ 0.0673, -0.0351,  0.0683,  ...,  0.0097, -0.0060, -0.0115],\n",
       "                      [-0.0025, -0.0653, -0.0190,  ...,  0.0271,  0.0058, -0.0071],\n",
       "                      [-0.0092, -0.0266,  0.0236,  ...,  0.0658,  0.0231,  0.0720]])),\n",
       "             ('encoder.layer.14.attention.output.dense.bias',\n",
       "              tensor([-0.0231,  0.0020,  0.0322,  ...,  0.0639, -0.0414, -0.0055])),\n",
       "             ('encoder.layer.14.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9956, 0.9819, 1.0039,  ..., 0.9893, 0.9888, 0.9868])),\n",
       "             ('encoder.layer.14.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0869, -0.0149, -0.1970,  ..., -0.0645, -0.1379, -0.0533])),\n",
       "             ('encoder.layer.14.intermediate.dense.weight',\n",
       "              tensor([[-0.0075, -0.0020,  0.0398,  ...,  0.0012, -0.0152,  0.0420],\n",
       "                      [ 0.0173, -0.0505, -0.0079,  ...,  0.0595, -0.0289,  0.0460],\n",
       "                      [ 0.0445, -0.0673,  0.0124,  ...,  0.0683,  0.0684, -0.0569],\n",
       "                      ...,\n",
       "                      [ 0.0705, -0.0381,  0.0104,  ...,  0.0089, -0.0035, -0.0321],\n",
       "                      [-0.0336, -0.0549, -0.0132,  ...,  0.0289,  0.0120,  0.0190],\n",
       "                      [ 0.0384, -0.0446, -0.0678,  ...,  0.0356, -0.0184,  0.0068]])),\n",
       "             ('encoder.layer.14.intermediate.dense.bias',\n",
       "              tensor([-0.1079, -0.1095, -0.1117,  ..., -0.0576, -0.0796, -0.0810])),\n",
       "             ('encoder.layer.14.output.dense.weight',\n",
       "              tensor([[-0.0217,  0.0294,  0.0087,  ...,  0.0284,  0.0089, -0.0641],\n",
       "                      [ 0.0035, -0.0115, -0.0866,  ..., -0.0077,  0.0157, -0.0653],\n",
       "                      [-0.0097, -0.0200,  0.0356,  ...,  0.0083,  0.0033,  0.0125],\n",
       "                      ...,\n",
       "                      [ 0.0573,  0.0070,  0.0095,  ..., -0.0446,  0.0353, -0.0303],\n",
       "                      [-0.0526,  0.0128,  0.0271,  ...,  0.0456,  0.0032, -0.0313],\n",
       "                      [ 0.0245,  0.0083, -0.0593,  ..., -0.0062, -0.0166, -0.0692]])),\n",
       "             ('encoder.layer.14.output.dense.bias',\n",
       "              tensor([-0.1091,  0.0495, -0.1954,  ..., -0.0115, -0.1292, -0.0688])),\n",
       "             ('encoder.layer.14.output.LayerNorm.weight',\n",
       "              tensor([0.9902, 0.9932, 1.0039,  ..., 0.9883, 0.9897, 0.9902])),\n",
       "             ('encoder.layer.14.output.LayerNorm.bias',\n",
       "              tensor([-0.0245, -0.1100,  0.2477,  ..., -0.0520, -0.0152, -0.0587])),\n",
       "             ('encoder.layer.15.attention.self.query.weight',\n",
       "              tensor([[ 0.0620,  0.0573, -0.1342,  ..., -0.0201,  0.0434,  0.0235],\n",
       "                      [-0.0207,  0.0164,  0.0818,  ...,  0.0889, -0.0285, -0.0184],\n",
       "                      [ 0.0010, -0.0185, -0.0278,  ..., -0.0594,  0.0469, -0.0782],\n",
       "                      ...,\n",
       "                      [ 0.0491,  0.1175, -0.0270,  ...,  0.0266, -0.0189,  0.0104],\n",
       "                      [-0.0193,  0.0881,  0.0353,  ..., -0.0011, -0.0586,  0.0416],\n",
       "                      [-0.0609, -0.0311,  0.0099,  ..., -0.0083,  0.0947, -0.0548]])),\n",
       "             ('encoder.layer.15.attention.self.query.bias',\n",
       "              tensor([-0.0130, -0.0726, -0.0253,  ..., -0.1169, -0.0905, -0.0038])),\n",
       "             ('encoder.layer.15.attention.self.key.weight',\n",
       "              tensor([[-0.0437,  0.0203, -0.2190,  ..., -0.0843, -0.0095, -0.0124],\n",
       "                      [ 0.0635, -0.0227, -0.0543,  ..., -0.0116,  0.0023, -0.0230],\n",
       "                      [-0.0341,  0.0834, -0.0762,  ..., -0.0372, -0.0598, -0.0167],\n",
       "                      ...,\n",
       "                      [ 0.0126,  0.0198, -0.0547,  ..., -0.1058,  0.0357, -0.0164],\n",
       "                      [-0.0944, -0.0173, -0.0423,  ..., -0.0055,  0.0136,  0.0278],\n",
       "                      [ 0.0543, -0.0385,  0.0255,  ..., -0.0328,  0.0463,  0.0562]])),\n",
       "             ('encoder.layer.15.attention.self.key.bias',\n",
       "              tensor([ 0.0009,  0.0151,  0.0054,  ..., -0.0016,  0.0033, -0.0033])),\n",
       "             ('encoder.layer.15.attention.self.value.weight',\n",
       "              tensor([[-0.0707,  0.0461,  0.0057,  ...,  0.0010, -0.0219,  0.0614],\n",
       "                      [-0.0140, -0.0301, -0.0324,  ..., -0.0392,  0.0406, -0.0698],\n",
       "                      [-0.0017, -0.0084,  0.0031,  ..., -0.1066,  0.1023,  0.0998],\n",
       "                      ...,\n",
       "                      [ 0.0151, -0.0585, -0.0005,  ...,  0.0378, -0.0101,  0.0754],\n",
       "                      [-0.0451, -0.0868,  0.0035,  ...,  0.0587,  0.0168,  0.0128],\n",
       "                      [-0.0610,  0.0026, -0.0128,  ..., -0.0711, -0.1055,  0.0049]])),\n",
       "             ('encoder.layer.15.attention.self.value.bias',\n",
       "              tensor([-0.0044,  0.0050,  0.0006,  ...,  0.0032, -0.0044, -0.0212])),\n",
       "             ('encoder.layer.15.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0615,  0.0556, -0.1339,  ..., -0.0208,  0.0425,  0.0237],\n",
       "                      [-0.0204,  0.0145,  0.0825,  ...,  0.0886, -0.0285, -0.0173],\n",
       "                      [ 0.0021, -0.0168, -0.0272,  ..., -0.0598,  0.0464, -0.0778],\n",
       "                      ...,\n",
       "                      [ 0.0488,  0.1176, -0.0269,  ...,  0.0259, -0.0200,  0.0100],\n",
       "                      [-0.0197,  0.0878,  0.0350,  ..., -0.0002, -0.0583,  0.0408],\n",
       "                      [-0.0617, -0.0318,  0.0094,  ..., -0.0086,  0.0946, -0.0556]])),\n",
       "             ('encoder.layer.15.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0126, -0.0731, -0.0265,  ..., -0.1166, -0.0897, -0.0031])),\n",
       "             ('encoder.layer.15.attention.self.e2w_query.weight',\n",
       "              tensor([[ 6.0822e-02,  5.6122e-02, -1.3403e-01,  ..., -2.1027e-02,\n",
       "                        4.2389e-02,  2.4231e-02],\n",
       "                      [-1.9714e-02,  1.5190e-02,  8.2947e-02,  ...,  8.8318e-02,\n",
       "                       -2.8641e-02, -1.8250e-02],\n",
       "                      [ 1.7185e-03, -1.7609e-02, -2.7405e-02,  ..., -5.8502e-02,\n",
       "                        4.6997e-02, -7.7515e-02],\n",
       "                      ...,\n",
       "                      [ 4.8645e-02,  1.1792e-01, -2.6932e-02,  ...,  2.6596e-02,\n",
       "                       -1.9196e-02,  9.8343e-03],\n",
       "                      [-1.9653e-02,  8.7585e-02,  3.4943e-02,  ...,  1.7643e-05,\n",
       "                       -5.8594e-02,  4.1351e-02],\n",
       "                      [-6.1676e-02, -3.2440e-02,  9.7961e-03,  ..., -8.0185e-03,\n",
       "                        9.4543e-02, -5.5237e-02]])),\n",
       "             ('encoder.layer.15.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0129, -0.0737, -0.0257,  ..., -0.1170, -0.0900, -0.0036])),\n",
       "             ('encoder.layer.15.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0620,  0.0565, -0.1329,  ..., -0.0208,  0.0421,  0.0244],\n",
       "                      [-0.0208,  0.0150,  0.0827,  ...,  0.0881, -0.0293, -0.0181],\n",
       "                      [ 0.0007, -0.0170, -0.0285,  ..., -0.0589,  0.0469, -0.0781],\n",
       "                      ...,\n",
       "                      [ 0.0493,  0.1179, -0.0271,  ...,  0.0265, -0.0199,  0.0104],\n",
       "                      [-0.0202,  0.0878,  0.0353,  ..., -0.0003, -0.0587,  0.0407],\n",
       "                      [-0.0625, -0.0317,  0.0096,  ..., -0.0085,  0.0941, -0.0557]])),\n",
       "             ('encoder.layer.15.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0135, -0.0731, -0.0248,  ..., -0.1169, -0.0898, -0.0032])),\n",
       "             ('encoder.layer.15.attention.output.dense.weight',\n",
       "              tensor([[ 0.0132,  0.0012, -0.0203,  ..., -0.0332, -0.0267,  0.0226],\n",
       "                      [ 0.0044,  0.0268, -0.0366,  ..., -0.0063,  0.0477, -0.0233],\n",
       "                      [-0.0302, -0.0359,  0.0726,  ..., -0.0166, -0.0181, -0.0119],\n",
       "                      ...,\n",
       "                      [-0.0005,  0.0582,  0.0144,  ..., -0.0342, -0.0097,  0.0253],\n",
       "                      [-0.0593,  0.0953,  0.0491,  ..., -0.0077,  0.0185,  0.0876],\n",
       "                      [ 0.0207, -0.0543,  0.0082,  ..., -0.0124, -0.0357, -0.0190]])),\n",
       "             ('encoder.layer.15.attention.output.dense.bias',\n",
       "              tensor([-0.0166,  0.0164,  0.0541,  ...,  0.0015, -0.0478, -0.0286])),\n",
       "             ('encoder.layer.15.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9883, 0.9897, 1.0039,  ..., 0.9878, 0.9893, 0.9829])),\n",
       "             ('encoder.layer.15.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1111, -0.0181, -0.1715,  ..., -0.0934, -0.0807, -0.1244])),\n",
       "             ('encoder.layer.15.intermediate.dense.weight',\n",
       "              tensor([[-0.0215, -0.0170,  0.0211,  ...,  0.0115,  0.0492, -0.0308],\n",
       "                      [ 0.0518, -0.0554,  0.0037,  ..., -0.0227, -0.0463, -0.0097],\n",
       "                      [-0.0313,  0.0470,  0.0212,  ...,  0.0958,  0.0073,  0.0263],\n",
       "                      ...,\n",
       "                      [-0.0418, -0.0390,  0.0205,  ...,  0.0809,  0.0416,  0.0274],\n",
       "                      [-0.0383,  0.0682,  0.0560,  ..., -0.0085,  0.0233, -0.0089],\n",
       "                      [ 0.0706,  0.0068,  0.0057,  ...,  0.0411,  0.0718,  0.0269]])),\n",
       "             ('encoder.layer.15.intermediate.dense.bias',\n",
       "              tensor([-0.0442, -0.1071, -0.0778,  ..., -0.0341,  0.0108, -0.0829])),\n",
       "             ('encoder.layer.15.output.dense.weight',\n",
       "              tensor([[-0.0259,  0.0648, -0.0484,  ..., -0.0003,  0.0195, -0.0052],\n",
       "                      [-0.0217,  0.0138, -0.0334,  ..., -0.0560, -0.0031, -0.0030],\n",
       "                      [-0.0159, -0.0107, -0.0237,  ...,  0.0003,  0.0148,  0.0125],\n",
       "                      ...,\n",
       "                      [ 0.0068,  0.0219,  0.0510,  ..., -0.0092,  0.0051,  0.0019],\n",
       "                      [-0.0022, -0.0071,  0.0690,  ..., -0.0408, -0.0167,  0.0035],\n",
       "                      [-0.0446,  0.0178,  0.0220,  ...,  0.0345, -0.0116,  0.0390]])),\n",
       "             ('encoder.layer.15.output.dense.bias',\n",
       "              tensor([-0.1306,  0.0305, -0.2480,  ..., -0.0320, -0.0864, -0.1000])),\n",
       "             ('encoder.layer.15.output.LayerNorm.weight',\n",
       "              tensor([0.9878, 0.9854, 1.0039,  ..., 0.9849, 0.9951, 0.9873])),\n",
       "             ('encoder.layer.15.output.LayerNorm.bias',\n",
       "              tensor([-0.0082, -0.0685,  0.1721,  ..., -0.0154, -0.0311, -0.0166])),\n",
       "             ('encoder.layer.16.attention.self.query.weight',\n",
       "              tensor([[-0.0196,  0.0019, -0.0433,  ..., -0.0118,  0.0911,  0.0331],\n",
       "                      [-0.0457, -0.0659, -0.0247,  ...,  0.0132, -0.0212, -0.0356],\n",
       "                      [ 0.0011, -0.0064,  0.0827,  ..., -0.0672, -0.0600, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0691, -0.0829, -0.0294,  ..., -0.0164,  0.0376,  0.0193],\n",
       "                      [-0.0475,  0.0381,  0.0540,  ..., -0.0856,  0.0352, -0.1118],\n",
       "                      [ 0.0099,  0.0125,  0.0357,  ..., -0.0338, -0.0843, -0.0390]])),\n",
       "             ('encoder.layer.16.attention.self.query.bias',\n",
       "              tensor([-0.0085, -0.0684,  0.0119,  ..., -0.0044,  0.0848,  0.0124])),\n",
       "             ('encoder.layer.16.attention.self.key.weight',\n",
       "              tensor([[-0.0525, -0.0161, -0.0313,  ...,  0.0792, -0.0124, -0.0522],\n",
       "                      [ 0.0487, -0.0139, -0.0323,  ...,  0.0777, -0.0447, -0.0287],\n",
       "                      [-0.0167,  0.0693,  0.0786,  ..., -0.0332, -0.0619,  0.0303],\n",
       "                      ...,\n",
       "                      [ 0.0493,  0.0341,  0.0049,  ..., -0.0754, -0.0120, -0.0034],\n",
       "                      [ 0.0020,  0.0837,  0.0179,  ..., -0.0268,  0.0577,  0.0075],\n",
       "                      [ 0.0501, -0.0217,  0.0757,  ..., -0.0883, -0.0316,  0.0468]])),\n",
       "             ('encoder.layer.16.attention.self.key.bias',\n",
       "              tensor([-1.5469e-03, -4.5252e-04,  8.2169e-03,  ..., -1.6356e-03,\n",
       "                      -6.8843e-05,  4.3526e-03])),\n",
       "             ('encoder.layer.16.attention.self.value.weight',\n",
       "              tensor([[ 0.0414, -0.0031,  0.0109,  ...,  0.0368, -0.0610,  0.0514],\n",
       "                      [ 0.0277,  0.0594, -0.0338,  ...,  0.0028,  0.0233,  0.0863],\n",
       "                      [ 0.0110,  0.0028, -0.0090,  ..., -0.0414, -0.0536,  0.0280],\n",
       "                      ...,\n",
       "                      [-0.0561, -0.1257,  0.0250,  ..., -0.0909, -0.0078,  0.0553],\n",
       "                      [-0.0044, -0.0323, -0.0327,  ..., -0.0278,  0.0043,  0.1261],\n",
       "                      [ 0.0391,  0.1238, -0.0008,  ..., -0.0181,  0.0112, -0.0254]])),\n",
       "             ('encoder.layer.16.attention.self.value.bias',\n",
       "              tensor([-0.0092,  0.0052,  0.0026,  ..., -0.0101, -0.0145, -0.0095])),\n",
       "             ('encoder.layer.16.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0202,  0.0024, -0.0433,  ..., -0.0114,  0.0900,  0.0332],\n",
       "                      [-0.0460, -0.0664, -0.0250,  ...,  0.0129, -0.0215, -0.0355],\n",
       "                      [ 0.0012, -0.0068,  0.0821,  ..., -0.0679, -0.0597, -0.0087],\n",
       "                      ...,\n",
       "                      [ 0.0688, -0.0836, -0.0291,  ..., -0.0163,  0.0365,  0.0195],\n",
       "                      [-0.0475,  0.0384,  0.0534,  ..., -0.0855,  0.0357, -0.1119],\n",
       "                      [ 0.0105,  0.0113,  0.0354,  ..., -0.0341, -0.0838, -0.0395]])),\n",
       "             ('encoder.layer.16.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0076, -0.0679,  0.0124,  ..., -0.0043,  0.0844,  0.0132])),\n",
       "             ('encoder.layer.16.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0207,  0.0022, -0.0438,  ..., -0.0119,  0.0906,  0.0344],\n",
       "                      [-0.0465, -0.0660, -0.0257,  ...,  0.0124, -0.0215, -0.0350],\n",
       "                      [ 0.0014, -0.0063,  0.0830,  ..., -0.0674, -0.0603, -0.0098],\n",
       "                      ...,\n",
       "                      [ 0.0689, -0.0839, -0.0284,  ..., -0.0160,  0.0371,  0.0189],\n",
       "                      [-0.0475,  0.0386,  0.0535,  ..., -0.0859,  0.0358, -0.1115],\n",
       "                      [ 0.0114,  0.0121,  0.0361,  ..., -0.0331, -0.0845, -0.0399]])),\n",
       "             ('encoder.layer.16.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0072, -0.0677,  0.0116,  ..., -0.0053,  0.0847,  0.0119])),\n",
       "             ('encoder.layer.16.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0203,  0.0027, -0.0444,  ..., -0.0119,  0.0905,  0.0335],\n",
       "                      [-0.0459, -0.0657, -0.0257,  ...,  0.0126, -0.0211, -0.0359],\n",
       "                      [ 0.0014, -0.0068,  0.0830,  ..., -0.0676, -0.0600, -0.0089],\n",
       "                      ...,\n",
       "                      [ 0.0688, -0.0831, -0.0289,  ..., -0.0164,  0.0368,  0.0191],\n",
       "                      [-0.0475,  0.0383,  0.0538,  ..., -0.0851,  0.0353, -0.1120],\n",
       "                      [ 0.0104,  0.0119,  0.0357,  ..., -0.0336, -0.0836, -0.0393]])),\n",
       "             ('encoder.layer.16.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0066, -0.0674,  0.0116,  ..., -0.0048,  0.0844,  0.0126])),\n",
       "             ('encoder.layer.16.attention.output.dense.weight',\n",
       "              tensor([[ 0.0403, -0.0420, -0.0107,  ...,  0.0044, -0.0164,  0.0075],\n",
       "                      [ 0.0006,  0.0209,  0.0110,  ...,  0.0295, -0.0629, -0.0204],\n",
       "                      [ 0.0556, -0.0715,  0.0200,  ..., -0.0106, -0.0302,  0.0325],\n",
       "                      ...,\n",
       "                      [ 0.0179,  0.0094, -0.0593,  ...,  0.0848,  0.0364,  0.0056],\n",
       "                      [-0.0514, -0.0359, -0.0234,  ...,  0.0208, -0.0741,  0.0303],\n",
       "                      [ 0.0041, -0.0126, -0.0050,  ..., -0.0682, -0.0622, -0.0392]])),\n",
       "             ('encoder.layer.16.attention.output.dense.bias',\n",
       "              tensor([-0.0025, -0.0225,  0.0823,  ..., -0.0599,  0.0308, -0.0674])),\n",
       "             ('encoder.layer.16.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9980, 0.9941, 1.0078,  ..., 0.9912, 0.9951, 0.9976])),\n",
       "             ('encoder.layer.16.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0794, -0.0439, -0.2108,  ..., -0.0630, -0.0681, -0.0668])),\n",
       "             ('encoder.layer.16.intermediate.dense.weight',\n",
       "              tensor([[ 0.0764, -0.0260,  0.0473,  ...,  0.0425,  0.0108, -0.0328],\n",
       "                      [ 0.0170,  0.0441,  0.0222,  ...,  0.0631, -0.0727,  0.0036],\n",
       "                      [ 0.0163,  0.0162, -0.0125,  ...,  0.1248, -0.0081, -0.0133],\n",
       "                      ...,\n",
       "                      [-0.0786, -0.0622,  0.0157,  ...,  0.0253,  0.0300, -0.0213],\n",
       "                      [-0.0357, -0.0204,  0.0433,  ...,  0.0102, -0.0472, -0.0468],\n",
       "                      [-0.0128, -0.0894,  0.0395,  ...,  0.0269, -0.0498,  0.0314]])),\n",
       "             ('encoder.layer.16.intermediate.dense.bias',\n",
       "              tensor([-0.0790, -0.0588, -0.1002,  ..., -0.0635, -0.0787, -0.0875])),\n",
       "             ('encoder.layer.16.output.dense.weight',\n",
       "              tensor([[ 0.0109,  0.0328,  0.0648,  ..., -0.0171, -0.0012,  0.0048],\n",
       "                      [-0.0130, -0.0046,  0.0122,  ..., -0.0371, -0.0667, -0.0250],\n",
       "                      [ 0.0068, -0.0015, -0.0163,  ...,  0.0015,  0.0015,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.0144,  0.0413,  0.0055,  ...,  0.0039, -0.0348, -0.0575],\n",
       "                      [-0.0558, -0.0398,  0.0435,  ..., -0.0102, -0.0304,  0.0128],\n",
       "                      [-0.0037,  0.0193, -0.0089,  ..., -0.0358,  0.0088,  0.0350]])),\n",
       "             ('encoder.layer.16.output.dense.bias',\n",
       "              tensor([-0.0817,  0.0129, -0.2003,  ..., -0.0397, -0.0638, -0.1039])),\n",
       "             ('encoder.layer.16.output.LayerNorm.weight',\n",
       "              tensor([0.9780, 0.9961, 1.0059,  ..., 0.9741, 0.9912, 0.9878])),\n",
       "             ('encoder.layer.16.output.LayerNorm.bias',\n",
       "              tensor([-0.0200, -0.0485,  0.1780,  ..., -0.0254, -0.0309, -0.0301])),\n",
       "             ('encoder.layer.17.attention.self.query.weight',\n",
       "              tensor([[-0.0025, -0.0144,  0.0612,  ...,  0.0392, -0.0261,  0.0591],\n",
       "                      [ 0.0263,  0.0111, -0.0267,  ...,  0.0825, -0.0316,  0.0501],\n",
       "                      [ 0.1404,  0.0714,  0.0313,  ..., -0.0880, -0.0371, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0271, -0.0431,  0.0747,  ..., -0.1210,  0.0543, -0.0731],\n",
       "                      [ 0.0447, -0.0082,  0.0473,  ..., -0.0052, -0.0441,  0.0298],\n",
       "                      [ 0.0129,  0.0948, -0.0287,  ...,  0.0013,  0.0137, -0.0209]])),\n",
       "             ('encoder.layer.17.attention.self.query.bias',\n",
       "              tensor([-0.1755,  0.0590,  0.0238,  ..., -0.0483,  0.0392,  0.0130])),\n",
       "             ('encoder.layer.17.attention.self.key.weight',\n",
       "              tensor([[ 0.0301, -0.0104,  0.0533,  ..., -0.0618,  0.0421,  0.0252],\n",
       "                      [ 0.0470,  0.0137,  0.0359,  ...,  0.0915, -0.0418,  0.0223],\n",
       "                      [ 0.0160,  0.0344, -0.0348,  ..., -0.0642,  0.0806, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0152,  0.0612,  0.0259,  ...,  0.0212,  0.0424, -0.0739],\n",
       "                      [ 0.0330,  0.0546,  0.0499,  ...,  0.0514,  0.0179,  0.0204],\n",
       "                      [-0.0836,  0.0066, -0.0238,  ...,  0.0525,  0.0056,  0.0623]])),\n",
       "             ('encoder.layer.17.attention.self.key.bias',\n",
       "              tensor([ 0.0075, -0.0181,  0.0159,  ..., -0.0047,  0.0122, -0.0042])),\n",
       "             ('encoder.layer.17.attention.self.value.weight',\n",
       "              tensor([[-0.0153,  0.0032, -0.0081,  ...,  0.0113,  0.0057,  0.0009],\n",
       "                      [-0.0189,  0.0219, -0.0104,  ...,  0.0272,  0.0158,  0.0283],\n",
       "                      [ 0.0119,  0.0108, -0.0102,  ...,  0.0194, -0.0050, -0.0737],\n",
       "                      ...,\n",
       "                      [-0.0241,  0.0324,  0.0048,  ..., -0.0291, -0.0849, -0.1095],\n",
       "                      [ 0.0182, -0.0961, -0.0291,  ...,  0.0067, -0.0311,  0.0963],\n",
       "                      [ 0.0613, -0.0652,  0.0103,  ..., -0.1134,  0.0496, -0.0442]])),\n",
       "             ('encoder.layer.17.attention.self.value.bias',\n",
       "              tensor([ 0.0080, -0.0018, -0.0005,  ...,  0.0019,  0.0087,  0.0055])),\n",
       "             ('encoder.layer.17.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0024, -0.0143,  0.0623,  ...,  0.0395, -0.0253,  0.0595],\n",
       "                      [ 0.0258,  0.0107, -0.0263,  ...,  0.0829, -0.0305,  0.0504],\n",
       "                      [ 0.1403,  0.0710,  0.0320,  ..., -0.0884, -0.0381, -0.0038],\n",
       "                      ...,\n",
       "                      [-0.0274, -0.0437,  0.0747,  ..., -0.1210,  0.0554, -0.0722],\n",
       "                      [ 0.0448, -0.0087,  0.0464,  ..., -0.0058, -0.0444,  0.0294],\n",
       "                      [ 0.0126,  0.0944, -0.0292,  ...,  0.0004,  0.0127, -0.0226]])),\n",
       "             ('encoder.layer.17.attention.self.w2e_query.bias',\n",
       "              tensor([-0.1764,  0.0582,  0.0237,  ..., -0.0485,  0.0391,  0.0143])),\n",
       "             ('encoder.layer.17.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0021, -0.0154,  0.0628,  ...,  0.0399, -0.0247,  0.0589],\n",
       "                      [ 0.0263,  0.0115, -0.0262,  ...,  0.0828, -0.0306,  0.0501],\n",
       "                      [ 0.1409,  0.0712,  0.0323,  ..., -0.0884, -0.0374, -0.0038],\n",
       "                      ...,\n",
       "                      [-0.0273, -0.0440,  0.0747,  ..., -0.1212,  0.0544, -0.0728],\n",
       "                      [ 0.0450, -0.0083,  0.0471,  ..., -0.0054, -0.0440,  0.0292],\n",
       "                      [ 0.0134,  0.0948, -0.0282,  ...,  0.0008,  0.0133, -0.0222]])),\n",
       "             ('encoder.layer.17.attention.self.e2w_query.bias',\n",
       "              tensor([-0.1770,  0.0579,  0.0234,  ..., -0.0483,  0.0386,  0.0137])),\n",
       "             ('encoder.layer.17.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0026, -0.0146,  0.0626,  ...,  0.0393, -0.0253,  0.0590],\n",
       "                      [ 0.0264,  0.0104, -0.0264,  ...,  0.0828, -0.0313,  0.0506],\n",
       "                      [ 0.1410,  0.0710,  0.0321,  ..., -0.0886, -0.0378, -0.0038],\n",
       "                      ...,\n",
       "                      [-0.0276, -0.0439,  0.0743,  ..., -0.1221,  0.0547, -0.0728],\n",
       "                      [ 0.0453, -0.0077,  0.0472,  ..., -0.0052, -0.0438,  0.0295],\n",
       "                      [ 0.0132,  0.0951, -0.0287,  ...,  0.0012,  0.0133, -0.0219]])),\n",
       "             ('encoder.layer.17.attention.self.e2e_query.bias',\n",
       "              tensor([-0.1763,  0.0583,  0.0239,  ..., -0.0474,  0.0381,  0.0135])),\n",
       "             ('encoder.layer.17.attention.output.dense.weight',\n",
       "              tensor([[-0.0009, -0.0266, -0.0172,  ..., -0.0211, -0.0330,  0.0538],\n",
       "                      [ 0.0276,  0.0079,  0.0107,  ...,  0.0716, -0.0469,  0.0632],\n",
       "                      [ 0.0026,  0.0094, -0.0050,  ...,  0.0144, -0.0335, -0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0324,  0.0123, -0.0420,  ...,  0.0280,  0.0308,  0.0225],\n",
       "                      [-0.0289,  0.0283, -0.0078,  ..., -0.0434,  0.0610, -0.0330],\n",
       "                      [ 0.0229, -0.0194, -0.0584,  ..., -0.0039, -0.0276,  0.0182]])),\n",
       "             ('encoder.layer.17.attention.output.dense.bias',\n",
       "              tensor([-0.0240,  0.0378,  0.0308,  ..., -0.0087, -0.0490, -0.0173])),\n",
       "             ('encoder.layer.17.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9868, 0.9985, 1.0039,  ..., 0.9844, 0.9844, 0.9849])),\n",
       "             ('encoder.layer.17.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0967, -0.0631, -0.2181,  ..., -0.0829, -0.0676, -0.0845])),\n",
       "             ('encoder.layer.17.intermediate.dense.weight',\n",
       "              tensor([[ 0.0078, -0.0436,  0.0242,  ..., -0.0528, -0.0386, -0.0195],\n",
       "                      [ 0.0850, -0.0295,  0.0198,  ...,  0.0225, -0.0598, -0.0074],\n",
       "                      [-0.0068,  0.0132,  0.0107,  ...,  0.0237,  0.0309,  0.0179],\n",
       "                      ...,\n",
       "                      [ 0.0731, -0.0610, -0.0360,  ..., -0.0074,  0.0329,  0.0040],\n",
       "                      [-0.0060,  0.0971,  0.0130,  ..., -0.0481, -0.0654,  0.0122],\n",
       "                      [-0.0505, -0.0226, -0.0033,  ..., -0.0255,  0.0027,  0.0257]])),\n",
       "             ('encoder.layer.17.intermediate.dense.bias',\n",
       "              tensor([-0.0443, -0.0903, -0.0247,  ...,  0.0204, -0.1123, -0.0455])),\n",
       "             ('encoder.layer.17.output.dense.weight',\n",
       "              tensor([[ 0.0262,  0.0680, -0.0140,  ...,  0.0036, -0.0249, -0.0595],\n",
       "                      [ 0.0068, -0.0053,  0.0210,  ...,  0.0045,  0.0037, -0.0068],\n",
       "                      [-0.0193, -0.0238, -0.0004,  ...,  0.0133,  0.0316, -0.0163],\n",
       "                      ...,\n",
       "                      [ 0.0141,  0.0981, -0.0147,  ...,  0.0158, -0.0340,  0.0019],\n",
       "                      [ 0.0084,  0.0090, -0.0237,  ...,  0.0167,  0.0405,  0.0001],\n",
       "                      [-0.0217, -0.0357, -0.0156,  ...,  0.0092, -0.0204,  0.0151]])),\n",
       "             ('encoder.layer.17.output.dense.bias',\n",
       "              tensor([-0.0692,  0.0158, -0.1523,  ..., -0.0157, -0.0880, -0.0509])),\n",
       "             ('encoder.layer.17.output.LayerNorm.weight',\n",
       "              tensor([0.9849, 0.9917, 1.0029,  ..., 0.9810, 0.9858, 0.9849])),\n",
       "             ('encoder.layer.17.output.LayerNorm.bias',\n",
       "              tensor([-0.0041, -0.0272,  0.1224,  ..., -0.0133, -0.0379, -0.0070])),\n",
       "             ('encoder.layer.18.attention.self.query.weight',\n",
       "              tensor([[ 0.0310, -0.0033,  0.1387,  ...,  0.0225,  0.0681, -0.0144],\n",
       "                      [ 0.0337,  0.0215,  0.0124,  ..., -0.1433, -0.0501, -0.0421],\n",
       "                      [-0.0525,  0.0578,  0.1871,  ...,  0.0400, -0.0252, -0.0235],\n",
       "                      ...,\n",
       "                      [-0.0516, -0.0028, -0.0283,  ...,  0.0216, -0.0192, -0.0321],\n",
       "                      [-0.0627,  0.0326, -0.0139,  ...,  0.0160,  0.0525,  0.1091],\n",
       "                      [-0.0167,  0.0413,  0.1418,  ..., -0.0323,  0.0136, -0.0525]])),\n",
       "             ('encoder.layer.18.attention.self.query.bias',\n",
       "              tensor([-0.2759, -0.0118, -0.1447,  ...,  0.0094, -0.0058,  0.0213])),\n",
       "             ('encoder.layer.18.attention.self.key.weight',\n",
       "              tensor([[-0.0598, -0.0174,  0.0510,  ...,  0.0112, -0.0073,  0.0454],\n",
       "                      [-0.0240,  0.0817,  0.1118,  ..., -0.0175, -0.0041, -0.0597],\n",
       "                      [ 0.0692,  0.0491,  0.2571,  ..., -0.0257,  0.0065,  0.0103],\n",
       "                      ...,\n",
       "                      [-0.0539, -0.0307, -0.0146,  ..., -0.0363,  0.0638,  0.0253],\n",
       "                      [ 0.0620, -0.0158, -0.0206,  ..., -0.0181, -0.0436, -0.0036],\n",
       "                      [-0.0193, -0.0167,  0.1440,  ...,  0.0345, -0.0144,  0.0785]])),\n",
       "             ('encoder.layer.18.attention.self.key.bias',\n",
       "              tensor([ 0.0002,  0.0003,  0.0022,  ..., -0.0043,  0.0019,  0.0061])),\n",
       "             ('encoder.layer.18.attention.self.value.weight',\n",
       "              tensor([[ 0.0257,  0.0199, -0.0020,  ...,  0.0125,  0.0016,  0.0071],\n",
       "                      [-0.0346,  0.0596,  0.0168,  ...,  0.0101,  0.0037,  0.0246],\n",
       "                      [ 0.0109, -0.0405, -0.0327,  ...,  0.1033, -0.0280,  0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0123, -0.0056,  0.0166,  ...,  0.0234, -0.0264, -0.0351],\n",
       "                      [ 0.0363, -0.0037, -0.0435,  ...,  0.0092,  0.0319,  0.0016],\n",
       "                      [ 0.0259,  0.0686, -0.0159,  ..., -0.0423,  0.0176,  0.0026]])),\n",
       "             ('encoder.layer.18.attention.self.value.bias',\n",
       "              tensor([-0.0075,  0.0049, -0.0170,  ..., -0.0064,  0.0030, -0.0120])),\n",
       "             ('encoder.layer.18.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0308, -0.0016,  0.1382,  ...,  0.0225,  0.0675, -0.0144],\n",
       "                      [ 0.0326,  0.0206,  0.0118,  ..., -0.1445, -0.0501, -0.0424],\n",
       "                      [-0.0527,  0.0578,  0.1864,  ...,  0.0393, -0.0249, -0.0241],\n",
       "                      ...,\n",
       "                      [-0.0516, -0.0031, -0.0276,  ...,  0.0214, -0.0200, -0.0306],\n",
       "                      [-0.0628,  0.0321, -0.0133,  ...,  0.0152,  0.0528,  0.1090],\n",
       "                      [-0.0167,  0.0409,  0.1429,  ..., -0.0325,  0.0147, -0.0523]])),\n",
       "             ('encoder.layer.18.attention.self.w2e_query.bias',\n",
       "              tensor([-0.2749, -0.0114, -0.1438,  ...,  0.0094, -0.0061,  0.0200])),\n",
       "             ('encoder.layer.18.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0312, -0.0020,  0.1373,  ...,  0.0215,  0.0679, -0.0149],\n",
       "                      [ 0.0325,  0.0206,  0.0124,  ..., -0.1437, -0.0499, -0.0419],\n",
       "                      [-0.0536,  0.0582,  0.1862,  ...,  0.0396, -0.0251, -0.0236],\n",
       "                      ...,\n",
       "                      [-0.0514, -0.0030, -0.0274,  ...,  0.0215, -0.0203, -0.0314],\n",
       "                      [-0.0645,  0.0326, -0.0127,  ...,  0.0157,  0.0520,  0.1094],\n",
       "                      [-0.0181,  0.0415,  0.1426,  ..., -0.0311,  0.0135, -0.0525]])),\n",
       "             ('encoder.layer.18.attention.self.e2w_query.bias',\n",
       "              tensor([-0.2744, -0.0119, -0.1432,  ...,  0.0094, -0.0058,  0.0204])),\n",
       "             ('encoder.layer.18.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0307, -0.0016,  0.1377,  ...,  0.0223,  0.0677, -0.0148],\n",
       "                      [ 0.0331,  0.0205,  0.0126,  ..., -0.1440, -0.0498, -0.0421],\n",
       "                      [-0.0530,  0.0579,  0.1863,  ...,  0.0394, -0.0252, -0.0237],\n",
       "                      ...,\n",
       "                      [-0.0513, -0.0036, -0.0270,  ...,  0.0216, -0.0195, -0.0315],\n",
       "                      [-0.0638,  0.0327, -0.0135,  ...,  0.0155,  0.0523,  0.1093],\n",
       "                      [-0.0182,  0.0419,  0.1423,  ..., -0.0317,  0.0130, -0.0525]])),\n",
       "             ('encoder.layer.18.attention.self.e2e_query.bias',\n",
       "              tensor([-0.2747, -0.0121, -0.1434,  ...,  0.0091, -0.0052,  0.0210])),\n",
       "             ('encoder.layer.18.attention.output.dense.weight',\n",
       "              tensor([[-0.0367, -0.0226, -0.0600,  ..., -0.0258, -0.0083, -0.0373],\n",
       "                      [-0.0209,  0.0700, -0.0516,  ..., -0.0005, -0.0236,  0.0150],\n",
       "                      [-0.0378,  0.0180, -0.0059,  ...,  0.0090, -0.0240, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0329,  0.0092,  0.0703,  ..., -0.0055,  0.0251,  0.0186],\n",
       "                      [-0.0049,  0.0205,  0.0229,  ..., -0.0155,  0.0012,  0.0128],\n",
       "                      [ 0.0003,  0.0100, -0.0009,  ..., -0.0007,  0.0266,  0.0232]])),\n",
       "             ('encoder.layer.18.attention.output.dense.bias',\n",
       "              tensor([-0.0156,  0.0790,  0.0290,  ...,  0.0712,  0.0370,  0.0011])),\n",
       "             ('encoder.layer.18.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9844, 0.9858, 1.0068,  ..., 0.9888, 0.9775, 0.9883])),\n",
       "             ('encoder.layer.18.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0967, -0.0235, -0.2546,  ..., -0.0727, -0.0626, -0.0679])),\n",
       "             ('encoder.layer.18.intermediate.dense.weight',\n",
       "              tensor([[ 0.0531, -0.0237,  0.0333,  ...,  0.0265,  0.0056, -0.0266],\n",
       "                      [ 0.0641, -0.0410,  0.0074,  ..., -0.0130,  0.0010, -0.0476],\n",
       "                      [ 0.0182, -0.0307, -0.0296,  ..., -0.0369,  0.0098, -0.0245],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0610,  0.0417,  ...,  0.0286,  0.0301,  0.0031],\n",
       "                      [-0.0314, -0.0527,  0.0035,  ...,  0.0367, -0.0278,  0.0063],\n",
       "                      [ 0.0405,  0.0546, -0.0630,  ...,  0.0055,  0.0096, -0.0022]])),\n",
       "             ('encoder.layer.18.intermediate.dense.bias',\n",
       "              tensor([-0.0728, -0.1020, -0.0056,  ..., -0.0469, -0.0974, -0.0814])),\n",
       "             ('encoder.layer.18.output.dense.weight',\n",
       "              tensor([[-0.0396,  0.0020,  0.0182,  ...,  0.0207, -0.0287, -0.0345],\n",
       "                      [-0.0392, -0.0367, -0.0211,  ...,  0.0434, -0.0288, -0.0079],\n",
       "                      [ 0.0152, -0.0046, -0.0080,  ..., -0.0103, -0.0041, -0.0163],\n",
       "                      ...,\n",
       "                      [ 0.0177,  0.0180,  0.0403,  ..., -0.0037, -0.0016, -0.0109],\n",
       "                      [-0.0109, -0.0129, -0.0012,  ..., -0.0042,  0.0506,  0.0118],\n",
       "                      [-0.0299, -0.0143,  0.0425,  ...,  0.0028, -0.0568, -0.0229]])),\n",
       "             ('encoder.layer.18.output.dense.bias',\n",
       "              tensor([-0.0771,  0.0003, -0.1595,  ...,  0.0427, -0.0415, -0.0423])),\n",
       "             ('encoder.layer.18.output.LayerNorm.weight',\n",
       "              tensor([0.9824, 0.9985, 1.0059,  ..., 0.9810, 0.9873, 0.9800])),\n",
       "             ('encoder.layer.18.output.LayerNorm.bias',\n",
       "              tensor([-0.0076, -0.0493,  0.1214,  ..., -0.0184, -0.0269, -0.0289])),\n",
       "             ('encoder.layer.19.attention.self.query.weight',\n",
       "              tensor([[-0.0494, -0.0257, -0.0154,  ..., -0.0724,  0.0584, -0.1398],\n",
       "                      [ 0.0345,  0.0157, -0.1326,  ...,  0.0322,  0.0449,  0.0746],\n",
       "                      [-0.0240, -0.0155,  0.0447,  ..., -0.0638, -0.0773,  0.0163],\n",
       "                      ...,\n",
       "                      [-0.0046, -0.0125, -0.1968,  ...,  0.0892, -0.0090, -0.0332],\n",
       "                      [-0.1260,  0.0588,  0.0543,  ..., -0.0107,  0.0589, -0.0865],\n",
       "                      [-0.0288,  0.0571,  0.0611,  ...,  0.1001,  0.0637, -0.0143]])),\n",
       "             ('encoder.layer.19.attention.self.query.bias',\n",
       "              tensor([-0.0135, -0.0614, -0.0117,  ..., -0.0227,  0.0388, -0.0207])),\n",
       "             ('encoder.layer.19.attention.self.key.weight',\n",
       "              tensor([[-0.0225,  0.0094, -0.0019,  ...,  0.0379, -0.0851, -0.0089],\n",
       "                      [ 0.0622,  0.0584, -0.0960,  ...,  0.0609, -0.0431,  0.0600],\n",
       "                      [-0.0529, -0.0144,  0.0392,  ...,  0.0244,  0.1096, -0.0484],\n",
       "                      ...,\n",
       "                      [-0.0778, -0.0218, -0.2588,  ...,  0.0128,  0.0647,  0.0310],\n",
       "                      [-0.0156,  0.0067,  0.1630,  ...,  0.0240,  0.0862,  0.0146],\n",
       "                      [ 0.0134, -0.0760,  0.1261,  ..., -0.0471, -0.0170,  0.0728]])),\n",
       "             ('encoder.layer.19.attention.self.key.bias',\n",
       "              tensor([ 0.0070, -0.0076,  0.0174,  ..., -0.0101, -0.0190,  0.0177])),\n",
       "             ('encoder.layer.19.attention.self.value.weight',\n",
       "              tensor([[ 0.0265,  0.0144,  0.0313,  ...,  0.0293,  0.0184,  0.0257],\n",
       "                      [-0.0121,  0.0083, -0.0035,  ...,  0.0392,  0.0278, -0.0225],\n",
       "                      [-0.0375, -0.0092,  0.0154,  ...,  0.0316, -0.0218, -0.0326],\n",
       "                      ...,\n",
       "                      [ 0.0145,  0.0169,  0.0109,  ..., -0.0225, -0.0126, -0.0178],\n",
       "                      [ 0.0264,  0.0260, -0.0373,  ...,  0.0266, -0.0201, -0.0450],\n",
       "                      [-0.0201,  0.0596, -0.0245,  ...,  0.0354,  0.0258, -0.0063]])),\n",
       "             ('encoder.layer.19.attention.self.value.bias',\n",
       "              tensor([ 3.0174e-03,  6.9275e-02,  3.5071e-04,  ..., -3.7098e-03,\n",
       "                       6.0618e-05, -1.7593e-02])),\n",
       "             ('encoder.layer.19.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0500, -0.0252, -0.0150,  ..., -0.0726,  0.0582, -0.1396],\n",
       "                      [ 0.0341,  0.0155, -0.1322,  ...,  0.0320,  0.0453,  0.0739],\n",
       "                      [-0.0241, -0.0164,  0.0439,  ..., -0.0631, -0.0767,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0045, -0.0131, -0.1970,  ...,  0.0890, -0.0089, -0.0335],\n",
       "                      [-0.1260,  0.0584,  0.0540,  ..., -0.0111,  0.0589, -0.0862],\n",
       "                      [-0.0275,  0.0560,  0.0606,  ...,  0.0992,  0.0646, -0.0158]])),\n",
       "             ('encoder.layer.19.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0136, -0.0610, -0.0111,  ..., -0.0235,  0.0390, -0.0218])),\n",
       "             ('encoder.layer.19.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0503, -0.0251, -0.0154,  ..., -0.0720,  0.0580, -0.1395],\n",
       "                      [ 0.0339,  0.0148, -0.1318,  ...,  0.0329,  0.0457,  0.0741],\n",
       "                      [-0.0236, -0.0161,  0.0439,  ..., -0.0634, -0.0760,  0.0161],\n",
       "                      ...,\n",
       "                      [-0.0044, -0.0131, -0.1967,  ...,  0.0901, -0.0094, -0.0330],\n",
       "                      [-0.1262,  0.0578,  0.0543,  ..., -0.0117,  0.0596, -0.0865],\n",
       "                      [-0.0278,  0.0565,  0.0616,  ...,  0.0999,  0.0643, -0.0152]])),\n",
       "             ('encoder.layer.19.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0135, -0.0617, -0.0112,  ..., -0.0238,  0.0389, -0.0220])),\n",
       "             ('encoder.layer.19.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0498, -0.0256, -0.0152,  ..., -0.0721,  0.0583, -0.1393],\n",
       "                      [ 0.0337,  0.0162, -0.1332,  ...,  0.0316,  0.0450,  0.0745],\n",
       "                      [-0.0246, -0.0158,  0.0433,  ..., -0.0641, -0.0771,  0.0161],\n",
       "                      ...,\n",
       "                      [-0.0043, -0.0128, -0.1969,  ...,  0.0898, -0.0094, -0.0330],\n",
       "                      [-0.1261,  0.0580,  0.0541,  ..., -0.0116,  0.0596, -0.0866],\n",
       "                      [-0.0270,  0.0565,  0.0615,  ...,  0.1002,  0.0643, -0.0152]])),\n",
       "             ('encoder.layer.19.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0136, -0.0602, -0.0104,  ..., -0.0232,  0.0385, -0.0220])),\n",
       "             ('encoder.layer.19.attention.output.dense.weight',\n",
       "              tensor([[-0.0027,  0.0258, -0.0212,  ...,  0.0243,  0.0151, -0.0046],\n",
       "                      [ 0.0063, -0.0319, -0.0446,  ..., -0.0044,  0.0387,  0.0027],\n",
       "                      [ 0.0010, -0.0481, -0.0291,  ...,  0.0385, -0.0255,  0.0137],\n",
       "                      ...,\n",
       "                      [-0.0619, -0.0606, -0.0305,  ...,  0.0019, -0.0008,  0.0151],\n",
       "                      [-0.0196,  0.0453,  0.0075,  ...,  0.0130,  0.0197, -0.0075],\n",
       "                      [-0.0241, -0.0363,  0.0073,  ..., -0.0078, -0.0267, -0.0442]])),\n",
       "             ('encoder.layer.19.attention.output.dense.bias',\n",
       "              tensor([ 0.0030,  0.0602,  0.0794,  ..., -0.0011,  0.0254,  0.0068])),\n",
       "             ('encoder.layer.19.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9785, 0.9893, 1.0078,  ..., 0.9731, 0.9873, 0.9927])),\n",
       "             ('encoder.layer.19.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0815, -0.0384, -0.2178,  ..., -0.0638, -0.0837, -0.0627])),\n",
       "             ('encoder.layer.19.intermediate.dense.weight',\n",
       "              tensor([[ 0.0501, -0.0396,  0.0133,  ...,  0.0851,  0.0759,  0.1544],\n",
       "                      [ 0.0516, -0.0094,  0.0536,  ...,  0.0216,  0.0282,  0.0237],\n",
       "                      [ 0.0202,  0.0144,  0.0355,  ..., -0.0170, -0.0037,  0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0777, -0.0235,  0.0235,  ..., -0.0045, -0.0512,  0.0404],\n",
       "                      [ 0.0240, -0.0102, -0.0387,  ..., -0.0165,  0.0339, -0.0841],\n",
       "                      [ 0.0152,  0.0425,  0.0344,  ..., -0.0325, -0.0134,  0.0352]])),\n",
       "             ('encoder.layer.19.intermediate.dense.bias',\n",
       "              tensor([-0.1025,  0.0120, -0.0414,  ..., -0.1160,  0.0042, -0.0175])),\n",
       "             ('encoder.layer.19.output.dense.weight',\n",
       "              tensor([[ 0.0163, -0.0167, -0.0239,  ..., -0.0398, -0.0058,  0.0093],\n",
       "                      [-0.0569, -0.0376,  0.0096,  ...,  0.0515, -0.0132,  0.0199],\n",
       "                      [ 0.0133, -0.0144,  0.0024,  ..., -0.0162,  0.0085, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0079, -0.0095,  0.0257,  ..., -0.0509,  0.0146,  0.0218],\n",
       "                      [ 0.0269, -0.0103, -0.0317,  ..., -0.0216, -0.0056, -0.0068],\n",
       "                      [ 0.0063,  0.0131, -0.0023,  ...,  0.0360,  0.0397,  0.0316]])),\n",
       "             ('encoder.layer.19.output.dense.bias',\n",
       "              tensor([-0.0396,  0.0266, -0.0498,  ...,  0.1116, -0.0568,  0.0031])),\n",
       "             ('encoder.layer.19.output.LayerNorm.weight',\n",
       "              tensor([0.9629, 0.9946, 1.0049,  ..., 0.9775, 0.9912, 0.9829])),\n",
       "             ('encoder.layer.19.output.LayerNorm.bias',\n",
       "              tensor([-0.0115, -0.0338,  0.0751,  ..., -0.0206, -0.0154, -0.0063])),\n",
       "             ('encoder.layer.20.attention.self.query.weight',\n",
       "              tensor([[-0.0135,  0.0141, -0.0103,  ...,  0.0032, -0.0529,  0.0183],\n",
       "                      [-0.0717, -0.0754,  0.0288,  ..., -0.0293, -0.0356, -0.0032],\n",
       "                      [-0.0204, -0.0227,  0.0008,  ...,  0.0310,  0.0418, -0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0937, -0.0094, -0.0301,  ...,  0.0708, -0.0226,  0.0198],\n",
       "                      [ 0.0023,  0.0027,  0.1947,  ..., -0.0231,  0.0257,  0.0019],\n",
       "                      [-0.0258,  0.0158, -0.1173,  ..., -0.0471, -0.0370, -0.0479]])),\n",
       "             ('encoder.layer.20.attention.self.query.bias',\n",
       "              tensor([-0.0265,  0.0259,  0.0136,  ...,  0.0921, -0.0348,  0.0515])),\n",
       "             ('encoder.layer.20.attention.self.key.weight',\n",
       "              tensor([[ 0.0144, -0.0725, -0.0235,  ..., -0.1069, -0.0224, -0.0865],\n",
       "                      [-0.0043, -0.0015,  0.0377,  ..., -0.0191,  0.0018, -0.0090],\n",
       "                      [ 0.0270,  0.0513, -0.0859,  ...,  0.1076,  0.0244,  0.0363],\n",
       "                      ...,\n",
       "                      [-0.0400, -0.0552, -0.1095,  ..., -0.0244, -0.0118,  0.0293],\n",
       "                      [-0.0255,  0.0522,  0.2231,  ...,  0.0243, -0.0611,  0.0279],\n",
       "                      [-0.0188,  0.0211, -0.0475,  ..., -0.0924, -0.0144,  0.0152]])),\n",
       "             ('encoder.layer.20.attention.self.key.bias',\n",
       "              tensor([-0.0054, -0.0048,  0.0036,  ..., -0.0118,  0.0155, -0.0161])),\n",
       "             ('encoder.layer.20.attention.self.value.weight',\n",
       "              tensor([[-0.0486, -0.0069,  0.0269,  ..., -0.0248, -0.0012,  0.0612],\n",
       "                      [-0.0159, -0.0032, -0.0187,  ..., -0.0260,  0.1050,  0.0197],\n",
       "                      [ 0.0007,  0.0330, -0.0116,  ..., -0.0687, -0.0788,  0.0149],\n",
       "                      ...,\n",
       "                      [-0.0618,  0.0314,  0.0218,  ...,  0.0701,  0.0107,  0.0195],\n",
       "                      [ 0.0903,  0.0067,  0.0598,  ..., -0.0047,  0.0148, -0.0497],\n",
       "                      [ 0.0185,  0.0395, -0.0121,  ..., -0.0281, -0.0061, -0.0716]])),\n",
       "             ('encoder.layer.20.attention.self.value.bias',\n",
       "              tensor([ 0.0001,  0.0057,  0.0203,  ...,  0.0018,  0.0026, -0.0055])),\n",
       "             ('encoder.layer.20.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0133,  0.0146, -0.0095,  ...,  0.0044, -0.0528,  0.0183],\n",
       "                      [-0.0706, -0.0753,  0.0301,  ..., -0.0288, -0.0353, -0.0044],\n",
       "                      [-0.0209, -0.0238, -0.0005,  ...,  0.0310,  0.0411, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0939, -0.0083, -0.0293,  ...,  0.0735, -0.0219,  0.0208],\n",
       "                      [ 0.0016,  0.0031,  0.1948,  ..., -0.0232,  0.0259,  0.0027],\n",
       "                      [-0.0251,  0.0149, -0.1173,  ..., -0.0484, -0.0371, -0.0480]])),\n",
       "             ('encoder.layer.20.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0268,  0.0252,  0.0147,  ...,  0.0913, -0.0343,  0.0510])),\n",
       "             ('encoder.layer.20.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0133,  0.0148, -0.0097,  ...,  0.0042, -0.0531,  0.0182],\n",
       "                      [-0.0715, -0.0742,  0.0296,  ..., -0.0288, -0.0370, -0.0040],\n",
       "                      [-0.0203, -0.0241,  0.0003,  ...,  0.0310,  0.0427, -0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0944, -0.0080, -0.0280,  ...,  0.0741, -0.0224,  0.0201],\n",
       "                      [ 0.0023,  0.0027,  0.1956,  ..., -0.0241,  0.0258,  0.0021],\n",
       "                      [-0.0256,  0.0147, -0.1180,  ..., -0.0476, -0.0369, -0.0470]])),\n",
       "             ('encoder.layer.20.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0264,  0.0260,  0.0138,  ...,  0.0901, -0.0344,  0.0513])),\n",
       "             ('encoder.layer.20.attention.self.e2e_query.weight',\n",
       "              tensor([[-1.3809e-02,  1.5686e-02, -1.0017e-02,  ...,  3.9482e-03,\n",
       "                       -5.3375e-02,  1.8295e-02],\n",
       "                      [-7.1411e-02, -7.4097e-02,  2.9404e-02,  ..., -2.9205e-02,\n",
       "                       -3.5736e-02, -4.2763e-03],\n",
       "                      [-2.0996e-02, -2.3773e-02,  2.3782e-05,  ...,  3.1204e-02,\n",
       "                        4.1504e-02, -1.9779e-03],\n",
       "                      ...,\n",
       "                      [ 9.3811e-02, -8.9722e-03, -2.9236e-02,  ...,  7.2815e-02,\n",
       "                       -2.2339e-02,  2.0401e-02],\n",
       "                      [ 1.6937e-03,  2.2144e-03,  1.9470e-01,  ..., -2.4597e-02,\n",
       "                        2.5742e-02,  2.0771e-03],\n",
       "                      [-2.5223e-02,  1.5930e-02, -1.1755e-01,  ..., -4.6814e-02,\n",
       "                       -3.6987e-02, -4.7394e-02]])),\n",
       "             ('encoder.layer.20.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0261,  0.0258,  0.0140,  ...,  0.0910, -0.0342,  0.0512])),\n",
       "             ('encoder.layer.20.attention.output.dense.weight',\n",
       "              tensor([[ 0.0141, -0.0607, -0.0506,  ...,  0.0567, -0.0533, -0.0154],\n",
       "                      [ 0.0120,  0.0442,  0.0133,  ..., -0.0169, -0.0104, -0.0556],\n",
       "                      [ 0.0259,  0.0087,  0.0064,  ..., -0.0156, -0.0530, -0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0406, -0.0266, -0.0499,  ..., -0.0740, -0.0228,  0.0360],\n",
       "                      [ 0.0176,  0.0038,  0.0066,  ..., -0.0204, -0.0138,  0.0309],\n",
       "                      [ 0.0114,  0.0019, -0.0138,  ...,  0.0191,  0.0138,  0.0343]])),\n",
       "             ('encoder.layer.20.attention.output.dense.bias',\n",
       "              tensor([-0.0117,  0.0777,  0.0611,  ...,  0.0725,  0.0870,  0.0222])),\n",
       "             ('encoder.layer.20.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9702, 0.9956, 1.0078,  ..., 0.9795, 0.9868, 0.9917])),\n",
       "             ('encoder.layer.20.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0316, -0.0351, -0.1602,  ..., -0.0451, -0.0892, -0.0469])),\n",
       "             ('encoder.layer.20.intermediate.dense.weight',\n",
       "              tensor([[ 0.0089, -0.0098,  0.0570,  ...,  0.0077,  0.0101,  0.0077],\n",
       "                      [ 0.0179, -0.0154,  0.0016,  ...,  0.0103, -0.0649,  0.0677],\n",
       "                      [-0.0117, -0.0205, -0.0567,  ..., -0.0200, -0.0344, -0.0379],\n",
       "                      ...,\n",
       "                      [ 0.0060, -0.0305,  0.0015,  ...,  0.0406,  0.0907,  0.0205],\n",
       "                      [ 0.0596,  0.0465,  0.0591,  ..., -0.0485, -0.0080, -0.0314],\n",
       "                      [-0.0179, -0.0428,  0.0470,  ..., -0.0086,  0.0035, -0.0075]])),\n",
       "             ('encoder.layer.20.intermediate.dense.bias',\n",
       "              tensor([-0.0217, -0.0353, -0.0472,  ..., -0.0266, -0.0187, -0.0252])),\n",
       "             ('encoder.layer.20.output.dense.weight',\n",
       "              tensor([[-0.0155,  0.0661,  0.0181,  ...,  0.0620, -0.0085, -0.0115],\n",
       "                      [ 0.0088,  0.0311,  0.0507,  ...,  0.0040,  0.0139, -0.0374],\n",
       "                      [-0.0135, -0.0023, -0.0088,  ..., -0.0088, -0.0088, -0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0119,  0.0092,  0.0508,  ..., -0.0165,  0.0142, -0.0054],\n",
       "                      [-0.0202, -0.0485,  0.0234,  ...,  0.0297,  0.0590,  0.0206],\n",
       "                      [-0.0327,  0.0060,  0.0361,  ...,  0.0190,  0.0394, -0.0054]])),\n",
       "             ('encoder.layer.20.output.dense.bias',\n",
       "              tensor([-0.0567,  0.0044, -0.0077,  ...,  0.0624, -0.0947,  0.0487])),\n",
       "             ('encoder.layer.20.output.LayerNorm.weight',\n",
       "              tensor([0.9624, 0.9907, 1.0049,  ..., 0.9785, 0.9854, 0.9761])),\n",
       "             ('encoder.layer.20.output.LayerNorm.bias',\n",
       "              tensor([-0.0321, -0.0316,  0.0782,  ..., -0.0300, -0.0230, -0.0139])),\n",
       "             ('encoder.layer.21.attention.self.query.weight',\n",
       "              tensor([[ 0.0313, -0.0815, -0.2120,  ..., -0.0113, -0.0081, -0.0399],\n",
       "                      [-0.0277, -0.0828,  0.0516,  ..., -0.0235,  0.0214,  0.0026],\n",
       "                      [ 0.0591,  0.0351,  0.0579,  ...,  0.0312, -0.0441, -0.0711],\n",
       "                      ...,\n",
       "                      [ 0.0060,  0.0133, -0.1488,  ...,  0.0714,  0.0658,  0.0537],\n",
       "                      [-0.0631,  0.0123,  0.0345,  ...,  0.0069,  0.0118,  0.0064],\n",
       "                      [-0.0428, -0.0414,  0.0454,  ...,  0.0176, -0.0244,  0.0657]])),\n",
       "             ('encoder.layer.21.attention.self.query.bias',\n",
       "              tensor([ 0.1652, -0.0307, -0.0158,  ..., -0.0682,  0.0426, -0.0465])),\n",
       "             ('encoder.layer.21.attention.self.key.weight',\n",
       "              tensor([[ 0.0140,  0.0523, -0.1556,  ...,  0.0720,  0.0301,  0.0097],\n",
       "                      [ 0.0472,  0.0454,  0.0679,  ...,  0.0105,  0.1289, -0.0372],\n",
       "                      [ 0.0011, -0.0027, -0.0004,  ..., -0.0781,  0.0145, -0.0092],\n",
       "                      ...,\n",
       "                      [-0.0558, -0.0397, -0.1234,  ...,  0.0264, -0.0538,  0.0315],\n",
       "                      [ 0.0833, -0.0176,  0.0697,  ..., -0.0969,  0.0291, -0.0070],\n",
       "                      [-0.0284,  0.0186,  0.0795,  ..., -0.0420,  0.0089, -0.0056]])),\n",
       "             ('encoder.layer.21.attention.self.key.bias',\n",
       "              tensor([-0.0618, -0.0095,  0.0029,  ..., -0.0012,  0.0003,  0.0061])),\n",
       "             ('encoder.layer.21.attention.self.value.weight',\n",
       "              tensor([[ 0.0320, -0.0331, -0.0540,  ..., -0.0317,  0.0140, -0.0276],\n",
       "                      [ 0.0392, -0.0499, -0.0010,  ...,  0.0255, -0.0567,  0.0702],\n",
       "                      [ 0.0150, -0.0408, -0.0123,  ...,  0.0084, -0.0396,  0.0258],\n",
       "                      ...,\n",
       "                      [ 0.0247, -0.0698, -0.0109,  ..., -0.0315, -0.0745, -0.0319],\n",
       "                      [ 0.0282, -0.0320, -0.0421,  ...,  0.0715, -0.0160,  0.0277],\n",
       "                      [-0.0226, -0.0001,  0.0956,  ..., -0.0317, -0.0220,  0.0236]])),\n",
       "             ('encoder.layer.21.attention.self.value.bias',\n",
       "              tensor([ 0.0071, -0.0036, -0.0004,  ..., -0.0107, -0.0255, -0.0129])),\n",
       "             ('encoder.layer.21.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0313, -0.0810, -0.2109,  ..., -0.0106, -0.0086, -0.0404],\n",
       "                      [-0.0278, -0.0827,  0.0525,  ..., -0.0221,  0.0214,  0.0026],\n",
       "                      [ 0.0590,  0.0358,  0.0587,  ...,  0.0324, -0.0428, -0.0715],\n",
       "                      ...,\n",
       "                      [ 0.0069,  0.0118, -0.1499,  ...,  0.0717,  0.0670,  0.0523],\n",
       "                      [-0.0630,  0.0124,  0.0348,  ...,  0.0073,  0.0112,  0.0061],\n",
       "                      [-0.0436, -0.0406,  0.0455,  ...,  0.0173, -0.0254,  0.0656]])),\n",
       "             ('encoder.layer.21.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.1646, -0.0317, -0.0166,  ..., -0.0676,  0.0429, -0.0446])),\n",
       "             ('encoder.layer.21.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0313, -0.0813, -0.2122,  ..., -0.0117, -0.0089, -0.0405],\n",
       "                      [-0.0273, -0.0820,  0.0516,  ..., -0.0233,  0.0215,  0.0021],\n",
       "                      [ 0.0584,  0.0349,  0.0594,  ...,  0.0333, -0.0430, -0.0711],\n",
       "                      ...,\n",
       "                      [ 0.0069,  0.0120, -0.1493,  ...,  0.0715,  0.0668,  0.0538],\n",
       "                      [-0.0625,  0.0132,  0.0349,  ...,  0.0070,  0.0119,  0.0068],\n",
       "                      [-0.0437, -0.0402,  0.0458,  ...,  0.0179, -0.0257,  0.0662]])),\n",
       "             ('encoder.layer.21.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.1658, -0.0312, -0.0172,  ..., -0.0682,  0.0421, -0.0457])),\n",
       "             ('encoder.layer.21.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0315, -0.0814, -0.2109,  ..., -0.0107, -0.0084, -0.0399],\n",
       "                      [-0.0273, -0.0832,  0.0533,  ..., -0.0219,  0.0218,  0.0027],\n",
       "                      [ 0.0585,  0.0361,  0.0583,  ...,  0.0324, -0.0433, -0.0723],\n",
       "                      ...,\n",
       "                      [ 0.0064,  0.0116, -0.1495,  ...,  0.0717,  0.0671,  0.0526],\n",
       "                      [-0.0641,  0.0124,  0.0349,  ...,  0.0071,  0.0113,  0.0065],\n",
       "                      [-0.0432, -0.0412,  0.0452,  ...,  0.0172, -0.0250,  0.0657]])),\n",
       "             ('encoder.layer.21.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.1649, -0.0322, -0.0164,  ..., -0.0685,  0.0426, -0.0457])),\n",
       "             ('encoder.layer.21.attention.output.dense.weight',\n",
       "              tensor([[-0.0025,  0.0458, -0.0312,  ..., -0.0226,  0.0055,  0.0366],\n",
       "                      [ 0.0351,  0.0163, -0.0299,  ...,  0.0084,  0.0271, -0.0189],\n",
       "                      [ 0.0117, -0.0417, -0.0055,  ..., -0.0131,  0.0153, -0.0416],\n",
       "                      ...,\n",
       "                      [ 0.0182,  0.0421, -0.0054,  ...,  0.0201, -0.0207,  0.0496],\n",
       "                      [-0.0484,  0.0739, -0.0655,  ..., -0.0018, -0.0438, -0.0179],\n",
       "                      [ 0.0147,  0.0130, -0.0577,  ...,  0.0553, -0.0240, -0.0257]])),\n",
       "             ('encoder.layer.21.attention.output.dense.bias',\n",
       "              tensor([-0.0268,  0.0895,  0.0458,  ...,  0.0353,  0.1769,  0.0312])),\n",
       "             ('encoder.layer.21.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9673, 0.9897, 1.0059,  ..., 0.9775, 0.9922, 0.9893])),\n",
       "             ('encoder.layer.21.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0484, -0.0605, -0.1381,  ..., -0.0503, -0.1013, -0.0357])),\n",
       "             ('encoder.layer.21.intermediate.dense.weight',\n",
       "              tensor([[-0.0125, -0.0347,  0.0392,  ...,  0.0515,  0.0213, -0.0267],\n",
       "                      [ 0.0027,  0.0022, -0.0748,  ...,  0.0260,  0.0011,  0.0546],\n",
       "                      [ 0.0002,  0.0395, -0.0457,  ..., -0.0037,  0.0085,  0.0097],\n",
       "                      ...,\n",
       "                      [-0.0079, -0.0183,  0.0262,  ..., -0.0218, -0.0705, -0.0249],\n",
       "                      [ 0.0079, -0.0706,  0.0151,  ..., -0.0226,  0.0091, -0.0209],\n",
       "                      [ 0.0024, -0.0013,  0.0187,  ..., -0.0108,  0.0338,  0.0370]])),\n",
       "             ('encoder.layer.21.intermediate.dense.bias',\n",
       "              tensor([-0.0268, -0.0455,  0.0202,  ..., -0.0285, -0.0021, -0.0143])),\n",
       "             ('encoder.layer.21.output.dense.weight',\n",
       "              tensor([[-1.4450e-02, -2.0564e-05, -4.2229e-03,  ...,  3.7720e-02,\n",
       "                       -2.1622e-02, -3.8422e-02],\n",
       "                      [ 2.3479e-03,  1.0132e-02, -3.6865e-02,  ...,  1.4954e-02,\n",
       "                       -3.3997e-02,  5.3680e-02],\n",
       "                      [-1.4687e-02,  3.7785e-03,  4.2953e-03,  ...,  1.0460e-02,\n",
       "                        7.3051e-03,  1.3718e-02],\n",
       "                      ...,\n",
       "                      [-2.8732e-02,  1.1368e-02,  2.6184e-02,  ..., -1.7822e-02,\n",
       "                       -1.1414e-02, -4.9210e-04],\n",
       "                      [-3.3173e-02,  1.6558e-04, -3.9825e-02,  ..., -3.5004e-02,\n",
       "                       -1.0461e-01,  3.9597e-03],\n",
       "                      [-9.4452e-03,  6.1989e-05,  3.2215e-03,  ..., -3.3455e-03,\n",
       "                       -2.9739e-02,  2.5444e-03]])),\n",
       "             ('encoder.layer.21.output.dense.bias',\n",
       "              tensor([-0.0097,  0.0060, -0.0721,  ...,  0.0911, -0.1213,  0.0145])),\n",
       "             ('encoder.layer.21.output.LayerNorm.weight',\n",
       "              tensor([0.9683, 0.9893, 1.0020,  ..., 0.9829, 0.9863, 0.9917])),\n",
       "             ('encoder.layer.21.output.LayerNorm.bias',\n",
       "              tensor([-0.0446, -0.0148,  0.0367,  ..., -0.0397, -0.0576, -0.0172])),\n",
       "             ('encoder.layer.22.attention.self.query.weight',\n",
       "              tensor([[ 0.0027,  0.0379,  0.0845,  ...,  0.0141,  0.0215,  0.0627],\n",
       "                      [ 0.0147, -0.0403,  0.0055,  ..., -0.1011,  0.0528,  0.0120],\n",
       "                      [ 0.1000, -0.0248,  0.1840,  ...,  0.0100,  0.0579, -0.0062],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0108, -0.0818,  ..., -0.0722, -0.0103,  0.0692],\n",
       "                      [-0.0422,  0.0748, -0.0131,  ...,  0.0585, -0.0373, -0.0318],\n",
       "                      [ 0.0331,  0.0579,  0.1002,  ...,  0.0266, -0.0220, -0.0792]])),\n",
       "             ('encoder.layer.22.attention.self.query.bias',\n",
       "              tensor([-0.0043,  0.0063,  0.1520,  ...,  0.0035,  0.1005, -0.0566])),\n",
       "             ('encoder.layer.22.attention.self.key.weight',\n",
       "              tensor([[-0.0690, -0.0500,  0.0703,  ..., -0.0008, -0.0575,  0.0455],\n",
       "                      [ 0.0248, -0.0253,  0.0254,  ...,  0.0299, -0.0143,  0.0128],\n",
       "                      [ 0.0058,  0.0305,  0.0906,  ...,  0.0168, -0.0210,  0.0133],\n",
       "                      ...,\n",
       "                      [ 0.0836, -0.0033, -0.1017,  ..., -0.0584, -0.0534, -0.0911],\n",
       "                      [ 0.0242,  0.0667, -0.0609,  ...,  0.0342,  0.0382, -0.0350],\n",
       "                      [-0.0486,  0.0172, -0.0297,  ..., -0.0002,  0.0193, -0.0102]])),\n",
       "             ('encoder.layer.22.attention.self.key.bias',\n",
       "              tensor([-0.0028,  0.0007, -0.0029,  ...,  0.0049, -0.0011, -0.0003])),\n",
       "             ('encoder.layer.22.attention.self.value.weight',\n",
       "              tensor([[ 0.0247,  0.0353,  0.0444,  ...,  0.0455, -0.0217,  0.0506],\n",
       "                      [-0.0467, -0.0139, -0.0150,  ...,  0.0509, -0.0746,  0.0573],\n",
       "                      [-0.0201,  0.0293,  0.0070,  ..., -0.0036, -0.0171,  0.0177],\n",
       "                      ...,\n",
       "                      [-0.0434,  0.0021, -0.0140,  ..., -0.0321,  0.0290, -0.0474],\n",
       "                      [ 0.0134,  0.0121,  0.0356,  ...,  0.0077,  0.0052, -0.0363],\n",
       "                      [-0.0213, -0.0169, -0.0398,  ..., -0.0293,  0.0153,  0.0217]])),\n",
       "             ('encoder.layer.22.attention.self.value.bias',\n",
       "              tensor([ 0.0083, -0.0132,  0.0137,  ..., -0.0119, -0.0051, -0.0074])),\n",
       "             ('encoder.layer.22.attention.self.w2e_query.weight',\n",
       "              tensor([[ 0.0027,  0.0377,  0.0852,  ...,  0.0142,  0.0212,  0.0618],\n",
       "                      [ 0.0148, -0.0396,  0.0044,  ..., -0.1011,  0.0522,  0.0117],\n",
       "                      [ 0.1010, -0.0232,  0.1851,  ...,  0.0107,  0.0584, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0107, -0.0817,  ..., -0.0717, -0.0110,  0.0698],\n",
       "                      [-0.0421,  0.0753, -0.0126,  ...,  0.0585, -0.0378, -0.0310],\n",
       "                      [ 0.0330,  0.0578,  0.0995,  ...,  0.0262, -0.0227, -0.0779]])),\n",
       "             ('encoder.layer.22.attention.self.w2e_query.bias',\n",
       "              tensor([-0.0048,  0.0069,  0.1509,  ...,  0.0028,  0.1001, -0.0558])),\n",
       "             ('encoder.layer.22.attention.self.e2w_query.weight',\n",
       "              tensor([[ 0.0026,  0.0376,  0.0854,  ...,  0.0149,  0.0217,  0.0614],\n",
       "                      [ 0.0148, -0.0399,  0.0049,  ..., -0.1002,  0.0526,  0.0118],\n",
       "                      [ 0.1011, -0.0236,  0.1849,  ...,  0.0100,  0.0576, -0.0078],\n",
       "                      ...,\n",
       "                      [-0.0073, -0.0104, -0.0812,  ..., -0.0717, -0.0103,  0.0692],\n",
       "                      [-0.0418,  0.0745, -0.0126,  ...,  0.0589, -0.0370, -0.0318],\n",
       "                      [ 0.0332,  0.0589,  0.1003,  ...,  0.0262, -0.0219, -0.0789]])),\n",
       "             ('encoder.layer.22.attention.self.e2w_query.bias',\n",
       "              tensor([-0.0046,  0.0064,  0.1508,  ...,  0.0027,  0.0996, -0.0565])),\n",
       "             ('encoder.layer.22.attention.self.e2e_query.weight',\n",
       "              tensor([[ 0.0026,  0.0377,  0.0861,  ...,  0.0155,  0.0211,  0.0614],\n",
       "                      [ 0.0147, -0.0394,  0.0043,  ..., -0.1008,  0.0518,  0.0119],\n",
       "                      [ 0.1009, -0.0234,  0.1849,  ...,  0.0101,  0.0583, -0.0072],\n",
       "                      ...,\n",
       "                      [-0.0078, -0.0103, -0.0817,  ..., -0.0718, -0.0112,  0.0698],\n",
       "                      [-0.0419,  0.0748, -0.0130,  ...,  0.0589, -0.0370, -0.0316],\n",
       "                      [ 0.0327,  0.0581,  0.1004,  ...,  0.0264, -0.0226, -0.0782]])),\n",
       "             ('encoder.layer.22.attention.self.e2e_query.bias',\n",
       "              tensor([-0.0050,  0.0067,  0.1514,  ...,  0.0032,  0.0996, -0.0564])),\n",
       "             ('encoder.layer.22.attention.output.dense.weight',\n",
       "              tensor([[ 0.0406,  0.0515, -0.0049,  ..., -0.0326, -0.0337, -0.0120],\n",
       "                      [ 0.0062,  0.0237,  0.0297,  ..., -0.0476,  0.0051,  0.0297],\n",
       "                      [ 0.0197,  0.0032, -0.0261,  ..., -0.0191,  0.0526, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0317,  0.0226, -0.0203,  ...,  0.0116, -0.0233, -0.0105],\n",
       "                      [ 0.0439, -0.0158,  0.0201,  ..., -0.0173, -0.0075, -0.0430],\n",
       "                      [ 0.0301,  0.0418,  0.0377,  ..., -0.0351,  0.0058, -0.0032]])),\n",
       "             ('encoder.layer.22.attention.output.dense.bias',\n",
       "              tensor([ 0.0753,  0.0949,  0.0677,  ...,  0.0082,  0.1453, -0.0301])),\n",
       "             ('encoder.layer.22.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9521, 0.9858, 0.9995,  ..., 0.9741, 0.9937, 0.9854])),\n",
       "             ('encoder.layer.22.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0498, -0.0141, -0.1243,  ..., -0.0382, -0.1030,  0.0334])),\n",
       "             ('encoder.layer.22.intermediate.dense.weight',\n",
       "              tensor([[-0.0006, -0.0699, -0.0486,  ..., -0.0094,  0.0135,  0.0815],\n",
       "                      [-0.0194, -0.0201,  0.0625,  ...,  0.0030,  0.0440,  0.0008],\n",
       "                      [-0.0282, -0.0662,  0.0743,  ...,  0.0126, -0.0145, -0.0492],\n",
       "                      ...,\n",
       "                      [-0.0321,  0.0190,  0.0443,  ...,  0.0499,  0.0122,  0.0396],\n",
       "                      [ 0.0627,  0.0158,  0.0451,  ...,  0.0217, -0.0021,  0.0066],\n",
       "                      [ 0.0054, -0.0773, -0.0609,  ..., -0.0082,  0.0189, -0.0474]])),\n",
       "             ('encoder.layer.22.intermediate.dense.bias',\n",
       "              tensor([ 0.0182, -0.0081, -0.0126,  ..., -0.0461,  0.0150, -0.0288])),\n",
       "             ('encoder.layer.22.output.dense.weight',\n",
       "              tensor([[-0.0263,  0.0348, -0.0259,  ...,  0.0071, -0.0061, -0.0853],\n",
       "                      [ 0.0141, -0.0254,  0.0232,  ..., -0.0068, -0.0297,  0.0171],\n",
       "                      [-0.0156, -0.0123, -0.0045,  ...,  0.0007, -0.0034,  0.0123],\n",
       "                      ...,\n",
       "                      [-0.0124, -0.0284,  0.0129,  ..., -0.0218, -0.0054,  0.0573],\n",
       "                      [ 0.0166, -0.0370,  0.0408,  ..., -0.0111, -0.0259, -0.0277],\n",
       "                      [-0.0075,  0.0702,  0.0308,  ..., -0.0215,  0.0051, -0.0247]])),\n",
       "             ('encoder.layer.22.output.dense.bias',\n",
       "              tensor([ 0.0238,  0.0466, -0.0745,  ...,  0.0241, -0.1318,  0.0461])),\n",
       "             ('encoder.layer.22.output.LayerNorm.weight',\n",
       "              tensor([0.9609, 0.9951, 1.0010,  ..., 0.9819, 0.9902, 0.9810])),\n",
       "             ('encoder.layer.22.output.LayerNorm.bias',\n",
       "              tensor([-0.0226,  0.0389,  0.0476,  ..., -0.0325, -0.0121,  0.0176])),\n",
       "             ('encoder.layer.23.attention.self.query.weight',\n",
       "              tensor([[-4.0314e-02,  1.9501e-02,  9.3323e-02,  ..., -8.6426e-02,\n",
       "                        8.3191e-02,  6.0547e-02],\n",
       "                      [-4.3365e-02,  1.8631e-02,  1.0114e-01,  ..., -7.4005e-03,\n",
       "                       -1.5900e-02, -2.0618e-03],\n",
       "                      [ 3.9764e-02,  4.3915e-02,  3.1433e-02,  ..., -8.6288e-03,\n",
       "                       -8.4595e-02, -3.3203e-02],\n",
       "                      ...,\n",
       "                      [ 9.9976e-02, -5.0201e-02,  1.2720e-01,  ...,  7.9727e-04,\n",
       "                       -7.8354e-03,  6.5002e-02],\n",
       "                      [-5.1636e-02, -6.2287e-05, -9.3269e-04,  ..., -4.9316e-02,\n",
       "                       -1.0574e-02, -1.5030e-02],\n",
       "                      [-3.7289e-03, -4.9347e-02, -5.7098e-02,  ...,  4.6631e-02,\n",
       "                       -6.2164e-02,  1.4992e-02]])),\n",
       "             ('encoder.layer.23.attention.self.query.bias',\n",
       "              tensor([ 0.0779,  0.2739,  0.0301,  ..., -0.2007,  0.1144,  0.0806])),\n",
       "             ('encoder.layer.23.attention.self.key.weight',\n",
       "              tensor([[ 0.0566, -0.0259,  0.2134,  ..., -0.0132,  0.0252,  0.0230],\n",
       "                      [ 0.0719, -0.0682,  0.0745,  ...,  0.0134, -0.0398,  0.0105],\n",
       "                      [-0.1116, -0.0293, -0.0365,  ..., -0.1260,  0.0406, -0.0294],\n",
       "                      ...,\n",
       "                      [ 0.0476,  0.0104,  0.0692,  ..., -0.0438,  0.0012,  0.0547],\n",
       "                      [ 0.0222,  0.0122, -0.0670,  ..., -0.0203,  0.0358,  0.0352],\n",
       "                      [-0.0143, -0.0095, -0.0158,  ...,  0.1092,  0.0035,  0.0642]])),\n",
       "             ('encoder.layer.23.attention.self.key.bias',\n",
       "              tensor([-0.0166, -0.0016, -0.0149,  ...,  0.0866, -0.0135,  0.0022])),\n",
       "             ('encoder.layer.23.attention.self.value.weight',\n",
       "              tensor([[-0.0673, -0.0646, -0.0130,  ..., -0.0350,  0.0011,  0.0113],\n",
       "                      [ 0.0217,  0.0052,  0.0398,  ..., -0.0249,  0.0017, -0.0498],\n",
       "                      [-0.0486,  0.0667, -0.0142,  ...,  0.0062, -0.0479, -0.0122],\n",
       "                      ...,\n",
       "                      [ 0.0513,  0.0268, -0.0282,  ..., -0.0098, -0.0249,  0.0138],\n",
       "                      [ 0.0006, -0.0338,  0.0256,  ..., -0.0154, -0.0006,  0.0147],\n",
       "                      [ 0.0104, -0.0231,  0.0254,  ..., -0.0006,  0.0199,  0.0167]])),\n",
       "             ('encoder.layer.23.attention.self.value.bias',\n",
       "              tensor([-0.0151,  0.0174,  0.0117,  ...,  0.0080,  0.0070,  0.0140])),\n",
       "             ('encoder.layer.23.attention.self.w2e_query.weight',\n",
       "              tensor([[-0.0405,  0.0187,  0.0936,  ..., -0.0858,  0.0836,  0.0594],\n",
       "                      [-0.0434,  0.0176,  0.1013,  ..., -0.0061, -0.0153, -0.0034],\n",
       "                      [ 0.0402,  0.0434,  0.0307,  ..., -0.0089, -0.0853, -0.0320],\n",
       "                      ...,\n",
       "                      [ 0.1009, -0.0491,  0.1274,  ...,  0.0009, -0.0081,  0.0644],\n",
       "                      [-0.0510, -0.0008,  0.0014,  ..., -0.0474, -0.0116, -0.0148],\n",
       "                      [-0.0046, -0.0502, -0.0556,  ...,  0.0484, -0.0629,  0.0157]])),\n",
       "             ('encoder.layer.23.attention.self.w2e_query.bias',\n",
       "              tensor([ 0.0772,  0.2725,  0.0311,  ..., -0.2014,  0.1130,  0.0799])),\n",
       "             ('encoder.layer.23.attention.self.e2w_query.weight',\n",
       "              tensor([[-0.0409,  0.0191,  0.0937,  ..., -0.0863,  0.0839,  0.0599],\n",
       "                      [-0.0432,  0.0182,  0.1023,  ..., -0.0069, -0.0157, -0.0030],\n",
       "                      [ 0.0404,  0.0438,  0.0305,  ..., -0.0095, -0.0849, -0.0327],\n",
       "                      ...,\n",
       "                      [ 0.1009, -0.0497,  0.1282,  ...,  0.0009, -0.0081,  0.0643],\n",
       "                      [-0.0516, -0.0006,  0.0004,  ..., -0.0482, -0.0116, -0.0151],\n",
       "                      [-0.0051, -0.0501, -0.0565,  ...,  0.0479, -0.0630,  0.0153]])),\n",
       "             ('encoder.layer.23.attention.self.e2w_query.bias',\n",
       "              tensor([ 0.0774,  0.2727,  0.0311,  ..., -0.2014,  0.1130,  0.0798])),\n",
       "             ('encoder.layer.23.attention.self.e2e_query.weight',\n",
       "              tensor([[-0.0409,  0.0191,  0.0937,  ..., -0.0863,  0.0839,  0.0599],\n",
       "                      [-0.0432,  0.0182,  0.1023,  ..., -0.0069, -0.0157, -0.0030],\n",
       "                      [ 0.0404,  0.0438,  0.0305,  ..., -0.0095, -0.0849, -0.0327],\n",
       "                      ...,\n",
       "                      [ 0.1009, -0.0497,  0.1282,  ...,  0.0009, -0.0081,  0.0643],\n",
       "                      [-0.0516, -0.0006,  0.0004,  ..., -0.0482, -0.0116, -0.0151],\n",
       "                      [-0.0051, -0.0501, -0.0565,  ...,  0.0479, -0.0630,  0.0153]])),\n",
       "             ('encoder.layer.23.attention.self.e2e_query.bias',\n",
       "              tensor([ 0.0774,  0.2727,  0.0311,  ..., -0.2014,  0.1130,  0.0798])),\n",
       "             ('encoder.layer.23.attention.output.dense.weight',\n",
       "              tensor([[-0.0220, -0.0084, -0.0024,  ...,  0.0421, -0.0300,  0.0327],\n",
       "                      [-0.0416, -0.0088,  0.0247,  ...,  0.0493, -0.0093, -0.0458],\n",
       "                      [-0.0201,  0.0063,  0.0087,  ..., -0.0102,  0.0114, -0.0321],\n",
       "                      ...,\n",
       "                      [-0.0005, -0.0511, -0.0129,  ..., -0.0214,  0.0007, -0.0029],\n",
       "                      [ 0.0151,  0.0432,  0.0133,  ...,  0.0107,  0.0044,  0.0040],\n",
       "                      [ 0.0373, -0.0148, -0.0276,  ..., -0.0148,  0.0150,  0.0123]])),\n",
       "             ('encoder.layer.23.attention.output.dense.bias',\n",
       "              tensor([ 0.0627,  0.0713, -0.0060,  ...,  0.0313,  0.0207,  0.1620])),\n",
       "             ('encoder.layer.23.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9604, 0.9790, 0.9917,  ..., 0.9766, 0.9897, 0.9604])),\n",
       "             ('encoder.layer.23.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1370,  0.0294, -0.1637,  ..., -0.0656, -0.1442, -0.0920])),\n",
       "             ('encoder.layer.23.intermediate.dense.weight',\n",
       "              tensor([[-0.0112, -0.0859, -0.0120,  ..., -0.0127,  0.0096, -0.0239],\n",
       "                      [-0.0166, -0.0244, -0.0121,  ..., -0.0124,  0.0184, -0.0579],\n",
       "                      [-0.0006, -0.0931, -0.0034,  ...,  0.0340,  0.0327, -0.0181],\n",
       "                      ...,\n",
       "                      [-0.0374,  0.0201,  0.0123,  ...,  0.0053, -0.0398, -0.0084],\n",
       "                      [ 0.0427,  0.0011, -0.0074,  ..., -0.0186,  0.0629,  0.0438],\n",
       "                      [ 0.0406, -0.0222, -0.0293,  ...,  0.0483,  0.0296,  0.0448]])),\n",
       "             ('encoder.layer.23.intermediate.dense.bias',\n",
       "              tensor([ 0.0101, -0.0674, -0.1279,  ..., -0.0406, -0.0937, -0.0124])),\n",
       "             ('encoder.layer.23.output.dense.weight',\n",
       "              tensor([[ 0.0252, -0.0401, -0.0190,  ...,  0.0146,  0.0268, -0.0052],\n",
       "                      [ 0.0116,  0.0637, -0.0232,  ..., -0.0174, -0.0400,  0.0264],\n",
       "                      [-0.0019, -0.0097,  0.0051,  ...,  0.0105, -0.0260, -0.0152],\n",
       "                      ...,\n",
       "                      [ 0.0148, -0.0326,  0.0784,  ...,  0.0278, -0.0183,  0.0429],\n",
       "                      [ 0.0102, -0.0142,  0.0476,  ...,  0.0056,  0.0263, -0.0089],\n",
       "                      [-0.0397,  0.0108,  0.0006,  ...,  0.0340,  0.0153, -0.0161]])),\n",
       "             ('encoder.layer.23.output.dense.bias',\n",
       "              tensor([-0.0757, -0.0241, -0.0950,  ...,  0.0761, -0.1588,  0.0798])),\n",
       "             ('encoder.layer.23.output.LayerNorm.weight',\n",
       "              tensor([0.9805, 0.9932, 0.9951,  ..., 0.9756, 1.0010, 0.9854])),\n",
       "             ('encoder.layer.23.output.LayerNorm.bias',\n",
       "              tensor([-0.0358, -0.0344, -0.0120,  ..., -0.0469,  0.0007, -0.0184])),\n",
       "             ('pooler.dense.weight',\n",
       "              tensor([[-2.1347e-02, -6.3019e-03, -2.8824e-02,  ...,  9.4299e-03,\n",
       "                       -7.8812e-03,  1.5518e-02],\n",
       "                      [-2.5616e-03, -1.5396e-02,  5.1193e-03,  ..., -4.0070e-02,\n",
       "                       -1.0307e-02, -4.0100e-02],\n",
       "                      [ 7.8888e-03,  1.9897e-02, -1.4257e-03,  ...,  5.3406e-03,\n",
       "                       -1.0704e-02,  1.6003e-03],\n",
       "                      ...,\n",
       "                      [ 1.4782e-05, -7.4730e-03, -1.6571e-02,  ..., -2.6047e-02,\n",
       "                       -7.8011e-04, -6.7635e-03],\n",
       "                      [-8.0185e-03, -1.4694e-02, -2.6031e-02,  ..., -3.0762e-02,\n",
       "                       -1.1337e-02,  2.3163e-02],\n",
       "                      [-6.0120e-03,  2.7924e-02, -4.3488e-02,  ...,  2.1973e-02,\n",
       "                        1.4162e-03, -1.3374e-02]])),\n",
       "             ('pooler.dense.bias', tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('embeddings.word_embeddings.weight',\n",
       "              tensor([[-0.1410, -0.0089,  0.0384,  ...,  0.0511, -0.0069, -0.0374],\n",
       "                      [ 0.0089, -0.0142,  0.0128,  ..., -0.0154,  0.0242,  0.0137],\n",
       "                      [-0.0790,  0.0005, -0.1168,  ...,  0.1091,  0.0659, -0.0386],\n",
       "                      ...,\n",
       "                      [ 0.0396,  0.0010,  0.0478,  ..., -0.0250, -0.0500,  0.0352],\n",
       "                      [ 0.0481,  0.0262,  0.0424,  ..., -0.0371, -0.0062,  0.0085],\n",
       "                      [-0.0130, -0.0106, -0.0229,  ...,  0.0451,  0.0108, -0.0358]])),\n",
       "             ('embeddings.position_embeddings.weight',\n",
       "              tensor([[-0.0038,  0.0252, -0.0092,  ...,  0.0177,  0.0062, -0.0161],\n",
       "                      [ 0.0128, -0.0005, -0.0293,  ...,  0.0063, -0.0179,  0.0244],\n",
       "                      [ 0.0298,  0.0152, -0.0552,  ..., -0.0707, -0.0465,  0.0450],\n",
       "                      ...,\n",
       "                      [-0.0208, -0.0063,  0.0473,  ..., -0.0387,  0.0448,  0.0538],\n",
       "                      [-0.0282,  0.1193,  0.0454,  ...,  0.0206, -0.1188,  0.0498],\n",
       "                      [ 0.0958, -0.0753,  0.0517,  ..., -0.1151, -0.1053,  0.0489]])),\n",
       "             ('embeddings.token_type_embeddings.weight',\n",
       "              tensor([[-0.0009,  0.0003,  0.0006,  ..., -0.0002, -0.0004, -0.0011]])),\n",
       "             ('embeddings.LayerNorm.weight',\n",
       "              tensor([0.9316, 0.9233, 0.9126,  ..., 0.9395, 0.9136, 0.9019])),\n",
       "             ('embeddings.LayerNorm.bias',\n",
       "              tensor([ 0.0300,  0.0422,  0.1936,  ..., -0.2252, -0.0895,  0.1246])),\n",
       "             ('entity_embeddings.entity_embeddings.weight',\n",
       "              tensor([[-0.0420, -0.0610, -0.0482,  ..., -0.0336, -0.0533, -0.0462],\n",
       "                      [ 0.0699,  0.0621,  0.0854,  ...,  0.0551,  0.0466,  0.0777],\n",
       "                      [ 0.0074, -0.0067,  0.0015,  ..., -0.0110, -0.0052, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0780, -0.0544, -0.0635,  ...,  0.0061, -0.0908, -0.0801],\n",
       "                      [-0.0913, -0.0692, -0.0123,  ..., -0.1384, -0.0340, -0.0620],\n",
       "                      [ 0.0124, -0.0218, -0.0023,  ..., -0.1331, -0.0721, -0.0953]])),\n",
       "             ('entity_embeddings.entity_embedding_dense.weight',\n",
       "              tensor([[-0.1981,  0.0007, -0.0202,  ...,  0.0644, -0.0417, -0.0397],\n",
       "                      [-0.0745,  0.0542, -0.0619,  ...,  0.0468,  0.2284,  0.1207],\n",
       "                      [-0.1025,  0.0271,  0.0388,  ...,  0.1439,  0.0417, -0.0260],\n",
       "                      ...,\n",
       "                      [ 0.0622,  0.0238,  0.0708,  ..., -0.0285, -0.0433, -0.0186],\n",
       "                      [-0.1016,  0.0193,  0.0932,  ..., -0.1866, -0.1316,  0.0490],\n",
       "                      [-0.0086, -0.0434,  0.0285,  ..., -0.0317,  0.0038,  0.0186]])),\n",
       "             ('entity_embeddings.position_embeddings.weight',\n",
       "              tensor([[-0.0203, -0.0177, -0.0124,  ..., -0.0187,  0.0235,  0.0100],\n",
       "                      [-0.0510,  0.0136, -0.0595,  ..., -0.2035, -0.0274, -0.0600],\n",
       "                      [-0.0271, -0.1049, -0.0072,  ..., -0.1039,  0.0197, -0.0698],\n",
       "                      ...,\n",
       "                      [ 0.0192,  0.0089,  0.0098,  ..., -0.0163, -0.0025, -0.0114],\n",
       "                      [-0.0093,  0.0056,  0.0043,  ..., -0.0050, -0.0219,  0.0092],\n",
       "                      [ 0.0004,  0.0029, -0.0044,  ..., -0.0157, -0.0013, -0.0172]])),\n",
       "             ('entity_embeddings.token_type_embeddings.weight',\n",
       "              tensor([[ 0.0201,  0.0640,  0.0471,  ..., -0.0645, -0.0461,  0.0504]])),\n",
       "             ('entity_embeddings.LayerNorm.weight',\n",
       "              tensor([1.0410, 1.0244, 1.0098,  ..., 1.0059, 1.0859, 0.9888])),\n",
       "             ('entity_embeddings.LayerNorm.bias',\n",
       "              tensor([ 0.3530, -0.1464,  0.0742,  ...,  0.5566,  0.0612, -0.2042])),\n",
       "             ('qa_outputs.weight',\n",
       "              tensor([[-0.0028, -0.0093,  0.0183,  ...,  0.0241, -0.0452, -0.0288],\n",
       "                      [-0.0175,  0.0186,  0.0271,  ..., -0.0182,  0.0148,  0.0008]])),\n",
       "             ('qa_outputs.bias', tensor([0.0002, 0.0005]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pytorch_qa\n",
    "# torch_model = torch.load(\"./pytorch_model.bin\",map_location='cpu')# luke github官方上面提供的squad微调后权重\n",
    "# torch_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layer.0.attention.self.query.weight',\n",
       "              tensor([[-0.0029,  0.0352,  0.0007,  ...,  0.0023,  0.0595, -0.0426],\n",
       "                      [-0.0248,  0.0529, -0.0145,  ..., -0.0303, -0.0143,  0.0116],\n",
       "                      [ 0.0061,  0.0708, -0.0336,  ...,  0.0807,  0.0115, -0.0131],\n",
       "                      ...,\n",
       "                      [-0.0589,  0.0206, -0.0426,  ..., -0.0298,  0.0041,  0.0700],\n",
       "                      [ 0.0421,  0.0225, -0.0608,  ..., -0.0552, -0.0157,  0.0173],\n",
       "                      [-0.0184, -0.0457, -0.0103,  ...,  0.0474,  0.0225, -0.0182]])),\n",
       "             ('encoder.layer.0.attention.self.query.bias',\n",
       "              tensor([ 0.3121,  0.0556, -0.0751,  ..., -0.0704, -0.0500, -0.0664])),\n",
       "             ('encoder.layer.0.attention.self.key.weight',\n",
       "              tensor([[-0.0043, -0.0184, -0.0136,  ..., -0.0037,  0.0096, -0.0156],\n",
       "                      [-0.0238, -0.0002,  0.0253,  ...,  0.0403,  0.0436, -0.0195],\n",
       "                      [-0.0264, -0.0522, -0.0125,  ..., -0.0359,  0.0077,  0.0150],\n",
       "                      ...,\n",
       "                      [-0.0718, -0.0261, -0.0203,  ..., -0.0186,  0.0097,  0.1023],\n",
       "                      [ 0.0157,  0.0065, -0.0171,  ..., -0.0038, -0.0090,  0.0435],\n",
       "                      [-0.0091, -0.0628,  0.0415,  ...,  0.0466,  0.0149, -0.0468]])),\n",
       "             ('encoder.layer.0.attention.self.key.bias',\n",
       "              tensor([-0.0041, -0.0033, -0.0012,  ...,  0.0013,  0.0017,  0.0018])),\n",
       "             ('encoder.layer.0.attention.self.value.weight',\n",
       "              tensor([[ 0.0306, -0.0006, -0.0241,  ..., -0.0187,  0.0017,  0.0217],\n",
       "                      [ 0.0565,  0.0432,  0.0015,  ..., -0.0156,  0.0920, -0.0204],\n",
       "                      [-0.0151, -0.0429,  0.0127,  ..., -0.0512,  0.0012,  0.0675],\n",
       "                      ...,\n",
       "                      [-0.0101,  0.0082, -0.0115,  ...,  0.0363,  0.0256,  0.0110],\n",
       "                      [-0.0032, -0.0139, -0.0513,  ...,  0.0377, -0.0338,  0.0291],\n",
       "                      [ 0.0047, -0.0094, -0.0135,  ..., -0.0247,  0.0876, -0.0179]])),\n",
       "             ('encoder.layer.0.attention.self.value.bias',\n",
       "              tensor([-0.0014,  0.0024, -0.0083,  ..., -0.0226, -0.0209, -0.0350])),\n",
       "             ('encoder.layer.0.attention.output.dense.weight',\n",
       "              tensor([[ 0.0015,  0.0398, -0.0170,  ..., -0.0140, -0.0325, -0.0164],\n",
       "                      [-0.0346,  0.0133, -0.0157,  ...,  0.0311, -0.0096,  0.0268],\n",
       "                      [ 0.0271, -0.0743,  0.0193,  ..., -0.0323, -0.0068,  0.0988],\n",
       "                      ...,\n",
       "                      [ 0.0285,  0.0010,  0.0225,  ...,  0.0128, -0.0104, -0.0361],\n",
       "                      [-0.0081,  0.0514, -0.0476,  ...,  0.0426,  0.0263, -0.0157],\n",
       "                      [ 0.0468,  0.0232,  0.0929,  ...,  0.0274,  0.0088,  0.0074]])),\n",
       "             ('encoder.layer.0.attention.output.dense.bias',\n",
       "              tensor([-0.0135,  0.0296,  0.0857,  ...,  0.0735, -0.0072,  0.0103])),\n",
       "             ('encoder.layer.0.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9795, 0.9904, 0.9729,  ..., 0.9834, 0.9905, 0.9973])),\n",
       "             ('encoder.layer.0.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.4307,  0.2765, -0.0064,  ...,  0.0114,  0.3294, -0.2977])),\n",
       "             ('encoder.layer.0.intermediate.dense.weight',\n",
       "              tensor([[ 0.0584, -0.0646, -0.0934,  ...,  0.0051,  0.0196, -0.0153],\n",
       "                      [ 0.0162, -0.0273,  0.0207,  ...,  0.0155, -0.0379,  0.1215],\n",
       "                      [ 0.0367, -0.0660, -0.0006,  ...,  0.0293, -0.0267, -0.0219],\n",
       "                      ...,\n",
       "                      [ 0.0165, -0.0888,  0.0040,  ...,  0.0331, -0.0512, -0.0009],\n",
       "                      [ 0.1342,  0.0519, -0.1299,  ..., -0.1486, -0.0311,  0.0218],\n",
       "                      [ 0.0719, -0.0296, -0.0604,  ...,  0.0095, -0.0576, -0.0292]])),\n",
       "             ('encoder.layer.0.intermediate.dense.bias',\n",
       "              tensor([-0.0940, -0.0767, -0.0839,  ..., -0.1082, -0.0689, -0.0947])),\n",
       "             ('encoder.layer.0.output.dense.weight',\n",
       "              tensor([[ 4.3990e-02,  1.0164e-01,  3.0074e-02,  ..., -5.5654e-02,\n",
       "                        5.9967e-02,  6.6240e-02],\n",
       "                      [ 1.8042e-02,  1.1118e-02, -9.3006e-03,  ..., -6.2040e-03,\n",
       "                        1.7984e-02, -1.1116e-02],\n",
       "                      [ 2.6197e-02, -7.4327e-02,  5.3103e-02,  ..., -4.2745e-02,\n",
       "                       -1.0942e-02,  8.8373e-05],\n",
       "                      ...,\n",
       "                      [-1.4168e-02, -2.8210e-03,  5.3669e-02,  ...,  9.4674e-03,\n",
       "                       -5.7152e-02,  1.5481e-02],\n",
       "                      [-4.9793e-02,  3.3638e-02, -6.9882e-02,  ...,  5.9173e-02,\n",
       "                       -3.5718e-02, -5.2920e-03],\n",
       "                      [-9.5240e-02, -4.4761e-02, -9.4406e-02,  ..., -5.3682e-02,\n",
       "                        5.1750e-02, -2.9576e-02]])),\n",
       "             ('encoder.layer.0.output.dense.bias',\n",
       "              tensor([ 0.0637, -0.0400,  0.0412,  ...,  0.0072, -0.0959,  0.0556])),\n",
       "             ('encoder.layer.0.output.LayerNorm.weight',\n",
       "              tensor([0.9692, 0.9611, 0.9668,  ..., 0.9724, 0.9691, 0.9486])),\n",
       "             ('encoder.layer.0.output.LayerNorm.bias',\n",
       "              tensor([ 0.3970, -0.1882,  0.0432,  ..., -0.0482, -0.2744,  0.2004])),\n",
       "             ('encoder.layer.1.attention.self.query.weight',\n",
       "              tensor([[ 0.0249, -0.1109,  0.0186,  ..., -0.0663, -0.0303, -0.0336],\n",
       "                      [ 0.0290, -0.0107,  0.0973,  ..., -0.0229,  0.0269,  0.1658],\n",
       "                      [ 0.0102, -0.0411,  0.0320,  ...,  0.0080, -0.0011, -0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0170,  0.0311,  0.0660,  ...,  0.0441, -0.0502,  0.1032],\n",
       "                      [-0.1139,  0.0699,  0.0681,  ..., -0.0135, -0.0743,  0.0625],\n",
       "                      [ 0.0288, -0.0682,  0.0241,  ...,  0.0622, -0.0020,  0.0033]])),\n",
       "             ('encoder.layer.1.attention.self.query.bias',\n",
       "              tensor([ 0.0735,  0.0507, -0.0682,  ...,  0.0832,  0.0456, -0.0775])),\n",
       "             ('encoder.layer.1.attention.self.key.weight',\n",
       "              tensor([[-0.0086,  0.0603, -0.0056,  ..., -0.0312,  0.0108, -0.0152],\n",
       "                      [-0.0210, -0.0885, -0.0570,  ...,  0.0548, -0.0376,  0.0184],\n",
       "                      [-0.0135,  0.0391,  0.0357,  ...,  0.0053,  0.0004, -0.0116],\n",
       "                      ...,\n",
       "                      [-0.0631, -0.0680,  0.0807,  ...,  0.0215,  0.0240,  0.0022],\n",
       "                      [ 0.0493, -0.0463,  0.0360,  ...,  0.0469,  0.0339, -0.0322],\n",
       "                      [ 0.0600,  0.0397,  0.0555,  ...,  0.0909,  0.0234, -0.0011]])),\n",
       "             ('encoder.layer.1.attention.self.key.bias',\n",
       "              tensor([ 1.3938e-03, -8.1081e-04, -8.1789e-04,  ...,  2.2814e-04,\n",
       "                      -7.8769e-05,  1.0090e-03])),\n",
       "             ('encoder.layer.1.attention.self.value.weight',\n",
       "              tensor([[-0.0645,  0.0059, -0.0254,  ..., -0.0037,  0.0521,  0.0278],\n",
       "                      [-0.0177,  0.0365, -0.0355,  ..., -0.0129,  0.0319, -0.0209],\n",
       "                      [ 0.0035,  0.0358,  0.0030,  ...,  0.0257,  0.0855, -0.0174],\n",
       "                      ...,\n",
       "                      [-0.0050, -0.0834, -0.0305,  ..., -0.0218,  0.0368,  0.0249],\n",
       "                      [-0.0625,  0.0668, -0.0159,  ..., -0.0053,  0.0224, -0.0132],\n",
       "                      [ 0.0363,  0.0354,  0.0155,  ...,  0.0105,  0.0325, -0.0157]])),\n",
       "             ('encoder.layer.1.attention.self.value.bias',\n",
       "              tensor([ 0.0092,  0.0021,  0.0057,  ...,  0.0010, -0.0558, -0.0005])),\n",
       "             ('encoder.layer.1.attention.output.dense.weight',\n",
       "              tensor([[-0.0359, -0.0120,  0.0086,  ..., -0.0711, -0.0290,  0.0189],\n",
       "                      [ 0.0017, -0.0536, -0.0034,  ..., -0.0751, -0.0399,  0.0076],\n",
       "                      [ 0.0419,  0.0129, -0.0245,  ..., -0.0330,  0.0179,  0.0328],\n",
       "                      ...,\n",
       "                      [-0.0061, -0.0164,  0.0348,  ...,  0.0245, -0.0246,  0.0448],\n",
       "                      [-0.0221,  0.0093,  0.0080,  ..., -0.0486, -0.0387, -0.0516],\n",
       "                      [ 0.0068, -0.0234,  0.0426,  ...,  0.0062, -0.0460,  0.0256]])),\n",
       "             ('encoder.layer.1.attention.output.dense.bias',\n",
       "              tensor([-0.2165,  0.0374, -0.0812,  ..., -0.0743,  0.2336,  0.0740])),\n",
       "             ('encoder.layer.1.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9844, 1.0002, 0.9572,  ..., 0.9796, 0.9894, 0.9716])),\n",
       "             ('encoder.layer.1.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.3287,  0.1820, -0.0330,  ..., -0.0722,  0.2502, -0.2283])),\n",
       "             ('encoder.layer.1.intermediate.dense.weight',\n",
       "              tensor([[ 0.0297, -0.0344,  0.0006,  ..., -0.0230,  0.0981,  0.0496],\n",
       "                      [-0.0053, -0.0674, -0.0317,  ...,  0.0352,  0.0014, -0.0523],\n",
       "                      [-0.0048,  0.0833,  0.0749,  ...,  0.0150,  0.0193, -0.0084],\n",
       "                      ...,\n",
       "                      [-0.0357, -0.0449,  0.1232,  ...,  0.0002,  0.0134, -0.0324],\n",
       "                      [ 0.0738, -0.0580, -0.1040,  ..., -0.0315,  0.0241, -0.0826],\n",
       "                      [ 0.0016,  0.0747,  0.0952,  ..., -0.0185, -0.0258,  0.0306]])),\n",
       "             ('encoder.layer.1.intermediate.dense.bias',\n",
       "              tensor([ 0.0946, -0.0516, -0.0546,  ..., -0.0861, -0.0854, -0.0850])),\n",
       "             ('encoder.layer.1.output.dense.weight',\n",
       "              tensor([[-0.0229, -0.0174,  0.0621,  ..., -0.0208, -0.0902,  0.0511],\n",
       "                      [ 0.0462, -0.0165, -0.0148,  ..., -0.0320, -0.0057, -0.0449],\n",
       "                      [ 0.0411,  0.0353,  0.0654,  ..., -0.0241, -0.0519,  0.0958],\n",
       "                      ...,\n",
       "                      [ 0.0427, -0.0576, -0.0733,  ..., -0.0398,  0.0234,  0.0343],\n",
       "                      [ 0.0103,  0.0833,  0.1085,  ...,  0.0641, -0.0420, -0.0219],\n",
       "                      [-0.0611,  0.0714,  0.0043,  ..., -0.0341,  0.0689,  0.0395]])),\n",
       "             ('encoder.layer.1.output.dense.bias',\n",
       "              tensor([ 0.0109,  0.0058,  0.0673,  ...,  0.0114, -0.0479,  0.0151])),\n",
       "             ('encoder.layer.1.output.LayerNorm.weight',\n",
       "              tensor([0.9609, 0.9935, 0.9404,  ..., 0.9701, 0.9679, 0.9373])),\n",
       "             ('encoder.layer.1.output.LayerNorm.bias',\n",
       "              tensor([ 0.1899, -0.1928, -0.0494,  ..., -0.0139, -0.2726,  0.1591])),\n",
       "             ('encoder.layer.2.attention.self.query.weight',\n",
       "              tensor([[ 1.7903e-04, -4.8080e-02,  4.3229e-02,  ..., -2.8078e-02,\n",
       "                       -6.8488e-02, -1.4657e-02],\n",
       "                      [-8.2460e-02, -9.0673e-02,  3.5695e-02,  ..., -2.3552e-02,\n",
       "                        3.7966e-03,  1.3901e-02],\n",
       "                      [-2.0614e-02, -7.9853e-03, -3.8865e-02,  ..., -3.4760e-02,\n",
       "                       -1.4943e-02, -2.1304e-02],\n",
       "                      ...,\n",
       "                      [ 2.2029e-02, -1.6189e-02, -1.1728e-02,  ..., -3.5378e-02,\n",
       "                        8.1019e-02, -6.9490e-02],\n",
       "                      [ 3.1909e-02, -9.4880e-02, -4.9948e-02,  ..., -4.2807e-02,\n",
       "                       -2.9465e-05,  1.2165e-02],\n",
       "                      [ 2.1400e-02, -2.3819e-02, -6.8722e-02,  ...,  7.4963e-02,\n",
       "                       -5.7642e-03,  4.5927e-02]])),\n",
       "             ('encoder.layer.2.attention.self.query.bias',\n",
       "              tensor([-0.0603,  0.1270,  0.1130,  ..., -0.1525,  0.0652, -0.1646])),\n",
       "             ('encoder.layer.2.attention.self.key.weight',\n",
       "              tensor([[ 0.0131,  0.1376,  0.0404,  ...,  0.0351,  0.0181, -0.0553],\n",
       "                      [-0.0002,  0.0150, -0.0176,  ..., -0.0312, -0.0374,  0.0427],\n",
       "                      [ 0.0667, -0.0101,  0.0049,  ..., -0.0530,  0.0257, -0.0443],\n",
       "                      ...,\n",
       "                      [-0.0023, -0.0353, -0.0334,  ..., -0.0015, -0.0527, -0.0661],\n",
       "                      [-0.0570, -0.0349,  0.0333,  ...,  0.0010, -0.0509,  0.0192],\n",
       "                      [ 0.0339,  0.0132, -0.0249,  ...,  0.0653,  0.0100,  0.0513]])),\n",
       "             ('encoder.layer.2.attention.self.key.bias',\n",
       "              tensor([-9.1675e-04, -2.7083e-03, -1.1208e-03,  ..., -7.6865e-05,\n",
       "                      -6.1910e-04, -9.5030e-04])),\n",
       "             ('encoder.layer.2.attention.self.value.weight',\n",
       "              tensor([[-3.6165e-03,  4.9413e-02,  2.3386e-02,  ...,  2.5121e-02,\n",
       "                        2.8032e-02, -2.3105e-03],\n",
       "                      [-5.3756e-05,  8.9603e-02, -2.0890e-02,  ..., -3.9435e-03,\n",
       "                       -3.1066e-02, -6.8478e-03],\n",
       "                      [ 2.0130e-02,  4.6793e-02,  3.5938e-03,  ...,  3.4825e-02,\n",
       "                        2.8720e-02, -2.3115e-02],\n",
       "                      ...,\n",
       "                      [-1.1500e-02,  2.6677e-02, -1.0887e-02,  ..., -3.8887e-02,\n",
       "                       -2.5036e-02,  4.0987e-02],\n",
       "                      [ 6.1762e-02,  3.9349e-02, -4.0395e-02,  ..., -2.3117e-02,\n",
       "                        6.6294e-02,  2.8771e-02],\n",
       "                      [-2.4074e-02, -9.9498e-03,  1.8958e-03,  ..., -1.1106e-02,\n",
       "                        1.5080e-02,  5.3839e-02]])),\n",
       "             ('encoder.layer.2.attention.self.value.bias',\n",
       "              tensor([-0.0298,  0.0152,  0.0091,  ..., -0.0032,  0.0105, -0.0050])),\n",
       "             ('encoder.layer.2.attention.output.dense.weight',\n",
       "              tensor([[-0.0131, -0.0190,  0.0225,  ..., -0.0045, -0.0107,  0.0217],\n",
       "                      [ 0.0056,  0.0167, -0.0151,  ..., -0.0479, -0.0117, -0.0024],\n",
       "                      [ 0.0060,  0.0404, -0.0695,  ..., -0.0137, -0.0132, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0310, -0.0071,  0.0504,  ..., -0.0042,  0.0107, -0.0110],\n",
       "                      [-0.0792,  0.0027,  0.0046,  ...,  0.0438,  0.0209, -0.0088],\n",
       "                      [ 0.0565,  0.0488,  0.0014,  ...,  0.0039,  0.0564,  0.0269]])),\n",
       "             ('encoder.layer.2.attention.output.dense.bias',\n",
       "              tensor([ 0.0616, -0.0575,  0.0326,  ...,  0.0814, -0.0387,  0.0749])),\n",
       "             ('encoder.layer.2.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9844, 0.9845, 0.9841,  ..., 0.9639, 0.9560, 0.9723])),\n",
       "             ('encoder.layer.2.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0935, -0.0033, -0.3413,  ..., -0.0274, -0.0714,  0.2172])),\n",
       "             ('encoder.layer.2.intermediate.dense.weight',\n",
       "              tensor([[ 2.1878e-02,  8.6251e-02,  6.9060e-02,  ..., -4.6270e-02,\n",
       "                       -1.3691e-02, -4.2707e-02],\n",
       "                      [-2.5301e-02, -6.8095e-02,  9.3382e-02,  ..., -7.4692e-02,\n",
       "                        5.3183e-02, -2.6697e-03],\n",
       "                      [-7.1913e-02,  4.3713e-03,  9.1703e-02,  ...,  1.4489e-02,\n",
       "                        2.1770e-02,  2.5752e-03],\n",
       "                      ...,\n",
       "                      [-3.8349e-02, -1.5429e-02,  5.6410e-03,  ...,  9.2020e-03,\n",
       "                       -1.2588e-03,  1.8834e-02],\n",
       "                      [-6.4372e-02,  1.1813e-02, -1.5158e-02,  ..., -1.8016e-03,\n",
       "                        3.4144e-02, -9.1456e-02],\n",
       "                      [ 4.0260e-07,  1.9655e-02,  1.6495e-02,  ..., -1.4749e-02,\n",
       "                       -5.0643e-02, -1.0096e-01]])),\n",
       "             ('encoder.layer.2.intermediate.dense.bias',\n",
       "              tensor([-0.0232, -0.0806, -0.0698,  ...,  0.0589, -0.0852, -0.0899])),\n",
       "             ('encoder.layer.2.output.dense.weight',\n",
       "              tensor([[-0.0220,  0.0848,  0.0177,  ..., -0.0383, -0.1062,  0.0024],\n",
       "                      [-0.0033,  0.0188, -0.0610,  ...,  0.0479, -0.0284, -0.0111],\n",
       "                      [ 0.0519, -0.0578,  0.0578,  ...,  0.0101, -0.0493, -0.0756],\n",
       "                      ...,\n",
       "                      [ 0.0027, -0.0465,  0.0661,  ...,  0.0128,  0.1045, -0.0009],\n",
       "                      [ 0.0482, -0.0084, -0.0305,  ...,  0.0113, -0.0944, -0.0101],\n",
       "                      [ 0.0298, -0.0504, -0.0233,  ..., -0.0408,  0.0131,  0.0020]])),\n",
       "             ('encoder.layer.2.output.dense.bias',\n",
       "              tensor([-0.0083, -0.0427,  0.0114,  ...,  0.0307,  0.0147,  0.0455])),\n",
       "             ('encoder.layer.2.output.LayerNorm.weight',\n",
       "              tensor([0.9660, 0.9679, 0.9523,  ..., 0.9763, 0.9777, 0.9589])),\n",
       "             ('encoder.layer.2.output.LayerNorm.bias',\n",
       "              tensor([-0.0323, -0.1087,  0.2080,  ..., -0.0499, -0.0273, -0.2025])),\n",
       "             ('encoder.layer.3.attention.self.query.weight',\n",
       "              tensor([[-0.0698,  0.0108,  0.0364,  ...,  0.0258,  0.0512, -0.0128],\n",
       "                      [-0.0190, -0.0791, -0.0413,  ...,  0.0368, -0.0142,  0.0316],\n",
       "                      [-0.0200, -0.0165,  0.0804,  ...,  0.0338, -0.0177,  0.0057],\n",
       "                      ...,\n",
       "                      [-0.0572, -0.0109, -0.0102,  ...,  0.0394,  0.0896, -0.0158],\n",
       "                      [ 0.0665, -0.0329,  0.0415,  ..., -0.0895,  0.0229, -0.0347],\n",
       "                      [ 0.0324,  0.0638, -0.0223,  ..., -0.0438, -0.0323,  0.0215]])),\n",
       "             ('encoder.layer.3.attention.self.query.bias',\n",
       "              tensor([-0.0254, -0.0520,  0.0315,  ...,  0.0128,  0.1172, -0.0813])),\n",
       "             ('encoder.layer.3.attention.self.key.weight',\n",
       "              tensor([[-0.1347, -0.0404, -0.0011,  ...,  0.0223,  0.0508, -0.0272],\n",
       "                      [ 0.0270,  0.0988,  0.0912,  ..., -0.0236,  0.0828, -0.0444],\n",
       "                      [ 0.0049, -0.0411,  0.0911,  ..., -0.0205,  0.0172, -0.0214],\n",
       "                      ...,\n",
       "                      [-0.0251, -0.0043, -0.0060,  ...,  0.0715, -0.0627, -0.0396],\n",
       "                      [ 0.0041, -0.0234, -0.0043,  ...,  0.0386, -0.0317,  0.0059],\n",
       "                      [ 0.0301,  0.0116,  0.0230,  ...,  0.0173, -0.0283, -0.0181]])),\n",
       "             ('encoder.layer.3.attention.self.key.bias',\n",
       "              tensor([-5.8962e-05,  1.0951e-03,  1.8938e-04,  ...,  1.3679e-03,\n",
       "                      -7.5666e-04,  4.3345e-04])),\n",
       "             ('encoder.layer.3.attention.self.value.weight',\n",
       "              tensor([[ 0.0103, -0.0299, -0.0731,  ...,  0.0308,  0.0959,  0.0228],\n",
       "                      [-0.0163,  0.0390,  0.0467,  ..., -0.0298,  0.0094, -0.0062],\n",
       "                      [ 0.0402,  0.0391, -0.0705,  ..., -0.0501,  0.0267,  0.0201],\n",
       "                      ...,\n",
       "                      [ 0.0702,  0.0024,  0.0117,  ...,  0.0203,  0.0322,  0.0439],\n",
       "                      [-0.0237,  0.0498, -0.0337,  ..., -0.0040, -0.1057, -0.0118],\n",
       "                      [ 0.0274, -0.0094, -0.0408,  ..., -0.0296, -0.0014, -0.0490]])),\n",
       "             ('encoder.layer.3.attention.self.value.bias',\n",
       "              tensor([-0.0188, -0.0087, -0.0084,  ...,  0.0064,  0.0055, -0.0011])),\n",
       "             ('encoder.layer.3.attention.output.dense.weight',\n",
       "              tensor([[-0.0013,  0.0025,  0.0341,  ...,  0.0013, -0.0271, -0.0287],\n",
       "                      [ 0.0111, -0.0078,  0.0414,  ...,  0.0163, -0.0205,  0.0137],\n",
       "                      [-0.0812,  0.0115, -0.0049,  ..., -0.0125,  0.0565, -0.0054],\n",
       "                      ...,\n",
       "                      [ 0.0230, -0.0006, -0.0211,  ...,  0.0214, -0.0291,  0.0161],\n",
       "                      [ 0.0114,  0.0440, -0.0002,  ..., -0.0078, -0.0101, -0.0086],\n",
       "                      [-0.0282, -0.0150,  0.0180,  ..., -0.0082,  0.0174,  0.0351]])),\n",
       "             ('encoder.layer.3.attention.output.dense.bias',\n",
       "              tensor([ 0.0411, -0.0173, -0.0278,  ..., -0.0227,  0.0338, -0.0198])),\n",
       "             ('encoder.layer.3.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9901, 0.9801, 0.9772,  ..., 0.9887, 0.9780, 0.9649])),\n",
       "             ('encoder.layer.3.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1799, -0.1176, -0.3025,  ..., -0.0450,  0.1124,  0.0361])),\n",
       "             ('encoder.layer.3.intermediate.dense.weight',\n",
       "              tensor([[ 0.0105,  0.0398, -0.0333,  ..., -0.0498, -0.0243, -0.0333],\n",
       "                      [ 0.0365,  0.0329,  0.0173,  ...,  0.0269, -0.0450, -0.0168],\n",
       "                      [ 0.0344, -0.0641, -0.0148,  ...,  0.0646, -0.1824,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0297, -0.0329, -0.0368,  ...,  0.0331, -0.0257, -0.0539],\n",
       "                      [ 0.0303, -0.0566, -0.0366,  ..., -0.0047,  0.0176, -0.0275],\n",
       "                      [ 0.0011,  0.0041, -0.0479,  ...,  0.0114, -0.0650, -0.0441]])),\n",
       "             ('encoder.layer.3.intermediate.dense.bias',\n",
       "              tensor([-0.0908, -0.1110, -0.0572,  ..., -0.1017, -0.0181, -0.0867])),\n",
       "             ('encoder.layer.3.output.dense.weight',\n",
       "              tensor([[-0.0189, -0.0289, -0.0228,  ...,  0.0221, -0.0686, -0.0087],\n",
       "                      [ 0.0233, -0.0099, -0.0246,  ..., -0.0060, -0.0480, -0.0695],\n",
       "                      [-0.0768, -0.0783,  0.0215,  ...,  0.0546, -0.0288, -0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0074,  0.0278,  0.0750,  ..., -0.0539, -0.0728,  0.0015],\n",
       "                      [-0.0946, -0.0783, -0.0584,  ..., -0.0204, -0.0069, -0.1095],\n",
       "                      [ 0.1377,  0.0522, -0.0616,  ...,  0.0494, -0.0426,  0.0299]])),\n",
       "             ('encoder.layer.3.output.dense.bias',\n",
       "              tensor([-0.0873,  0.0333,  0.0643,  ...,  0.0418, -0.0818,  0.2699])),\n",
       "             ('encoder.layer.3.output.LayerNorm.weight',\n",
       "              tensor([0.9747, 0.9886, 0.9677,  ..., 0.9718, 0.9657, 0.9479])),\n",
       "             ('encoder.layer.3.output.LayerNorm.bias',\n",
       "              tensor([ 0.0134, -0.0071,  0.1965,  ..., -0.0247, -0.1181, -0.0818])),\n",
       "             ('encoder.layer.4.attention.self.query.weight',\n",
       "              tensor([[-0.0044, -0.0114, -0.0578,  ..., -0.0106, -0.0350, -0.0083],\n",
       "                      [-0.0038,  0.0028,  0.0457,  ..., -0.0596, -0.0143,  0.0689],\n",
       "                      [-0.0832,  0.0047,  0.0123,  ..., -0.0640, -0.0127,  0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0156,  0.0047, -0.0128,  ...,  0.0218,  0.0032,  0.0020],\n",
       "                      [ 0.0183, -0.0275,  0.0391,  ..., -0.0099, -0.0260,  0.0171],\n",
       "                      [ 0.0347, -0.1023, -0.0174,  ..., -0.0393, -0.0354, -0.1445]])),\n",
       "             ('encoder.layer.4.attention.self.query.bias',\n",
       "              tensor([ 0.2109,  0.0042,  0.2277,  ..., -0.2539, -0.1896,  0.2005])),\n",
       "             ('encoder.layer.4.attention.self.key.weight',\n",
       "              tensor([[-0.0178, -0.0759, -0.0478,  ..., -0.0066, -0.0431,  0.0082],\n",
       "                      [-0.0546, -0.0123,  0.0208,  ...,  0.0055, -0.0208,  0.0098],\n",
       "                      [ 0.0132,  0.0283,  0.0391,  ...,  0.0340,  0.0643, -0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0369,  0.0788,  0.0371,  ...,  0.0539,  0.0096,  0.0890],\n",
       "                      [-0.0157, -0.0246, -0.0284,  ..., -0.0196, -0.0419,  0.0647],\n",
       "                      [ 0.0038,  0.0276, -0.0061,  ...,  0.0227, -0.0330,  0.0216]])),\n",
       "             ('encoder.layer.4.attention.self.key.bias',\n",
       "              tensor([ 0.0006, -0.0003,  0.0003,  ..., -0.0005, -0.0002, -0.0002])),\n",
       "             ('encoder.layer.4.attention.self.value.weight',\n",
       "              tensor([[-0.0095, -0.0010,  0.0109,  ...,  0.0189,  0.0035,  0.0482],\n",
       "                      [ 0.0212,  0.0588, -0.0221,  ...,  0.0631, -0.0173, -0.0744],\n",
       "                      [ 0.0368,  0.0137,  0.0230,  ...,  0.0501, -0.0364,  0.0638],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0655,  0.0223,  ...,  0.0452, -0.0029,  0.0250],\n",
       "                      [ 0.0252,  0.0060,  0.0164,  ...,  0.0274, -0.0029,  0.0623],\n",
       "                      [ 0.0096, -0.1020, -0.0007,  ..., -0.0420,  0.0346,  0.0089]])),\n",
       "             ('encoder.layer.4.attention.self.value.bias',\n",
       "              tensor([-0.0100, -0.0036,  0.0082,  ..., -0.0073,  0.0024, -0.0014])),\n",
       "             ('encoder.layer.4.attention.output.dense.weight',\n",
       "              tensor([[-0.0348, -0.0718, -0.0194,  ..., -0.0074, -0.0107,  0.0657],\n",
       "                      [-0.0434, -0.0146, -0.0087,  ...,  0.0479,  0.0303,  0.0070],\n",
       "                      [ 0.0075,  0.0086, -0.0098,  ..., -0.0414,  0.0454, -0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0056,  0.0072,  0.0049,  ...,  0.0099,  0.0034, -0.0151],\n",
       "                      [-0.0007,  0.0143, -0.0288,  ...,  0.0527,  0.0054, -0.0251],\n",
       "                      [ 0.0171,  0.0429, -0.0141,  ..., -0.0100, -0.0244,  0.0058]])),\n",
       "             ('encoder.layer.4.attention.output.dense.bias',\n",
       "              tensor([-0.0069, -0.0124, -0.0089,  ...,  0.0169, -0.0645, -0.0034])),\n",
       "             ('encoder.layer.4.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9908, 0.9949, 0.9872,  ..., 0.9863, 0.9967, 0.9586])),\n",
       "             ('encoder.layer.4.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1673,  0.0787, -0.2976,  ...,  0.0612,  0.0170,  0.2061])),\n",
       "             ('encoder.layer.4.intermediate.dense.weight',\n",
       "              tensor([[ 0.0025,  0.0551, -0.0292,  ..., -0.0751,  0.0536,  0.0421],\n",
       "                      [-0.0134, -0.0158, -0.0072,  ..., -0.0162, -0.0179, -0.0862],\n",
       "                      [-0.0319,  0.0117,  0.0499,  ..., -0.0912,  0.0204, -0.0461],\n",
       "                      ...,\n",
       "                      [-0.0576, -0.0228,  0.0652,  ...,  0.1229,  0.0427, -0.0719],\n",
       "                      [-0.0047, -0.0802, -0.0871,  ..., -0.0535, -0.0262,  0.0606],\n",
       "                      [-0.0366, -0.0248,  0.0056,  ...,  0.0241,  0.0362, -0.0435]])),\n",
       "             ('encoder.layer.4.intermediate.dense.bias',\n",
       "              tensor([-0.0903, -0.0786, -0.0720,  ..., -0.0707, -0.0524, -0.0544])),\n",
       "             ('encoder.layer.4.output.dense.weight',\n",
       "              tensor([[-0.0186,  0.0600,  0.0072,  ...,  0.0372, -0.0128, -0.0299],\n",
       "                      [-0.0461, -0.0284, -0.0535,  ...,  0.0352, -0.0571,  0.0032],\n",
       "                      [ 0.0153,  0.0824,  0.1038,  ...,  0.0018,  0.0530, -0.0293],\n",
       "                      ...,\n",
       "                      [-0.0374, -0.0460, -0.0649,  ...,  0.1381, -0.0662,  0.0142],\n",
       "                      [ 0.0360, -0.0238,  0.0314,  ...,  0.1027, -0.0172,  0.0126],\n",
       "                      [ 0.0046, -0.0462, -0.0977,  ..., -0.0386,  0.0634, -0.0395]])),\n",
       "             ('encoder.layer.4.output.dense.bias',\n",
       "              tensor([-0.0836, -0.0174,  0.1397,  ...,  0.0101, -0.0993,  0.1644])),\n",
       "             ('encoder.layer.4.output.LayerNorm.weight',\n",
       "              tensor([0.9718, 0.9921, 0.9753,  ..., 0.9767, 0.9954, 0.9648])),\n",
       "             ('encoder.layer.4.output.LayerNorm.bias',\n",
       "              tensor([ 0.0117, -0.1013,  0.2790,  ..., -0.1057, -0.1010, -0.1655])),\n",
       "             ('encoder.layer.5.attention.self.query.weight',\n",
       "              tensor([[ 0.0696, -0.0163,  0.0254,  ..., -0.0148,  0.0225,  0.0364],\n",
       "                      [ 0.0543, -0.0098,  0.0068,  ...,  0.0015,  0.0435, -0.1380],\n",
       "                      [-0.0540, -0.0058, -0.0184,  ...,  0.0226, -0.0478, -0.0012],\n",
       "                      ...,\n",
       "                      [-0.0165, -0.0190, -0.0749,  ..., -0.0332,  0.0130, -0.0007],\n",
       "                      [ 0.0466, -0.0848,  0.0224,  ...,  0.0327,  0.0237, -0.0317],\n",
       "                      [-0.0121,  0.0237,  0.0589,  ...,  0.0098,  0.0665,  0.0554]])),\n",
       "             ('encoder.layer.5.attention.self.query.bias',\n",
       "              tensor([ 0.0316, -0.0281, -0.0246,  ...,  0.0288,  0.1264, -0.1341])),\n",
       "             ('encoder.layer.5.attention.self.key.weight',\n",
       "              tensor([[ 2.3092e-03,  6.4044e-02, -7.1683e-03,  ..., -3.9817e-02,\n",
       "                        4.1984e-03, -1.2712e-02],\n",
       "                      [ 3.9761e-02, -2.0056e-02, -3.4850e-02,  ...,  3.8880e-02,\n",
       "                        2.8714e-02, -3.7616e-02],\n",
       "                      [ 3.3961e-02, -1.5217e-02, -3.2086e-02,  ..., -1.1059e-01,\n",
       "                        4.3813e-02,  3.1376e-02],\n",
       "                      ...,\n",
       "                      [ 2.9711e-02, -3.1813e-04, -6.0137e-03,  ...,  2.4636e-02,\n",
       "                        6.5164e-02,  4.0419e-02],\n",
       "                      [ 1.8341e-02,  5.8092e-03, -1.2373e-02,  ...,  1.3784e-02,\n",
       "                       -1.6825e-02,  8.8078e-03],\n",
       "                      [-7.3110e-02,  1.0303e-04,  5.9765e-02,  ..., -5.3902e-02,\n",
       "                        1.7319e-02,  3.1098e-03]])),\n",
       "             ('encoder.layer.5.attention.self.key.bias',\n",
       "              tensor([ 8.0597e-04, -8.8424e-05, -1.3993e-04,  ...,  1.6795e-03,\n",
       "                       8.8568e-04,  4.0937e-04])),\n",
       "             ('encoder.layer.5.attention.self.value.weight',\n",
       "              tensor([[-0.0195,  0.0534,  0.0133,  ..., -0.0018, -0.0065,  0.0484],\n",
       "                      [-0.0664, -0.0651, -0.0115,  ...,  0.0233, -0.0415, -0.0607],\n",
       "                      [ 0.0390, -0.0433, -0.0294,  ..., -0.0011, -0.0136,  0.0326],\n",
       "                      ...,\n",
       "                      [ 0.0310, -0.0242, -0.0111,  ..., -0.0297, -0.0089, -0.0315],\n",
       "                      [ 0.0570,  0.0287, -0.0969,  ...,  0.0112, -0.0092,  0.0482],\n",
       "                      [-0.0347,  0.0685, -0.0240,  ..., -0.0206, -0.0196,  0.0237]])),\n",
       "             ('encoder.layer.5.attention.self.value.bias',\n",
       "              tensor([ 0.0028, -0.0057, -0.0017,  ...,  0.0085, -0.0057, -0.0092])),\n",
       "             ('encoder.layer.5.attention.output.dense.weight',\n",
       "              tensor([[ 0.0292, -0.0089, -0.0181,  ...,  0.0230,  0.0251, -0.0234],\n",
       "                      [-0.0990,  0.0267,  0.0090,  ..., -0.0534, -0.0192,  0.0456],\n",
       "                      [ 0.0624, -0.0078,  0.0418,  ..., -0.0177, -0.0418, -0.0311],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0522, -0.0147,  ..., -0.0070,  0.0220, -0.0336],\n",
       "                      [ 0.0028,  0.0135,  0.0371,  ...,  0.0060, -0.0698, -0.0063],\n",
       "                      [ 0.0034,  0.0251,  0.0204,  ...,  0.0318, -0.0081, -0.0023]])),\n",
       "             ('encoder.layer.5.attention.output.dense.bias',\n",
       "              tensor([ 0.0067,  0.0095, -0.0753,  ..., -0.0280, -0.0625, -0.0142])),\n",
       "             ('encoder.layer.5.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9906, 0.9987, 0.9987,  ..., 0.9799, 0.9916, 0.9689])),\n",
       "             ('encoder.layer.5.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1211,  0.0559, -0.2694,  ...,  0.0003, -0.0934,  0.0936])),\n",
       "             ('encoder.layer.5.intermediate.dense.weight',\n",
       "              tensor([[-0.0032, -0.0368, -0.0339,  ..., -0.0096, -0.0078, -0.0220],\n",
       "                      [-0.0058, -0.0418, -0.0224,  ..., -0.0153,  0.0688, -0.0404],\n",
       "                      [ 0.0630,  0.0466,  0.0045,  ..., -0.0107, -0.0383,  0.0520],\n",
       "                      ...,\n",
       "                      [-0.0165, -0.0531, -0.0117,  ...,  0.0615, -0.0014, -0.0722],\n",
       "                      [-0.0218, -0.1218, -0.0005,  ...,  0.0276,  0.0662, -0.0345],\n",
       "                      [-0.0619,  0.0393,  0.0500,  ...,  0.0252,  0.0033,  0.0479]])),\n",
       "             ('encoder.layer.5.intermediate.dense.bias',\n",
       "              tensor([-0.0727, -0.1093, -0.1093,  ..., -0.0526, -0.1161, -0.1049])),\n",
       "             ('encoder.layer.5.output.dense.weight',\n",
       "              tensor([[-0.0304,  0.0577, -0.0004,  ..., -0.0179, -0.0366,  0.0265],\n",
       "                      [ 0.0434, -0.0578,  0.0631,  ..., -0.0422, -0.0215, -0.0706],\n",
       "                      [ 0.0057,  0.0139, -0.0123,  ..., -0.0310, -0.0120, -0.0772],\n",
       "                      ...,\n",
       "                      [-0.0680, -0.0633, -0.0333,  ...,  0.0013,  0.0983, -0.0226],\n",
       "                      [-0.0097,  0.0199, -0.0760,  ..., -0.0245, -0.0087,  0.0076],\n",
       "                      [ 0.0016, -0.0223,  0.0838,  ..., -0.0546,  0.0301, -0.0396]])),\n",
       "             ('encoder.layer.5.output.dense.bias',\n",
       "              tensor([-0.1062,  0.0095,  0.1370,  ..., -0.0004, -0.0587,  0.1451])),\n",
       "             ('encoder.layer.5.output.LayerNorm.weight',\n",
       "              tensor([0.9731, 0.9933, 0.9898,  ..., 0.9770, 0.9891, 0.9560])),\n",
       "             ('encoder.layer.5.output.LayerNorm.bias',\n",
       "              tensor([-0.0169, -0.1137,  0.2703,  ..., -0.1180, -0.0460, -0.1477])),\n",
       "             ('encoder.layer.6.attention.self.query.weight',\n",
       "              tensor([[ 0.0193, -0.0282,  0.0555,  ...,  0.0436,  0.0511,  0.0068],\n",
       "                      [-0.0069, -0.0345, -0.0071,  ..., -0.0070, -0.0507, -0.0337],\n",
       "                      [ 0.0384,  0.1495,  0.0862,  ...,  0.0525,  0.0460,  0.0363],\n",
       "                      ...,\n",
       "                      [ 0.0106, -0.0027, -0.1028,  ...,  0.0311,  0.0670,  0.0630],\n",
       "                      [ 0.0264,  0.0181,  0.0189,  ...,  0.0045,  0.0252,  0.0854],\n",
       "                      [-0.0407, -0.0038,  0.0382,  ..., -0.0428,  0.0334, -0.0291]])),\n",
       "             ('encoder.layer.6.attention.self.query.bias',\n",
       "              tensor([-0.3079,  0.0540, -0.3076,  ..., -0.0925,  0.0277,  0.0065])),\n",
       "             ('encoder.layer.6.attention.self.key.weight',\n",
       "              tensor([[ 2.6705e-02, -2.8981e-02,  6.8154e-02,  ..., -1.1777e-02,\n",
       "                       -3.0053e-02, -1.9034e-03],\n",
       "                      [-1.6680e-02,  2.9191e-02,  1.8661e-02,  ...,  9.9379e-02,\n",
       "                        7.6574e-02,  1.5761e-02],\n",
       "                      [ 5.3984e-02,  2.4852e-03,  1.7267e-02,  ..., -1.3851e-02,\n",
       "                       -3.2867e-02, -4.0169e-02],\n",
       "                      ...,\n",
       "                      [-1.7930e-02,  1.3601e-02,  4.1075e-03,  ..., -3.2285e-02,\n",
       "                        1.5076e-02, -1.4399e-03],\n",
       "                      [-7.4041e-02, -6.4802e-02, -7.8933e-05,  ...,  4.5134e-02,\n",
       "                        3.5375e-02, -7.1534e-03],\n",
       "                      [ 1.1552e-02,  3.6611e-02,  8.1868e-04,  ...,  5.5836e-02,\n",
       "                        3.0947e-02, -7.8149e-03]])),\n",
       "             ('encoder.layer.6.attention.self.key.bias',\n",
       "              tensor([ 7.9842e-04, -6.5520e-04,  1.4203e-04,  ...,  7.3000e-05,\n",
       "                       2.9231e-04,  2.2482e-04])),\n",
       "             ('encoder.layer.6.attention.self.value.weight',\n",
       "              tensor([[ 0.0095,  0.0150, -0.0074,  ..., -0.0322, -0.0136, -0.0014],\n",
       "                      [-0.0020,  0.0404,  0.0197,  ..., -0.0579,  0.0352,  0.0148],\n",
       "                      [-0.0175, -0.0582,  0.0553,  ...,  0.0445, -0.0039,  0.0240],\n",
       "                      ...,\n",
       "                      [ 0.0173,  0.0393,  0.0006,  ...,  0.0474, -0.0571, -0.0001],\n",
       "                      [ 0.0115, -0.0162, -0.0035,  ..., -0.0301,  0.0196, -0.0149],\n",
       "                      [ 0.0130,  0.0234,  0.0215,  ..., -0.0326,  0.1090,  0.0263]])),\n",
       "             ('encoder.layer.6.attention.self.value.bias',\n",
       "              tensor([-0.0070, -0.0081, -0.0023,  ..., -0.0176, -0.0096, -0.0026])),\n",
       "             ('encoder.layer.6.attention.output.dense.weight',\n",
       "              tensor([[-0.0384,  0.0059, -0.0178,  ...,  0.0409, -0.0343, -0.0063],\n",
       "                      [ 0.0191, -0.0331, -0.0383,  ...,  0.0190,  0.0385, -0.0207],\n",
       "                      [ 0.0498, -0.0559, -0.0351,  ...,  0.0260,  0.0656, -0.0325],\n",
       "                      ...,\n",
       "                      [-0.0159,  0.0489,  0.0015,  ..., -0.0401,  0.0061,  0.0205],\n",
       "                      [-0.0273,  0.0247,  0.0399,  ...,  0.0239, -0.0203, -0.0406],\n",
       "                      [-0.0220,  0.0466,  0.0235,  ...,  0.0572,  0.0051,  0.0196]])),\n",
       "             ('encoder.layer.6.attention.output.dense.bias',\n",
       "              tensor([ 0.0075, -0.0104,  0.0316,  ..., -0.0004, -0.0670,  0.0097])),\n",
       "             ('encoder.layer.6.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9862, 1.0013, 1.0003,  ..., 0.9837, 0.9787, 0.9506])),\n",
       "             ('encoder.layer.6.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1212,  0.0572, -0.2838,  ..., -0.0245, -0.1038,  0.1010])),\n",
       "             ('encoder.layer.6.intermediate.dense.weight',\n",
       "              tensor([[-0.0071, -0.0098, -0.0131,  ..., -0.0038,  0.0376,  0.0661],\n",
       "                      [ 0.0072,  0.0063,  0.0014,  ..., -0.0299,  0.0244, -0.0398],\n",
       "                      [-0.0613, -0.0765, -0.0498,  ...,  0.0772, -0.0169, -0.0795],\n",
       "                      ...,\n",
       "                      [-0.0476, -0.0446,  0.0056,  ..., -0.0155,  0.0779, -0.0476],\n",
       "                      [ 0.0016, -0.0004,  0.0085,  ...,  0.0120,  0.0120,  0.0071],\n",
       "                      [-0.0136, -0.0633,  0.0474,  ..., -0.0173,  0.0199, -0.1291]])),\n",
       "             ('encoder.layer.6.intermediate.dense.bias',\n",
       "              tensor([-0.0794, -0.0932, -0.1010,  ..., -0.0772, -0.0703, -0.1741])),\n",
       "             ('encoder.layer.6.output.dense.weight',\n",
       "              tensor([[ 0.0259, -0.0312, -0.0172,  ...,  0.0349,  0.0255, -0.0041],\n",
       "                      [ 0.0102, -0.0422, -0.0054,  ..., -0.0080,  0.0352,  0.0073],\n",
       "                      [ 0.0256, -0.0111, -0.0873,  ...,  0.0446, -0.0434,  0.0401],\n",
       "                      ...,\n",
       "                      [ 0.0116, -0.0619,  0.0109,  ..., -0.0156,  0.0554, -0.0435],\n",
       "                      [ 0.0303, -0.0007, -0.1110,  ..., -0.0189, -0.0282,  0.0628],\n",
       "                      [ 0.0386,  0.0105,  0.0164,  ..., -0.0278,  0.0253, -0.0163]])),\n",
       "             ('encoder.layer.6.output.dense.bias',\n",
       "              tensor([-0.0679,  0.0304,  0.0647,  ..., -0.0079, -0.0546,  0.0710])),\n",
       "             ('encoder.layer.6.output.LayerNorm.weight',\n",
       "              tensor([0.9811, 0.9906, 1.0012,  ..., 0.9828, 0.9761, 0.9582])),\n",
       "             ('encoder.layer.6.output.LayerNorm.bias',\n",
       "              tensor([-0.0245, -0.1129,  0.1106,  ..., -0.0819, -0.0282, -0.1339])),\n",
       "             ('encoder.layer.7.attention.self.query.weight',\n",
       "              tensor([[ 0.0185, -0.0181,  0.0032,  ...,  0.0228, -0.0403, -0.1065],\n",
       "                      [ 0.0543, -0.0862, -0.0167,  ...,  0.0195, -0.0403,  0.0204],\n",
       "                      [-0.1298,  0.0167,  0.0233,  ...,  0.0143,  0.0835,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0290,  0.1950,  0.0444,  ...,  0.0560,  0.0095,  0.1272],\n",
       "                      [ 0.0081,  0.0291, -0.0298,  ...,  0.0118,  0.0728,  0.1009],\n",
       "                      [ 0.0154,  0.0120,  0.0262,  ...,  0.0966,  0.1194,  0.0450]])),\n",
       "             ('encoder.layer.7.attention.self.query.bias',\n",
       "              tensor([ 0.0172,  0.0146, -0.0546,  ..., -0.2543,  0.1250, -0.2619])),\n",
       "             ('encoder.layer.7.attention.self.key.weight',\n",
       "              tensor([[ 0.0053, -0.0027, -0.0561,  ..., -0.0105, -0.0037,  0.0654],\n",
       "                      [ 0.0121,  0.0041,  0.0258,  ...,  0.0238, -0.0428,  0.0036],\n",
       "                      [-0.0117, -0.0464, -0.0178,  ...,  0.0210, -0.0370,  0.0288],\n",
       "                      ...,\n",
       "                      [-0.0215,  0.0022,  0.0297,  ..., -0.0529, -0.0783, -0.0426],\n",
       "                      [-0.0691,  0.0410,  0.0162,  ..., -0.0070, -0.0230,  0.0787],\n",
       "                      [ 0.0220,  0.0222,  0.0276,  ...,  0.0083, -0.0122, -0.0368]])),\n",
       "             ('encoder.layer.7.attention.self.key.bias',\n",
       "              tensor([-0.0004, -0.0008, -0.0002,  ...,  0.0003,  0.0006,  0.0002])),\n",
       "             ('encoder.layer.7.attention.self.value.weight',\n",
       "              tensor([[ 0.0064, -0.0172,  0.0034,  ...,  0.0201, -0.0030, -0.0417],\n",
       "                      [-0.0117, -0.0284, -0.0227,  ..., -0.0109, -0.0347, -0.0474],\n",
       "                      [ 0.0262, -0.0224,  0.0259,  ..., -0.0141,  0.0229,  0.0074],\n",
       "                      ...,\n",
       "                      [-0.0030, -0.0401,  0.0145,  ...,  0.0222,  0.0395,  0.0107],\n",
       "                      [-0.0544, -0.0532, -0.0039,  ...,  0.0456,  0.0202,  0.0216],\n",
       "                      [ 0.0333,  0.0006, -0.0263,  ...,  0.0330, -0.0355, -0.0155]])),\n",
       "             ('encoder.layer.7.attention.self.value.bias',\n",
       "              tensor([ 0.0005,  0.0337, -0.0511,  ...,  0.0139,  0.0032,  0.0053])),\n",
       "             ('encoder.layer.7.attention.output.dense.weight',\n",
       "              tensor([[ 0.0014, -0.0079,  0.0260,  ...,  0.0085,  0.0625, -0.0315],\n",
       "                      [-0.0364,  0.0011,  0.0137,  ...,  0.0389, -0.0769,  0.0252],\n",
       "                      [-0.0337, -0.0215,  0.0126,  ...,  0.0451,  0.0387,  0.0361],\n",
       "                      ...,\n",
       "                      [ 0.0459,  0.0038,  0.0501,  ...,  0.0082, -0.0653, -0.0061],\n",
       "                      [ 0.0114,  0.0145,  0.0091,  ..., -0.0073,  0.0165,  0.0344],\n",
       "                      [-0.0114, -0.0275, -0.0085,  ..., -0.0516, -0.0145,  0.0456]])),\n",
       "             ('encoder.layer.7.attention.output.dense.bias',\n",
       "              tensor([-0.0162,  0.0174, -0.0388,  ..., -0.0476, -0.0033,  0.0281])),\n",
       "             ('encoder.layer.7.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9892, 0.9851, 1.0002,  ..., 0.9823, 0.9610, 0.9736])),\n",
       "             ('encoder.layer.7.attention.output.LayerNorm.bias',\n",
       "              tensor([-9.1459e-02,  1.1070e-02, -4.2613e-01,  ..., -7.4886e-02,\n",
       "                      -1.0098e-01, -2.6492e-04])),\n",
       "             ('encoder.layer.7.intermediate.dense.weight',\n",
       "              tensor([[-0.0622,  0.0347,  0.0149,  ...,  0.0606,  0.0032,  0.0336],\n",
       "                      [ 0.0627,  0.0388, -0.0102,  ...,  0.0020, -0.0377, -0.0387],\n",
       "                      [-0.0395, -0.0221, -0.0025,  ...,  0.0158,  0.0172,  0.0366],\n",
       "                      ...,\n",
       "                      [ 0.0088,  0.0281,  0.0030,  ..., -0.0396, -0.0211,  0.0097],\n",
       "                      [-0.0288, -0.1046,  0.0171,  ..., -0.0873, -0.0070, -0.0130],\n",
       "                      [-0.0510, -0.0493,  0.0014,  ...,  0.0427, -0.0267,  0.0326]])),\n",
       "             ('encoder.layer.7.intermediate.dense.bias',\n",
       "              tensor([-0.0927, -0.0914, -0.0100,  ...,  0.0070, -0.0867, -0.0315])),\n",
       "             ('encoder.layer.7.output.dense.weight',\n",
       "              tensor([[-0.0143,  0.0695, -0.0158,  ...,  0.0147, -0.1045, -0.0385],\n",
       "                      [ 0.0345,  0.0794,  0.0069,  ..., -0.0374, -0.0633,  0.0325],\n",
       "                      [-0.0168, -0.0062, -0.0097,  ...,  0.0289,  0.0146,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.0587,  0.0390, -0.0264,  ...,  0.0261, -0.0131,  0.0302],\n",
       "                      [-0.0050, -0.0009, -0.0234,  ...,  0.0273,  0.0183,  0.0127],\n",
       "                      [ 0.0669, -0.0191,  0.0128,  ..., -0.0290,  0.0171, -0.0455]])),\n",
       "             ('encoder.layer.7.output.dense.bias',\n",
       "              tensor([-0.1937,  0.0492,  0.1148,  ..., -0.0160, -0.0419,  0.0590])),\n",
       "             ('encoder.layer.7.output.LayerNorm.weight',\n",
       "              tensor([0.9858, 0.9909, 1.0013,  ..., 0.9834, 0.9777, 0.9512])),\n",
       "             ('encoder.layer.7.output.LayerNorm.bias',\n",
       "              tensor([-0.0313, -0.0966, -0.0044,  ..., -0.0471, -0.0410, -0.1098])),\n",
       "             ('encoder.layer.8.attention.self.query.weight',\n",
       "              tensor([[ 0.0351,  0.0371,  0.0312,  ...,  0.0180,  0.0147,  0.0129],\n",
       "                      [-0.0933, -0.0729,  0.0665,  ..., -0.0361,  0.0025,  0.0127],\n",
       "                      [-0.0665,  0.1043, -0.0322,  ...,  0.0454,  0.0678, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0612,  0.0406, -0.0004,  ..., -0.0051,  0.0854, -0.0045],\n",
       "                      [ 0.0092,  0.0818, -0.0326,  ..., -0.1256,  0.0416,  0.0261],\n",
       "                      [-0.0282, -0.0788,  0.0009,  ...,  0.0038, -0.0191, -0.0440]])),\n",
       "             ('encoder.layer.8.attention.self.query.bias',\n",
       "              tensor([-0.0935,  0.1285,  0.0198,  ..., -0.1389, -0.0540,  0.0562])),\n",
       "             ('encoder.layer.8.attention.self.key.weight',\n",
       "              tensor([[-0.0730,  0.0460,  0.0024,  ..., -0.1577,  0.0517, -0.0138],\n",
       "                      [-0.0546, -0.0021,  0.1166,  ...,  0.0128,  0.0206, -0.0323],\n",
       "                      [ 0.0404, -0.0357, -0.0613,  ..., -0.0279,  0.1109, -0.0297],\n",
       "                      ...,\n",
       "                      [-0.0064,  0.0534, -0.0418,  ...,  0.0588, -0.0009,  0.0285],\n",
       "                      [-0.0803,  0.0314, -0.0484,  ...,  0.0144,  0.0390, -0.0270],\n",
       "                      [-0.0106, -0.0839,  0.0360,  ...,  0.0136,  0.0093, -0.0624]])),\n",
       "             ('encoder.layer.8.attention.self.key.bias',\n",
       "              tensor([ 3.2234e-05,  1.8108e-04, -3.4856e-04,  ...,  7.0236e-05,\n",
       "                      -1.5363e-04,  1.8703e-05])),\n",
       "             ('encoder.layer.8.attention.self.value.weight',\n",
       "              tensor([[ 0.0439, -0.0007, -0.0148,  ..., -0.0445,  0.0309,  0.0346],\n",
       "                      [-0.0505, -0.0335,  0.0007,  ...,  0.0650, -0.0004,  0.0492],\n",
       "                      [ 0.0007, -0.0052,  0.0047,  ...,  0.0014, -0.0077, -0.0692],\n",
       "                      ...,\n",
       "                      [ 0.0039,  0.0314,  0.0156,  ..., -0.0239, -0.0404, -0.0338],\n",
       "                      [ 0.0536, -0.0503,  0.0018,  ..., -0.0025, -0.0172, -0.0141],\n",
       "                      [-0.0270, -0.0169,  0.0029,  ...,  0.0402, -0.0169, -0.0430]])),\n",
       "             ('encoder.layer.8.attention.self.value.bias',\n",
       "              tensor([-0.0070, -0.0151,  0.0099,  ..., -0.0151, -0.0063, -0.0036])),\n",
       "             ('encoder.layer.8.attention.output.dense.weight',\n",
       "              tensor([[ 0.0298, -0.0042, -0.0118,  ..., -0.0030,  0.0065, -0.0016],\n",
       "                      [ 0.0009, -0.0151,  0.0305,  ..., -0.0021,  0.0388,  0.0569],\n",
       "                      [ 0.0494, -0.0360,  0.0225,  ...,  0.0421, -0.0344,  0.0082],\n",
       "                      ...,\n",
       "                      [-0.0306,  0.0278, -0.0137,  ...,  0.0361, -0.0330, -0.0275],\n",
       "                      [-0.0483,  0.0049, -0.0289,  ...,  0.0045,  0.0040,  0.0034],\n",
       "                      [ 0.0271, -0.0149,  0.0309,  ...,  0.0216,  0.0029,  0.0464]])),\n",
       "             ('encoder.layer.8.attention.output.dense.bias',\n",
       "              tensor([-0.0499,  0.0059,  0.0052,  ...,  0.0087, -0.0965,  0.0551])),\n",
       "             ('encoder.layer.8.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9830, 0.9986, 1.0016,  ..., 0.9911, 0.9961, 0.9750])),\n",
       "             ('encoder.layer.8.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1708,  0.0124, -0.5007,  ..., -0.0113, -0.1096, -0.0141])),\n",
       "             ('encoder.layer.8.intermediate.dense.weight',\n",
       "              tensor([[-0.0092, -0.0002,  0.0200,  ..., -0.0606,  0.0106, -0.0812],\n",
       "                      [ 0.0290, -0.0733,  0.0402,  ..., -0.1034,  0.0447, -0.0235],\n",
       "                      [ 0.0341,  0.0984,  0.0415,  ...,  0.0243,  0.0098,  0.0092],\n",
       "                      ...,\n",
       "                      [ 0.0443, -0.0213,  0.0360,  ..., -0.0206, -0.0193,  0.0361],\n",
       "                      [ 0.0910,  0.0172,  0.0101,  ...,  0.0148,  0.0548, -0.0727],\n",
       "                      [ 0.0333,  0.0059,  0.0179,  ..., -0.0268, -0.0697, -0.0133]])),\n",
       "             ('encoder.layer.8.intermediate.dense.bias',\n",
       "              tensor([-0.0612, -0.0992, -0.0530,  ..., -0.0767, -0.0571, -0.0340])),\n",
       "             ('encoder.layer.8.output.dense.weight',\n",
       "              tensor([[ 0.0077,  0.0096,  0.0083,  ..., -0.0271,  0.0689,  0.0103],\n",
       "                      [-0.0341,  0.0179,  0.0646,  ...,  0.0182,  0.0101,  0.0275],\n",
       "                      [ 0.0354, -0.0170,  0.0201,  ...,  0.0370, -0.0034, -0.0018],\n",
       "                      ...,\n",
       "                      [ 0.0720, -0.0569,  0.0309,  ...,  0.0525, -0.0132,  0.0223],\n",
       "                      [-0.0147, -0.0088,  0.0149,  ..., -0.0612, -0.0100, -0.0042],\n",
       "                      [ 0.0218,  0.0214,  0.0238,  ...,  0.0240,  0.0054,  0.0248]])),\n",
       "             ('encoder.layer.8.output.dense.bias',\n",
       "              tensor([-0.1678,  0.0005,  0.0610,  ..., -0.0409, -0.0501, -0.0571])),\n",
       "             ('encoder.layer.8.output.LayerNorm.weight',\n",
       "              tensor([0.9728, 0.9973, 1.0027,  ..., 0.9770, 0.9783, 0.9529])),\n",
       "             ('encoder.layer.8.output.LayerNorm.bias',\n",
       "              tensor([ 0.0211, -0.1112, -0.0228,  ..., -0.0837, -0.0246, -0.0840])),\n",
       "             ('encoder.layer.9.attention.self.query.weight',\n",
       "              tensor([[ 0.0050, -0.0850, -0.0390,  ..., -0.0273,  0.0681, -0.0683],\n",
       "                      [ 0.0204, -0.0374, -0.0089,  ..., -0.0319,  0.0459,  0.0598],\n",
       "                      [ 0.0180,  0.0219,  0.0151,  ..., -0.0363,  0.0131, -0.0270],\n",
       "                      ...,\n",
       "                      [ 0.0577, -0.0246, -0.0012,  ..., -0.0113, -0.0648, -0.0141],\n",
       "                      [ 0.0474,  0.0105, -0.0789,  ...,  0.0323,  0.0250,  0.0232],\n",
       "                      [-0.0189, -0.0179, -0.0387,  ..., -0.0521, -0.0355, -0.0435]])),\n",
       "             ('encoder.layer.9.attention.self.query.bias',\n",
       "              tensor([-0.0118, -0.0291, -0.0616,  ...,  0.0024, -0.0573,  0.0734])),\n",
       "             ('encoder.layer.9.attention.self.key.weight',\n",
       "              tensor([[-0.0366,  0.0490,  0.0026,  ..., -0.0112, -0.0294, -0.0408],\n",
       "                      [ 0.0357, -0.0099, -0.0043,  ..., -0.0299, -0.0200, -0.0415],\n",
       "                      [ 0.0353, -0.0511,  0.0697,  ..., -0.0658, -0.0248,  0.0148],\n",
       "                      ...,\n",
       "                      [-0.0159,  0.0132,  0.0030,  ...,  0.0085,  0.0561,  0.0069],\n",
       "                      [ 0.0426,  0.0309, -0.0116,  ...,  0.0172,  0.0319,  0.0064],\n",
       "                      [-0.0381,  0.0078, -0.0301,  ...,  0.0883, -0.0020,  0.0989]])),\n",
       "             ('encoder.layer.9.attention.self.key.bias',\n",
       "              tensor([-1.2432e-04, -6.5041e-06,  2.7444e-04,  ...,  1.6023e-04,\n",
       "                       2.4733e-04,  1.4629e-04])),\n",
       "             ('encoder.layer.9.attention.self.value.weight',\n",
       "              tensor([[-0.0248,  0.0018,  0.0100,  ..., -0.0214, -0.0540,  0.0280],\n",
       "                      [-0.0229, -0.0303, -0.0006,  ..., -0.0541, -0.0064,  0.0547],\n",
       "                      [-0.0002,  0.0566,  0.0114,  ...,  0.0241,  0.0257,  0.0119],\n",
       "                      ...,\n",
       "                      [-0.0514,  0.0349,  0.0138,  ...,  0.0578,  0.0554, -0.0078],\n",
       "                      [ 0.0542,  0.0163, -0.0111,  ..., -0.0335, -0.0387, -0.0317],\n",
       "                      [-0.0338,  0.0155,  0.0086,  ...,  0.0233,  0.0047, -0.0208]])),\n",
       "             ('encoder.layer.9.attention.self.value.bias',\n",
       "              tensor([ 0.0010, -0.0077,  0.0017,  ...,  0.0326, -0.0019,  0.0015])),\n",
       "             ('encoder.layer.9.attention.output.dense.weight',\n",
       "              tensor([[-0.0206, -0.0253,  0.0100,  ...,  0.0210, -0.0113, -0.0178],\n",
       "                      [ 0.0026, -0.0394,  0.0025,  ..., -0.0450,  0.0160,  0.0200],\n",
       "                      [ 0.0354, -0.0691, -0.0008,  ..., -0.0133,  0.0389, -0.0043],\n",
       "                      ...,\n",
       "                      [ 0.0087,  0.0148,  0.0283,  ..., -0.0186,  0.0039,  0.0010],\n",
       "                      [-0.0147, -0.0096, -0.0080,  ..., -0.0775, -0.0189,  0.0559],\n",
       "                      [ 0.0275,  0.0102,  0.0276,  ..., -0.0142, -0.0114, -0.0006]])),\n",
       "             ('encoder.layer.9.attention.output.dense.bias',\n",
       "              tensor([ 0.0467,  0.0037, -0.1739,  ...,  0.0523, -0.0487,  0.0047])),\n",
       "             ('encoder.layer.9.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9829, 0.9930, 1.0029,  ..., 0.9825, 0.9903, 0.9871])),\n",
       "             ('encoder.layer.9.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1799, -0.0283, -0.4671,  ..., -0.0486, -0.1158, -0.0284])),\n",
       "             ('encoder.layer.9.intermediate.dense.weight',\n",
       "              tensor([[ 0.0266,  0.0059, -0.0159,  ...,  0.0252,  0.0225,  0.0016],\n",
       "                      [ 0.0090, -0.0011,  0.0183,  ...,  0.0807,  0.0242,  0.0174],\n",
       "                      [ 0.0483, -0.0525,  0.0124,  ...,  0.0628, -0.0763,  0.0652],\n",
       "                      ...,\n",
       "                      [ 0.0073, -0.0334,  0.0134,  ...,  0.0689,  0.0028, -0.0205],\n",
       "                      [ 0.0123, -0.0572,  0.0071,  ...,  0.0023, -0.0063, -0.0105],\n",
       "                      [-0.0198, -0.0297,  0.0125,  ...,  0.0124,  0.0494,  0.0228]])),\n",
       "             ('encoder.layer.9.intermediate.dense.bias',\n",
       "              tensor([-0.0539, -0.0791, -0.0242,  ..., -0.0477, -0.0428, -0.1011])),\n",
       "             ('encoder.layer.9.output.dense.weight',\n",
       "              tensor([[-0.0015,  0.0225, -0.0158,  ..., -0.0076,  0.0216,  0.0041],\n",
       "                      [ 0.0322,  0.0783, -0.0158,  ..., -0.0223, -0.0089, -0.0102],\n",
       "                      [ 0.0139,  0.0040,  0.0172,  ...,  0.0133, -0.0042, -0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0117,  0.0916, -0.0471,  ..., -0.0515,  0.0214,  0.0149],\n",
       "                      [ 0.0341,  0.0454, -0.0476,  ...,  0.0439,  0.0305, -0.0034],\n",
       "                      [-0.0380, -0.0251, -0.0483,  ..., -0.0112,  0.0199,  0.0035]])),\n",
       "             ('encoder.layer.9.output.dense.bias',\n",
       "              tensor([-0.1375,  0.0669, -0.0882,  ..., -0.0442, -0.0771, -0.0243])),\n",
       "             ('encoder.layer.9.output.LayerNorm.weight',\n",
       "              tensor([0.9744, 0.9903, 1.0025,  ..., 0.9692, 0.9811, 0.9733])),\n",
       "             ('encoder.layer.9.output.LayerNorm.bias',\n",
       "              tensor([ 0.0324, -0.0623, -0.2119,  ..., -0.0588, -0.0130, -0.0455])),\n",
       "             ('encoder.layer.10.attention.self.query.weight',\n",
       "              tensor([[ 0.0644,  0.0338, -0.0017,  ..., -0.1437,  0.0438,  0.0173],\n",
       "                      [ 0.0672, -0.1071, -0.0099,  ...,  0.0778,  0.0017, -0.0280],\n",
       "                      [-0.0353, -0.0128, -0.0358,  ...,  0.0767,  0.0924, -0.0374],\n",
       "                      ...,\n",
       "                      [ 0.0331,  0.0380,  0.2361,  ...,  0.0898, -0.0637,  0.0597],\n",
       "                      [ 0.0209,  0.0161,  0.0616,  ...,  0.1120,  0.0528, -0.0468],\n",
       "                      [ 0.0264, -0.0206,  0.0475,  ...,  0.0488,  0.0565, -0.0206]])),\n",
       "             ('encoder.layer.10.attention.self.query.bias',\n",
       "              tensor([ 0.0235,  0.0309, -0.0160,  ...,  0.0291,  0.0557,  0.0001])),\n",
       "             ('encoder.layer.10.attention.self.key.weight',\n",
       "              tensor([[-0.0142,  0.0392,  0.0552,  ..., -0.0167, -0.0268, -0.0469],\n",
       "                      [-0.0984, -0.1001,  0.0237,  ..., -0.0702,  0.0008, -0.0055],\n",
       "                      [-0.0340,  0.0719, -0.0070,  ..., -0.1036, -0.0012, -0.0556],\n",
       "                      ...,\n",
       "                      [ 0.0171, -0.0104,  0.2561,  ..., -0.0767, -0.0306,  0.0270],\n",
       "                      [ 0.0076, -0.0074,  0.0801,  ..., -0.0080,  0.0006, -0.0116],\n",
       "                      [-0.0303, -0.0456,  0.0275,  ...,  0.0629, -0.0084, -0.0254]])),\n",
       "             ('encoder.layer.10.attention.self.key.bias',\n",
       "              tensor([-0.0006, -0.0008,  0.0005,  ...,  0.0003,  0.0003, -0.0005])),\n",
       "             ('encoder.layer.10.attention.self.value.weight',\n",
       "              tensor([[ 0.0086,  0.0407,  0.0104,  ..., -0.0014, -0.0259, -0.0282],\n",
       "                      [ 0.0223, -0.0056, -0.0101,  ..., -0.0125, -0.0044, -0.0189],\n",
       "                      [ 0.0357,  0.0495,  0.0034,  ..., -0.0036,  0.0292, -0.0175],\n",
       "                      ...,\n",
       "                      [-0.0152,  0.0079, -0.0006,  ..., -0.0031,  0.0079, -0.0372],\n",
       "                      [ 0.0151,  0.0161, -0.0083,  ...,  0.0183, -0.0391,  0.0172],\n",
       "                      [-0.0257, -0.0174, -0.0151,  ..., -0.0243,  0.0073, -0.0161]])),\n",
       "             ('encoder.layer.10.attention.self.value.bias',\n",
       "              tensor([ 0.0218,  0.0105, -0.0324,  ...,  0.0199, -0.0135,  0.0499])),\n",
       "             ('encoder.layer.10.attention.output.dense.weight',\n",
       "              tensor([[ 0.0046,  0.0546, -0.0090,  ...,  0.0136,  0.0357,  0.0183],\n",
       "                      [-0.0202, -0.0055, -0.0151,  ..., -0.0426,  0.0250, -0.0162],\n",
       "                      [ 0.0306,  0.0227, -0.1361,  ..., -0.0683, -0.0638, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0072, -0.0356,  ..., -0.0122, -0.0058, -0.0201],\n",
       "                      [ 0.0323, -0.0034,  0.0452,  ..., -0.0149,  0.0017,  0.0148],\n",
       "                      [ 0.0519, -0.0065,  0.0046,  ..., -0.0071, -0.0169, -0.0294]])),\n",
       "             ('encoder.layer.10.attention.output.dense.bias',\n",
       "              tensor([-0.0045,  0.0636,  0.0535,  ...,  0.0352, -0.0606,  0.0739])),\n",
       "             ('encoder.layer.10.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9715, 0.9911, 1.0044,  ..., 0.9864, 0.9889, 0.9854])),\n",
       "             ('encoder.layer.10.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1821, -0.0033, -0.4731,  ..., -0.0692, -0.1516, -0.0398])),\n",
       "             ('encoder.layer.10.intermediate.dense.weight',\n",
       "              tensor([[-1.8863e-02, -8.5708e-03,  5.7679e-03,  ..., -1.6899e-02,\n",
       "                       -1.0733e-02, -7.1949e-02],\n",
       "                      [ 1.0198e-02,  2.1469e-02,  3.5953e-02,  ...,  6.0731e-02,\n",
       "                        2.9064e-02, -6.9223e-03],\n",
       "                      [ 1.1926e-03, -1.4712e-02,  2.8346e-02,  ...,  3.0735e-02,\n",
       "                        3.4861e-02, -1.3617e-04],\n",
       "                      ...,\n",
       "                      [ 2.0645e-02,  1.4888e-02,  2.1014e-02,  ...,  7.4071e-02,\n",
       "                       -3.6218e-02,  4.6956e-03],\n",
       "                      [ 5.2346e-02, -1.8612e-02,  4.3913e-03,  ...,  3.0272e-02,\n",
       "                        5.7747e-02, -2.8693e-02],\n",
       "                      [-1.1361e-02, -1.4763e-02, -8.9312e-03,  ...,  1.9233e-02,\n",
       "                       -7.5267e-05, -2.6773e-02]])),\n",
       "             ('encoder.layer.10.intermediate.dense.bias',\n",
       "              tensor([-0.0571, -0.0497, -0.0150,  ..., -0.0925, -0.0907, -0.0141])),\n",
       "             ('encoder.layer.10.output.dense.weight',\n",
       "              tensor([[-1.9962e-02,  3.1327e-03, -2.4093e-03,  ...,  6.0104e-02,\n",
       "                        1.4444e-02, -5.3363e-02],\n",
       "                      [-3.6860e-02,  8.6992e-03,  9.3445e-04,  ...,  1.0934e-02,\n",
       "                       -2.3515e-02, -4.6879e-02],\n",
       "                      [ 4.8024e-03, -8.5307e-03,  1.0649e-02,  ...,  1.8204e-03,\n",
       "                       -1.0152e-02, -2.3250e-03],\n",
       "                      ...,\n",
       "                      [ 1.0390e-02, -1.7303e-02, -1.5642e-02,  ...,  1.9846e-02,\n",
       "                       -2.0711e-03, -7.6077e-03],\n",
       "                      [-1.9605e-02, -9.3557e-02, -4.6648e-02,  ...,  2.1371e-02,\n",
       "                        8.9965e-03, -4.7900e-02],\n",
       "                      [-2.7127e-02, -8.9644e-05, -4.0727e-02,  ...,  2.3736e-03,\n",
       "                        2.6104e-02,  1.0914e-02]])),\n",
       "             ('encoder.layer.10.output.dense.bias',\n",
       "              tensor([-0.2127,  0.0540,  0.0876,  ..., -0.0426,  0.0076, -0.1139])),\n",
       "             ('encoder.layer.10.output.LayerNorm.weight',\n",
       "              tensor([0.9778, 0.9890, 1.0052,  ..., 0.9772, 0.9739, 0.9845])),\n",
       "             ('encoder.layer.10.output.LayerNorm.bias',\n",
       "              tensor([ 0.0436, -0.0737, -0.2045,  ..., -0.0419,  0.0370, -0.0470])),\n",
       "             ('encoder.layer.11.attention.self.query.weight',\n",
       "              tensor([[ 0.0355, -0.0125,  0.0251,  ...,  0.0173,  0.0878,  0.0395],\n",
       "                      [ 0.0721, -0.0448, -0.0032,  ..., -0.0246,  0.0175,  0.0327],\n",
       "                      [ 0.0518, -0.0480, -0.2144,  ...,  0.0501,  0.0696, -0.0314],\n",
       "                      ...,\n",
       "                      [ 0.0934, -0.0240,  0.0841,  ..., -0.0402,  0.0634, -0.0685],\n",
       "                      [-0.0665, -0.0240, -0.0634,  ..., -0.0202, -0.0352, -0.0834],\n",
       "                      [ 0.0377,  0.0004, -0.0355,  ..., -0.0563, -0.0939, -0.0461]])),\n",
       "             ('encoder.layer.11.attention.self.query.bias',\n",
       "              tensor([0.0099, 0.0296, 0.0469,  ..., 0.1012, 0.0175, 0.0241])),\n",
       "             ('encoder.layer.11.attention.self.key.weight',\n",
       "              tensor([[-0.0919, -0.0075,  0.0755,  ...,  0.0049, -0.0243, -0.0111],\n",
       "                      [-0.0619, -0.0137, -0.0295,  ...,  0.0226, -0.0332,  0.0743],\n",
       "                      [-0.1137,  0.0075, -0.2324,  ..., -0.0596, -0.0072,  0.0065],\n",
       "                      ...,\n",
       "                      [-0.0401, -0.0296,  0.1082,  ..., -0.0749, -0.0312, -0.0233],\n",
       "                      [-0.0141,  0.0060, -0.0660,  ..., -0.1021,  0.0085,  0.0085],\n",
       "                      [ 0.0123, -0.0766, -0.0357,  ..., -0.0419, -0.0183, -0.0171]])),\n",
       "             ('encoder.layer.11.attention.self.key.bias',\n",
       "              tensor([ 7.2337e-04, -4.6847e-05, -6.4676e-05,  ...,  8.2156e-05,\n",
       "                       1.0134e-04,  1.1949e-04])),\n",
       "             ('encoder.layer.11.attention.self.value.weight',\n",
       "              tensor([[ 1.0809e-02, -3.0021e-02,  3.1223e-03,  ...,  6.4239e-02,\n",
       "                        2.1183e-03, -2.4565e-03],\n",
       "                      [ 2.4641e-02, -3.2812e-02, -4.3955e-03,  ..., -7.8550e-02,\n",
       "                       -3.2371e-02,  4.7834e-02],\n",
       "                      [-4.5756e-02, -1.2596e-02,  2.7060e-02,  ..., -8.6397e-02,\n",
       "                       -2.9847e-02,  2.0190e-02],\n",
       "                      ...,\n",
       "                      [-5.2640e-03,  4.4996e-02, -1.4779e-03,  ...,  5.9061e-07,\n",
       "                       -4.7987e-02, -5.3482e-03],\n",
       "                      [ 4.3557e-02,  2.7927e-02,  1.0601e-03,  ...,  4.6891e-02,\n",
       "                        3.4784e-02, -7.7991e-02],\n",
       "                      [-1.0719e-02, -7.0173e-04,  1.5301e-03,  ..., -4.4896e-02,\n",
       "                        3.1770e-02, -2.4320e-02]])),\n",
       "             ('encoder.layer.11.attention.self.value.bias',\n",
       "              tensor([-0.0101,  0.0121,  0.0058,  ..., -0.0276,  0.0115, -0.0347])),\n",
       "             ('encoder.layer.11.attention.output.dense.weight',\n",
       "              tensor([[-0.0049, -0.0266,  0.0217,  ...,  0.0086,  0.0080, -0.0285],\n",
       "                      [ 0.0444,  0.0068,  0.0266,  ...,  0.0090, -0.0039,  0.0341],\n",
       "                      [-0.0253,  0.0090, -0.0319,  ...,  0.0069,  0.0182, -0.0226],\n",
       "                      ...,\n",
       "                      [-0.0107, -0.0001,  0.0627,  ...,  0.0240,  0.0260, -0.0172],\n",
       "                      [ 0.0047,  0.0191,  0.0357,  ...,  0.0087,  0.0314, -0.0048],\n",
       "                      [ 0.0238, -0.0562, -0.0012,  ..., -0.0171, -0.0254, -0.0248]])),\n",
       "             ('encoder.layer.11.attention.output.dense.bias',\n",
       "              tensor([-0.0508,  0.1443, -0.1531,  ...,  0.0583, -0.0472,  0.0364])),\n",
       "             ('encoder.layer.11.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9784, 0.9940, 1.0031,  ..., 0.9917, 0.9919, 0.9777])),\n",
       "             ('encoder.layer.11.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.2133,  0.0080, -0.3489,  ..., -0.0626, -0.0714, -0.0352])),\n",
       "             ('encoder.layer.11.intermediate.dense.weight',\n",
       "              tensor([[ 0.0034, -0.0353, -0.0067,  ...,  0.0515,  0.0252,  0.0378],\n",
       "                      [ 0.1033,  0.0596,  0.0042,  ...,  0.0590,  0.0305, -0.0861],\n",
       "                      [ 0.0029,  0.0423, -0.0076,  ...,  0.0361,  0.0067, -0.0344],\n",
       "                      ...,\n",
       "                      [ 0.0571,  0.0102,  0.0086,  ..., -0.0046,  0.0499, -0.1040],\n",
       "                      [ 0.0515,  0.0052,  0.0140,  ...,  0.0609, -0.0283,  0.0374],\n",
       "                      [ 0.0684,  0.0218,  0.0373,  ...,  0.0183,  0.0012,  0.0081]])),\n",
       "             ('encoder.layer.11.intermediate.dense.bias',\n",
       "              tensor([ 0.0248, -0.0456, -0.0559,  ..., -0.0544, -0.0630,  0.0368])),\n",
       "             ('encoder.layer.11.output.dense.weight',\n",
       "              tensor([[ 0.0098,  0.0335, -0.0023,  ...,  0.0081,  0.0275, -0.0522],\n",
       "                      [ 0.0206,  0.0413,  0.0047,  ..., -0.0191,  0.0437,  0.0100],\n",
       "                      [-0.0155,  0.0215, -0.0077,  ..., -0.0035, -0.0071, -0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0472, -0.0580,  0.0628,  ..., -0.0206, -0.0164, -0.0100],\n",
       "                      [ 0.0138,  0.0288, -0.0423,  ...,  0.0095, -0.0359,  0.0126],\n",
       "                      [ 0.0225, -0.0406, -0.0006,  ..., -0.0285, -0.0605, -0.0121]])),\n",
       "             ('encoder.layer.11.output.dense.bias',\n",
       "              tensor([-0.1760, -0.0100, -0.0053,  ..., -0.0141, -0.0834, -0.1056])),\n",
       "             ('encoder.layer.11.output.LayerNorm.weight',\n",
       "              tensor([0.9848, 0.9923, 1.0042,  ..., 0.9757, 0.9795, 0.9907])),\n",
       "             ('encoder.layer.11.output.LayerNorm.bias',\n",
       "              tensor([ 0.0848, -0.0880,  0.1761,  ..., -0.0380, -0.0194, -0.0721])),\n",
       "             ('encoder.layer.12.attention.self.query.weight',\n",
       "              tensor([[-0.0475, -0.0247,  0.0863,  ..., -0.0003,  0.0499,  0.0537],\n",
       "                      [-0.0175,  0.0044,  0.0121,  ..., -0.0778, -0.0659,  0.0494],\n",
       "                      [-0.0530,  0.0365, -0.1333,  ...,  0.0568,  0.0263,  0.0522],\n",
       "                      ...,\n",
       "                      [ 0.0246,  0.0211,  0.2901,  ..., -0.0095,  0.0939, -0.0188],\n",
       "                      [-0.0828,  0.0136, -0.1099,  ..., -0.0338,  0.0128, -0.0223],\n",
       "                      [-0.0058, -0.0064, -0.0309,  ..., -0.0869, -0.0018, -0.0832]])),\n",
       "             ('encoder.layer.12.attention.self.query.bias',\n",
       "              tensor([ 0.0048, -0.0058,  0.0028,  ..., -0.2408, -0.0190, -0.0040])),\n",
       "             ('encoder.layer.12.attention.self.key.weight',\n",
       "              tensor([[-2.7966e-02, -1.2413e-01,  7.4972e-02,  ...,  3.4682e-02,\n",
       "                       -1.9090e-02,  6.5085e-02],\n",
       "                      [ 1.1954e-01,  2.4933e-02, -7.4801e-02,  ..., -1.0793e-03,\n",
       "                       -2.5870e-02, -1.8685e-02],\n",
       "                      [-4.1669e-02,  3.1998e-02, -1.0668e-01,  ..., -7.5540e-02,\n",
       "                       -4.4513e-02, -6.4146e-04],\n",
       "                      ...,\n",
       "                      [-5.6582e-03,  1.3525e-02, -3.7026e-01,  ...,  5.9345e-02,\n",
       "                       -6.1806e-03, -8.7209e-03],\n",
       "                      [ 2.6102e-02, -3.3029e-04, -1.9335e-01,  ..., -2.7492e-02,\n",
       "                        5.9658e-03,  5.4690e-02],\n",
       "                      [ 6.7986e-04, -4.6881e-02, -9.6600e-02,  ...,  4.7075e-02,\n",
       "                        9.1047e-03,  8.7366e-03]])),\n",
       "             ('encoder.layer.12.attention.self.key.bias',\n",
       "              tensor([ 1.3773e-04, -1.0360e-04,  8.2409e-05,  ..., -1.7777e-02,\n",
       "                       4.8635e-04, -1.2870e-04])),\n",
       "             ('encoder.layer.12.attention.self.value.weight',\n",
       "              tensor([[-0.0888, -0.0015,  0.0080,  ...,  0.0433, -0.0050, -0.0079],\n",
       "                      [ 0.0298,  0.0997, -0.0126,  ..., -0.0121,  0.0308,  0.0835],\n",
       "                      [ 0.0191,  0.0048,  0.0001,  ..., -0.0400, -0.0426,  0.0317],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0454, -0.0153,  ..., -0.0057, -0.0121,  0.0577],\n",
       "                      [-0.0394, -0.0411,  0.0241,  ..., -0.0107, -0.0391,  0.0621],\n",
       "                      [ 0.0014, -0.0248, -0.0005,  ..., -0.0355,  0.0319,  0.0286]])),\n",
       "             ('encoder.layer.12.attention.self.value.bias',\n",
       "              tensor([ 0.0056, -0.0075,  0.0002,  ...,  0.0293,  0.0157,  0.0312])),\n",
       "             ('encoder.layer.12.attention.output.dense.weight',\n",
       "              tensor([[-0.0418, -0.0167, -0.0213,  ...,  0.0174, -0.0028, -0.0055],\n",
       "                      [-0.0475,  0.0129, -0.0429,  ..., -0.0312,  0.0026,  0.0150],\n",
       "                      [-0.0410, -0.0201,  0.0273,  ...,  0.0147,  0.0391,  0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0169, -0.0115, -0.0398,  ..., -0.0103,  0.0162,  0.0304],\n",
       "                      [-0.0006, -0.0290,  0.0191,  ..., -0.0254,  0.0188, -0.0163],\n",
       "                      [ 0.0032,  0.0390,  0.0416,  ..., -0.0067, -0.0348, -0.0306]])),\n",
       "             ('encoder.layer.12.attention.output.dense.bias',\n",
       "              tensor([ 0.0294, -0.0023,  0.0637,  ..., -0.0140, -0.0425, -0.0076])),\n",
       "             ('encoder.layer.12.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9915, 0.9816, 1.0016,  ..., 0.9834, 0.9800, 0.9860])),\n",
       "             ('encoder.layer.12.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1654,  0.0028, -0.3039,  ..., -0.0289, -0.1440, -0.0363])),\n",
       "             ('encoder.layer.12.intermediate.dense.weight',\n",
       "              tensor([[ 0.0739, -0.0294,  0.0162,  ..., -0.0094, -0.0057,  0.0086],\n",
       "                      [-0.0161, -0.0292,  0.0173,  ..., -0.0408, -0.0390, -0.0223],\n",
       "                      [-0.0434, -0.0426,  0.0854,  ...,  0.0042, -0.0191, -0.0245],\n",
       "                      ...,\n",
       "                      [ 0.0186, -0.0462,  0.0266,  ..., -0.1220, -0.0161, -0.0043],\n",
       "                      [ 0.0189, -0.0508,  0.0145,  ..., -0.0490, -0.0209,  0.0485],\n",
       "                      [ 0.0222,  0.0111, -0.0055,  ...,  0.0167, -0.0272, -0.0102]])),\n",
       "             ('encoder.layer.12.intermediate.dense.bias',\n",
       "              tensor([-0.1080, -0.0400, -0.0091,  ..., -0.1226, -0.0848, -0.1054])),\n",
       "             ('encoder.layer.12.output.dense.weight',\n",
       "              tensor([[ 0.0308, -0.0021, -0.0703,  ...,  0.0262,  0.0270, -0.0182],\n",
       "                      [-0.0123,  0.0219,  0.0142,  ..., -0.0284,  0.0052, -0.0354],\n",
       "                      [ 0.0023,  0.0002, -0.0147,  ..., -0.0034, -0.0256, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0099, -0.0109,  0.0162,  ..., -0.0425, -0.0155, -0.0239],\n",
       "                      [-0.0204, -0.0094, -0.0201,  ..., -0.0120, -0.0294,  0.0208],\n",
       "                      [-0.0518, -0.0348,  0.0324,  ...,  0.0067, -0.0031, -0.0126]])),\n",
       "             ('encoder.layer.12.output.dense.bias',\n",
       "              tensor([-0.2184,  0.1663, -0.0793,  ..., -0.0086, -0.0801, -0.1236])),\n",
       "             ('encoder.layer.12.output.LayerNorm.weight',\n",
       "              tensor([0.9865, 0.9920, 1.0001,  ..., 0.9871, 0.9832, 0.9916])),\n",
       "             ('encoder.layer.12.output.LayerNorm.bias',\n",
       "              tensor([ 0.0082, -0.0793,  0.0842,  ..., -0.0699, -0.0124, -0.0616])),\n",
       "             ('encoder.layer.13.attention.self.query.weight',\n",
       "              tensor([[ 4.5931e-02,  4.9266e-02, -4.8655e-02,  ..., -4.2673e-02,\n",
       "                        4.5467e-02, -5.5790e-02],\n",
       "                      [-2.5418e-02, -9.2127e-03,  5.0338e-02,  ...,  3.3152e-02,\n",
       "                        2.9748e-02,  6.2740e-02],\n",
       "                      [-3.3130e-03, -2.5667e-02,  1.7722e-05,  ..., -3.6651e-02,\n",
       "                       -7.7123e-02, -1.0718e-02],\n",
       "                      ...,\n",
       "                      [ 6.6869e-02, -7.7316e-03,  2.9270e-02,  ...,  5.5123e-03,\n",
       "                       -1.6933e-02, -6.8674e-03],\n",
       "                      [-5.4457e-03, -2.5675e-02, -1.4281e-01,  ...,  4.3716e-02,\n",
       "                        1.9372e-02, -2.7634e-02],\n",
       "                      [ 5.0016e-02,  5.0795e-02, -1.5821e-01,  ...,  2.3957e-02,\n",
       "                        3.8473e-02,  6.6796e-02]])),\n",
       "             ('encoder.layer.13.attention.self.query.bias',\n",
       "              tensor([-0.0486,  0.0439,  0.0074,  ...,  0.0420,  0.1790,  0.0482])),\n",
       "             ('encoder.layer.13.attention.self.key.weight',\n",
       "              tensor([[ 0.0244,  0.0645, -0.0422,  ..., -0.0145,  0.0274, -0.0693],\n",
       "                      [ 0.0558, -0.0046,  0.0740,  ..., -0.0332, -0.0066, -0.0338],\n",
       "                      [ 0.0171,  0.0649, -0.0412,  ...,  0.1563,  0.0276, -0.0790],\n",
       "                      ...,\n",
       "                      [-0.0485, -0.0495, -0.0118,  ..., -0.0220,  0.0125, -0.0263],\n",
       "                      [ 0.0218,  0.0306, -0.2099,  ..., -0.0200, -0.0193,  0.0028],\n",
       "                      [-0.0396,  0.0443, -0.1316,  ...,  0.0213,  0.0100,  0.0307]])),\n",
       "             ('encoder.layer.13.attention.self.key.bias',\n",
       "              tensor([ 1.1498e-04, -1.3241e-04, -1.2540e-04,  ..., -2.2583e-05,\n",
       "                       5.2453e-04, -2.1498e-04])),\n",
       "             ('encoder.layer.13.attention.self.value.weight',\n",
       "              tensor([[ 0.0549,  0.0651, -0.0165,  ...,  0.0338,  0.0051,  0.0385],\n",
       "                      [ 0.0179,  0.0546,  0.0071,  ..., -0.0367, -0.0208,  0.0255],\n",
       "                      [-0.0040,  0.0470, -0.0015,  ..., -0.0208, -0.0574,  0.0385],\n",
       "                      ...,\n",
       "                      [ 0.0433, -0.0719,  0.0184,  ...,  0.0006, -0.0157, -0.0233],\n",
       "                      [-0.0011, -0.0001, -0.0385,  ...,  0.0091, -0.1073, -0.0093],\n",
       "                      [-0.0407, -0.0931,  0.0246,  ...,  0.0003, -0.0071, -0.0169]])),\n",
       "             ('encoder.layer.13.attention.self.value.bias',\n",
       "              tensor([ 0.0119,  0.0081,  0.0065,  ..., -0.0068, -0.0074, -0.0007])),\n",
       "             ('encoder.layer.13.attention.output.dense.weight',\n",
       "              tensor([[ 0.0010, -0.0387,  0.0297,  ..., -0.0498,  0.0024,  0.1017],\n",
       "                      [ 0.0072,  0.0100, -0.0059,  ...,  0.0062, -0.0690,  0.0440],\n",
       "                      [-0.0034, -0.0105, -0.0188,  ...,  0.0149, -0.0185, -0.0016],\n",
       "                      ...,\n",
       "                      [-0.0385, -0.0035, -0.0453,  ...,  0.0008, -0.0419,  0.0191],\n",
       "                      [-0.0425,  0.0604, -0.0025,  ...,  0.0148,  0.0768, -0.0290],\n",
       "                      [ 0.0017,  0.0066,  0.0111,  ...,  0.0193,  0.0353,  0.0488]])),\n",
       "             ('encoder.layer.13.attention.output.dense.bias',\n",
       "              tensor([-0.0425,  0.0442, -0.0898,  ...,  0.0373, -0.0335,  0.0340])),\n",
       "             ('encoder.layer.13.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9773, 0.9899, 1.0011,  ..., 0.9885, 0.9825, 0.9779])),\n",
       "             ('encoder.layer.13.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1790,  0.0707, -0.2714,  ..., -0.0597, -0.1190, -0.0613])),\n",
       "             ('encoder.layer.13.intermediate.dense.weight',\n",
       "              tensor([[ 0.0216, -0.0244,  0.0193,  ..., -0.0302, -0.0576, -0.0473],\n",
       "                      [ 0.0322,  0.0015,  0.0092,  ...,  0.0496,  0.0252,  0.0008],\n",
       "                      [ 0.0582, -0.0420,  0.0541,  ...,  0.1166, -0.0122, -0.0597],\n",
       "                      ...,\n",
       "                      [ 0.0292, -0.0133, -0.0071,  ..., -0.0128,  0.0365,  0.0081],\n",
       "                      [ 0.0144,  0.0402,  0.0946,  ...,  0.0483,  0.0402, -0.0367],\n",
       "                      [ 0.0354, -0.1298,  0.0403,  ...,  0.0050, -0.0003, -0.0759]])),\n",
       "             ('encoder.layer.13.intermediate.dense.bias',\n",
       "              tensor([-0.1376, -0.0521, -0.0895,  ..., -0.1024, -0.0341, -0.0380])),\n",
       "             ('encoder.layer.13.output.dense.weight',\n",
       "              tensor([[ 0.0600, -0.0353, -0.0435,  ..., -0.0355,  0.0136, -0.0079],\n",
       "                      [-0.0116,  0.0065, -0.0548,  ...,  0.0078,  0.0216, -0.0241],\n",
       "                      [ 0.0126,  0.0040,  0.0016,  ..., -0.0020,  0.0062, -0.0058],\n",
       "                      ...,\n",
       "                      [-0.0289,  0.0376,  0.0041,  ..., -0.0336,  0.0148,  0.0330],\n",
       "                      [-0.0877, -0.0282,  0.0317,  ..., -0.0191,  0.0590,  0.0147],\n",
       "                      [-0.0290,  0.0002, -0.0252,  ..., -0.0242, -0.0015, -0.0408]])),\n",
       "             ('encoder.layer.13.output.dense.bias',\n",
       "              tensor([-0.1069,  0.0573, -0.0724,  ..., -0.0240, -0.0501, -0.0879])),\n",
       "             ('encoder.layer.13.output.LayerNorm.weight',\n",
       "              tensor([0.9845, 0.9911, 1.0013,  ..., 0.9877, 0.9873, 0.9934])),\n",
       "             ('encoder.layer.13.output.LayerNorm.bias',\n",
       "              tensor([ 0.0680, -0.1173,  0.2386,  ..., -0.0539, -0.0039, -0.0447])),\n",
       "             ('encoder.layer.14.attention.self.query.weight',\n",
       "              tensor([[-0.0438, -0.0600,  0.0088,  ..., -0.1198, -0.0220, -0.1596],\n",
       "                      [-0.0610,  0.0297, -0.0964,  ...,  0.0370, -0.0195, -0.0380],\n",
       "                      [-0.0037, -0.0291,  0.1076,  ..., -0.0105, -0.0602,  0.0382],\n",
       "                      ...,\n",
       "                      [ 0.0627,  0.0002,  0.0214,  ..., -0.0383, -0.0672,  0.0401],\n",
       "                      [-0.0182,  0.0390, -0.0182,  ..., -0.0932, -0.0235,  0.0570],\n",
       "                      [-0.0666,  0.0228,  0.0085,  ...,  0.0536,  0.0323,  0.0383]])),\n",
       "             ('encoder.layer.14.attention.self.query.bias',\n",
       "              tensor([ 0.1926,  0.1405, -0.1237,  ...,  0.2336, -0.1188,  0.0179])),\n",
       "             ('encoder.layer.14.attention.self.key.weight',\n",
       "              tensor([[ 0.0354,  0.0812,  0.0715,  ...,  0.0088,  0.0041, -0.0102],\n",
       "                      [-0.0657, -0.0068, -0.1240,  ..., -0.0144,  0.0064,  0.0471],\n",
       "                      [-0.0233,  0.0264,  0.0475,  ...,  0.0193, -0.1161,  0.0621],\n",
       "                      ...,\n",
       "                      [ 0.0785, -0.0641,  0.0662,  ..., -0.0959, -0.0448, -0.0301],\n",
       "                      [ 0.0468,  0.0876,  0.0444,  ..., -0.0765,  0.0650,  0.0386],\n",
       "                      [-0.0681, -0.0044,  0.0570,  ..., -0.0035, -0.0492, -0.0236]])),\n",
       "             ('encoder.layer.14.attention.self.key.bias',\n",
       "              tensor([ 1.7745e-04,  3.3622e-05, -9.8875e-05,  ..., -9.8425e-05,\n",
       "                      -1.3332e-05,  8.5422e-05])),\n",
       "             ('encoder.layer.14.attention.self.value.weight',\n",
       "              tensor([[-0.0435, -0.0002,  0.0427,  ..., -0.0439, -0.0056, -0.0078],\n",
       "                      [-0.0029, -0.0686, -0.0085,  ...,  0.0625,  0.0636, -0.0514],\n",
       "                      [ 0.0255, -0.0353, -0.0249,  ..., -0.0668,  0.0140,  0.0226],\n",
       "                      ...,\n",
       "                      [ 0.0813,  0.0259, -0.0174,  ...,  0.0201, -0.0263, -0.0331],\n",
       "                      [ 0.0706,  0.0156, -0.0130,  ..., -0.0045, -0.0207,  0.0283],\n",
       "                      [-0.0754, -0.0401, -0.0234,  ..., -0.0038,  0.0483, -0.1304]])),\n",
       "             ('encoder.layer.14.attention.self.value.bias',\n",
       "              tensor([-1.8636e-04,  6.4229e-03, -1.1856e-02,  ..., -9.3933e-05,\n",
       "                       4.8655e-03,  1.3101e-02])),\n",
       "             ('encoder.layer.14.attention.output.dense.weight',\n",
       "              tensor([[-0.0063, -0.0223,  0.0441,  ..., -0.0431, -0.0003,  0.0841],\n",
       "                      [ 0.0137, -0.0438,  0.0450,  ..., -0.0730, -0.0461, -0.0052],\n",
       "                      [ 0.0368, -0.0349,  0.0114,  ...,  0.0031,  0.0143, -0.0201],\n",
       "                      ...,\n",
       "                      [ 0.0680, -0.0351,  0.0682,  ...,  0.0098, -0.0068, -0.0115],\n",
       "                      [-0.0016, -0.0647, -0.0193,  ...,  0.0277,  0.0057, -0.0075],\n",
       "                      [-0.0081, -0.0274,  0.0244,  ...,  0.0664,  0.0236,  0.0721]])),\n",
       "             ('encoder.layer.14.attention.output.dense.bias',\n",
       "              tensor([-0.0228,  0.0022,  0.0320,  ...,  0.0642, -0.0416, -0.0051])),\n",
       "             ('encoder.layer.14.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9958, 0.9826, 1.0043,  ..., 0.9881, 0.9890, 0.9878])),\n",
       "             ('encoder.layer.14.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0865, -0.0146, -0.1970,  ..., -0.0640, -0.1382, -0.0532])),\n",
       "             ('encoder.layer.14.intermediate.dense.weight',\n",
       "              tensor([[-0.0062, -0.0017,  0.0399,  ...,  0.0019, -0.0156,  0.0422],\n",
       "                      [ 0.0177, -0.0497, -0.0078,  ...,  0.0602, -0.0282,  0.0459],\n",
       "                      [ 0.0444, -0.0673,  0.0126,  ...,  0.0684,  0.0689, -0.0569],\n",
       "                      ...,\n",
       "                      [ 0.0702, -0.0376,  0.0104,  ...,  0.0091, -0.0033, -0.0320],\n",
       "                      [-0.0341, -0.0549, -0.0129,  ...,  0.0301,  0.0117,  0.0191],\n",
       "                      [ 0.0388, -0.0448, -0.0671,  ...,  0.0358, -0.0182,  0.0064]])),\n",
       "             ('encoder.layer.14.intermediate.dense.bias',\n",
       "              tensor([-0.1082, -0.1096, -0.1120,  ..., -0.0575, -0.0797, -0.0815])),\n",
       "             ('encoder.layer.14.output.dense.weight',\n",
       "              tensor([[-0.0209,  0.0290,  0.0093,  ...,  0.0281,  0.0090, -0.0643],\n",
       "                      [ 0.0034, -0.0118, -0.0870,  ..., -0.0076,  0.0156, -0.0652],\n",
       "                      [-0.0097, -0.0201,  0.0360,  ...,  0.0081,  0.0035,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0569,  0.0065,  0.0096,  ..., -0.0441,  0.0354, -0.0309],\n",
       "                      [-0.0528,  0.0137,  0.0266,  ...,  0.0455,  0.0029, -0.0318],\n",
       "                      [ 0.0253,  0.0096, -0.0587,  ..., -0.0055, -0.0170, -0.0694]])),\n",
       "             ('encoder.layer.14.output.dense.bias',\n",
       "              tensor([-0.1092,  0.0495, -0.1954,  ..., -0.0113, -0.1290, -0.0694])),\n",
       "             ('encoder.layer.14.output.LayerNorm.weight',\n",
       "              tensor([0.9904, 0.9931, 1.0041,  ..., 0.9876, 0.9896, 0.9908])),\n",
       "             ('encoder.layer.14.output.LayerNorm.bias',\n",
       "              tensor([-0.0244, -0.1100,  0.2479,  ..., -0.0517, -0.0146, -0.0590])),\n",
       "             ('encoder.layer.15.attention.self.query.weight',\n",
       "              tensor([[ 0.0617,  0.0563, -0.1336,  ..., -0.0207,  0.0422,  0.0241],\n",
       "                      [-0.0202,  0.0151,  0.0826,  ...,  0.0882, -0.0288, -0.0180],\n",
       "                      [ 0.0011, -0.0176, -0.0278,  ..., -0.0588,  0.0470, -0.0780],\n",
       "                      ...,\n",
       "                      [ 0.0489,  0.1174, -0.0266,  ...,  0.0266, -0.0197,  0.0100],\n",
       "                      [-0.0200,  0.0876,  0.0352,  ..., -0.0003, -0.0585,  0.0410],\n",
       "                      [-0.0621, -0.0320,  0.0095,  ..., -0.0085,  0.0942, -0.0555]])),\n",
       "             ('encoder.layer.15.attention.self.query.bias',\n",
       "              tensor([-0.0131, -0.0734, -0.0256,  ..., -0.1173, -0.0899, -0.0032])),\n",
       "             ('encoder.layer.15.attention.self.key.weight',\n",
       "              tensor([[-0.0441,  0.0203, -0.2187,  ..., -0.0846, -0.0094, -0.0125],\n",
       "                      [ 0.0641, -0.0226, -0.0543,  ..., -0.0110,  0.0022, -0.0230],\n",
       "                      [-0.0348,  0.0823, -0.0763,  ..., -0.0361, -0.0601, -0.0165],\n",
       "                      ...,\n",
       "                      [ 0.0132,  0.0201, -0.0542,  ..., -0.1059,  0.0357, -0.0161],\n",
       "                      [-0.0950, -0.0169, -0.0422,  ..., -0.0057,  0.0132,  0.0274],\n",
       "                      [ 0.0547, -0.0387,  0.0258,  ..., -0.0329,  0.0467,  0.0558]])),\n",
       "             ('encoder.layer.15.attention.self.key.bias',\n",
       "              tensor([ 1.5233e-04,  2.7773e-05,  7.6974e-05,  ...,  4.1702e-04,\n",
       "                       1.2912e-04, -2.3172e-04])),\n",
       "             ('encoder.layer.15.attention.self.value.weight',\n",
       "              tensor([[-0.0711,  0.0465,  0.0061,  ...,  0.0015, -0.0216,  0.0621],\n",
       "                      [-0.0146, -0.0309, -0.0326,  ..., -0.0390,  0.0402, -0.0699],\n",
       "                      [-0.0020, -0.0084,  0.0030,  ..., -0.1066,  0.1022,  0.0991],\n",
       "                      ...,\n",
       "                      [ 0.0156, -0.0585, -0.0004,  ...,  0.0369, -0.0101,  0.0754],\n",
       "                      [-0.0448, -0.0865,  0.0038,  ...,  0.0589,  0.0172,  0.0123],\n",
       "                      [-0.0618,  0.0023, -0.0130,  ..., -0.0709, -0.1061,  0.0040]])),\n",
       "             ('encoder.layer.15.attention.self.value.bias',\n",
       "              tensor([-0.0045,  0.0053,  0.0009,  ...,  0.0032, -0.0045, -0.0213])),\n",
       "             ('encoder.layer.15.attention.output.dense.weight',\n",
       "              tensor([[ 0.0133,  0.0009, -0.0208,  ..., -0.0330, -0.0272,  0.0218],\n",
       "                      [ 0.0045,  0.0267, -0.0367,  ..., -0.0060,  0.0478, -0.0228],\n",
       "                      [-0.0306, -0.0352,  0.0722,  ..., -0.0162, -0.0180, -0.0119],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0572,  0.0139,  ..., -0.0333, -0.0101,  0.0245],\n",
       "                      [-0.0596,  0.0948,  0.0487,  ..., -0.0078,  0.0181,  0.0865],\n",
       "                      [ 0.0203, -0.0543,  0.0086,  ..., -0.0126, -0.0361, -0.0191]])),\n",
       "             ('encoder.layer.15.attention.output.dense.bias',\n",
       "              tensor([-0.0167,  0.0165,  0.0541,  ...,  0.0020, -0.0479, -0.0286])),\n",
       "             ('encoder.layer.15.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9885, 0.9894, 1.0038,  ..., 0.9884, 0.9899, 0.9828])),\n",
       "             ('encoder.layer.15.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1112, -0.0180, -0.1718,  ..., -0.0929, -0.0808, -0.1244])),\n",
       "             ('encoder.layer.15.intermediate.dense.weight',\n",
       "              tensor([[-0.0218, -0.0163,  0.0210,  ...,  0.0116,  0.0502, -0.0317],\n",
       "                      [ 0.0520, -0.0565,  0.0031,  ..., -0.0221, -0.0460, -0.0101],\n",
       "                      [-0.0308,  0.0470,  0.0210,  ...,  0.0958,  0.0077,  0.0269],\n",
       "                      ...,\n",
       "                      [-0.0417, -0.0388,  0.0208,  ...,  0.0816,  0.0411,  0.0280],\n",
       "                      [-0.0383,  0.0676,  0.0558,  ..., -0.0086,  0.0230, -0.0090],\n",
       "                      [ 0.0705,  0.0072,  0.0057,  ...,  0.0411,  0.0713,  0.0266]])),\n",
       "             ('encoder.layer.15.intermediate.dense.bias',\n",
       "              tensor([-0.0440, -0.1065, -0.0772,  ..., -0.0341,  0.0109, -0.0829])),\n",
       "             ('encoder.layer.15.output.dense.weight',\n",
       "              tensor([[-0.0264,  0.0648, -0.0489,  ..., -0.0006,  0.0184, -0.0060],\n",
       "                      [-0.0219,  0.0146, -0.0323,  ..., -0.0555, -0.0032, -0.0031],\n",
       "                      [-0.0162, -0.0105, -0.0241,  ...,  0.0004,  0.0150,  0.0126],\n",
       "                      ...,\n",
       "                      [ 0.0063,  0.0219,  0.0515,  ..., -0.0098,  0.0046,  0.0018],\n",
       "                      [-0.0026, -0.0076,  0.0690,  ..., -0.0408, -0.0170,  0.0036],\n",
       "                      [-0.0448,  0.0172,  0.0225,  ...,  0.0345, -0.0116,  0.0392]])),\n",
       "             ('encoder.layer.15.output.dense.bias',\n",
       "              tensor([-0.1301,  0.0302, -0.2483,  ..., -0.0317, -0.0860, -0.1000])),\n",
       "             ('encoder.layer.15.output.LayerNorm.weight',\n",
       "              tensor([0.9877, 0.9858, 1.0038,  ..., 0.9846, 0.9953, 0.9868])),\n",
       "             ('encoder.layer.15.output.LayerNorm.bias',\n",
       "              tensor([-0.0082, -0.0686,  0.1723,  ..., -0.0151, -0.0310, -0.0166])),\n",
       "             ('encoder.layer.16.attention.self.query.weight',\n",
       "              tensor([[-0.0204,  0.0023, -0.0436,  ..., -0.0114,  0.0906,  0.0336],\n",
       "                      [-0.0461, -0.0661, -0.0252,  ...,  0.0126, -0.0212, -0.0356],\n",
       "                      [ 0.0013, -0.0064,  0.0823,  ..., -0.0680, -0.0597, -0.0090],\n",
       "                      ...,\n",
       "                      [ 0.0688, -0.0830, -0.0291,  ..., -0.0165,  0.0369,  0.0193],\n",
       "                      [-0.0475,  0.0384,  0.0536,  ..., -0.0852,  0.0353, -0.1119],\n",
       "                      [ 0.0105,  0.0120,  0.0356,  ..., -0.0339, -0.0836, -0.0393]])),\n",
       "             ('encoder.layer.16.attention.self.query.bias',\n",
       "              tensor([-0.0076, -0.0680,  0.0123,  ..., -0.0046,  0.0847,  0.0128])),\n",
       "             ('encoder.layer.16.attention.self.key.weight',\n",
       "              tensor([[-0.0527, -0.0156, -0.0312,  ...,  0.0794, -0.0119, -0.0525],\n",
       "                      [ 0.0490, -0.0127, -0.0323,  ...,  0.0775, -0.0442, -0.0281],\n",
       "                      [-0.0165,  0.0708,  0.0789,  ..., -0.0335, -0.0621,  0.0312],\n",
       "                      ...,\n",
       "                      [ 0.0494,  0.0349,  0.0055,  ..., -0.0747, -0.0122, -0.0040],\n",
       "                      [ 0.0028,  0.0838,  0.0179,  ..., -0.0255,  0.0575,  0.0074],\n",
       "                      [ 0.0496, -0.0210,  0.0759,  ..., -0.0883, -0.0320,  0.0472]])),\n",
       "             ('encoder.layer.16.attention.self.key.bias',\n",
       "              tensor([-0.0001, -0.0002,  0.0003,  ..., -0.0002, -0.0003,  0.0002])),\n",
       "             ('encoder.layer.16.attention.self.value.weight',\n",
       "              tensor([[ 0.0419, -0.0023,  0.0111,  ...,  0.0373, -0.0608,  0.0520],\n",
       "                      [ 0.0271,  0.0591, -0.0338,  ...,  0.0030,  0.0230,  0.0865],\n",
       "                      [ 0.0111,  0.0025, -0.0086,  ..., -0.0412, -0.0537,  0.0276],\n",
       "                      ...,\n",
       "                      [-0.0563, -0.1265,  0.0250,  ..., -0.0902, -0.0079,  0.0554],\n",
       "                      [-0.0042, -0.0321, -0.0331,  ..., -0.0271,  0.0049,  0.1260],\n",
       "                      [ 0.0387,  0.1237, -0.0008,  ..., -0.0191,  0.0105, -0.0255]])),\n",
       "             ('encoder.layer.16.attention.self.value.bias',\n",
       "              tensor([-0.0093,  0.0055,  0.0025,  ..., -0.0104, -0.0152, -0.0094])),\n",
       "             ('encoder.layer.16.attention.output.dense.weight',\n",
       "              tensor([[ 0.0402, -0.0427, -0.0111,  ...,  0.0041, -0.0165,  0.0073],\n",
       "                      [ 0.0011,  0.0204,  0.0123,  ...,  0.0294, -0.0622, -0.0199],\n",
       "                      [ 0.0552, -0.0709,  0.0198,  ..., -0.0107, -0.0297,  0.0325],\n",
       "                      ...,\n",
       "                      [ 0.0175,  0.0092, -0.0593,  ...,  0.0848,  0.0355,  0.0053],\n",
       "                      [-0.0512, -0.0361, -0.0231,  ...,  0.0206, -0.0749,  0.0293],\n",
       "                      [ 0.0033, -0.0116, -0.0055,  ..., -0.0686, -0.0621, -0.0393]])),\n",
       "             ('encoder.layer.16.attention.output.dense.bias',\n",
       "              tensor([-0.0021, -0.0225,  0.0822,  ..., -0.0595,  0.0317, -0.0674])),\n",
       "             ('encoder.layer.16.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9976, 0.9940, 1.0064,  ..., 0.9905, 0.9949, 0.9969])),\n",
       "             ('encoder.layer.16.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0789, -0.0437, -0.2108,  ..., -0.0623, -0.0674, -0.0665])),\n",
       "             ('encoder.layer.16.intermediate.dense.weight',\n",
       "              tensor([[ 0.0756, -0.0263,  0.0461,  ...,  0.0417,  0.0108, -0.0325],\n",
       "                      [ 0.0155,  0.0435,  0.0222,  ...,  0.0643, -0.0722,  0.0036],\n",
       "                      [ 0.0160,  0.0160, -0.0133,  ...,  0.1243, -0.0085, -0.0134],\n",
       "                      ...,\n",
       "                      [-0.0786, -0.0622,  0.0156,  ...,  0.0250,  0.0288, -0.0213],\n",
       "                      [-0.0354, -0.0204,  0.0431,  ...,  0.0098, -0.0467, -0.0466],\n",
       "                      [-0.0138, -0.0896,  0.0388,  ...,  0.0267, -0.0495,  0.0317]])),\n",
       "             ('encoder.layer.16.intermediate.dense.bias',\n",
       "              tensor([-0.0782, -0.0588, -0.0995,  ..., -0.0640, -0.0788, -0.0872])),\n",
       "             ('encoder.layer.16.output.dense.weight',\n",
       "              tensor([[ 0.0114,  0.0332,  0.0653,  ..., -0.0164, -0.0009,  0.0054],\n",
       "                      [-0.0133, -0.0045,  0.0124,  ..., -0.0371, -0.0673, -0.0248],\n",
       "                      [ 0.0062, -0.0016, -0.0168,  ...,  0.0015,  0.0006,  0.0058],\n",
       "                      ...,\n",
       "                      [-0.0156,  0.0407,  0.0053,  ...,  0.0032, -0.0348, -0.0570],\n",
       "                      [-0.0555, -0.0401,  0.0438,  ..., -0.0107, -0.0307,  0.0133],\n",
       "                      [-0.0043,  0.0192, -0.0094,  ..., -0.0367,  0.0091,  0.0351]])),\n",
       "             ('encoder.layer.16.output.dense.bias',\n",
       "              tensor([-0.0819,  0.0128, -0.1999,  ..., -0.0392, -0.0636, -0.1037])),\n",
       "             ('encoder.layer.16.output.LayerNorm.weight',\n",
       "              tensor([0.9784, 0.9968, 1.0046,  ..., 0.9740, 0.9908, 0.9872])),\n",
       "             ('encoder.layer.16.output.LayerNorm.bias',\n",
       "              tensor([-0.0199, -0.0482,  0.1786,  ..., -0.0248, -0.0305, -0.0301])),\n",
       "             ('encoder.layer.17.attention.self.query.weight',\n",
       "              tensor([[-0.0022, -0.0146,  0.0625,  ...,  0.0394, -0.0251,  0.0590],\n",
       "                      [ 0.0263,  0.0110, -0.0261,  ...,  0.0832, -0.0306,  0.0504],\n",
       "                      [ 0.1409,  0.0714,  0.0324,  ..., -0.0882, -0.0373, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0278, -0.0437,  0.0742,  ..., -0.1217,  0.0545, -0.0729],\n",
       "                      [ 0.0452, -0.0079,  0.0468,  ..., -0.0054, -0.0439,  0.0295],\n",
       "                      [ 0.0133,  0.0949, -0.0286,  ...,  0.0012,  0.0133, -0.0219]])),\n",
       "             ('encoder.layer.17.attention.self.query.bias',\n",
       "              tensor([-0.1766,  0.0578,  0.0232,  ..., -0.0477,  0.0387,  0.0136])),\n",
       "             ('encoder.layer.17.attention.self.key.weight',\n",
       "              tensor([[ 0.0301, -0.0105,  0.0526,  ..., -0.0610,  0.0422,  0.0253],\n",
       "                      [ 0.0464,  0.0134,  0.0360,  ...,  0.0913, -0.0415,  0.0215],\n",
       "                      [ 0.0167,  0.0345, -0.0336,  ..., -0.0644,  0.0814, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0144,  0.0621,  0.0250,  ...,  0.0216,  0.0426, -0.0742],\n",
       "                      [ 0.0336,  0.0546,  0.0500,  ...,  0.0521,  0.0182,  0.0201],\n",
       "                      [-0.0834,  0.0058, -0.0235,  ...,  0.0526,  0.0060,  0.0626]])),\n",
       "             ('encoder.layer.17.attention.self.key.bias',\n",
       "              tensor([ 2.4738e-03, -2.5235e-05,  1.6445e-04,  ..., -2.6413e-05,\n",
       "                       3.8383e-04,  2.0976e-04])),\n",
       "             ('encoder.layer.17.attention.self.value.weight',\n",
       "              tensor([[-0.0157,  0.0035, -0.0084,  ...,  0.0108,  0.0063,  0.0009],\n",
       "                      [-0.0189,  0.0214, -0.0105,  ...,  0.0270,  0.0158,  0.0280],\n",
       "                      [ 0.0113,  0.0094, -0.0098,  ...,  0.0198, -0.0041, -0.0738],\n",
       "                      ...,\n",
       "                      [-0.0248,  0.0320,  0.0041,  ..., -0.0293, -0.0850, -0.1101],\n",
       "                      [ 0.0186, -0.0963, -0.0295,  ...,  0.0067, -0.0301,  0.0967],\n",
       "                      [ 0.0616, -0.0647,  0.0104,  ..., -0.1127,  0.0494, -0.0439]])),\n",
       "             ('encoder.layer.17.attention.self.value.bias',\n",
       "              tensor([ 0.0083, -0.0016, -0.0004,  ...,  0.0019,  0.0086,  0.0056])),\n",
       "             ('encoder.layer.17.attention.output.dense.weight',\n",
       "              tensor([[ 0.0003, -0.0254, -0.0185,  ..., -0.0218, -0.0325,  0.0543],\n",
       "                      [ 0.0271,  0.0068,  0.0112,  ...,  0.0719, -0.0464,  0.0628],\n",
       "                      [ 0.0023,  0.0093, -0.0052,  ...,  0.0142, -0.0330, -0.0084],\n",
       "                      ...,\n",
       "                      [ 0.0324,  0.0114, -0.0411,  ...,  0.0283,  0.0311,  0.0224],\n",
       "                      [-0.0273,  0.0283, -0.0084,  ..., -0.0440,  0.0614, -0.0333],\n",
       "                      [ 0.0226, -0.0197, -0.0574,  ..., -0.0031, -0.0274,  0.0176]])),\n",
       "             ('encoder.layer.17.attention.output.dense.bias',\n",
       "              tensor([-0.0234,  0.0374,  0.0307,  ..., -0.0084, -0.0488, -0.0181])),\n",
       "             ('encoder.layer.17.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9868, 0.9978, 1.0033,  ..., 0.9845, 0.9842, 0.9849])),\n",
       "             ('encoder.layer.17.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0958, -0.0629, -0.2176,  ..., -0.0822, -0.0671, -0.0850])),\n",
       "             ('encoder.layer.17.intermediate.dense.weight',\n",
       "              tensor([[ 0.0071, -0.0425,  0.0238,  ..., -0.0530, -0.0390, -0.0199],\n",
       "                      [ 0.0850, -0.0291,  0.0202,  ...,  0.0225, -0.0607, -0.0070],\n",
       "                      [-0.0065,  0.0131,  0.0106,  ...,  0.0236,  0.0310,  0.0183],\n",
       "                      ...,\n",
       "                      [ 0.0740, -0.0602, -0.0357,  ..., -0.0064,  0.0324,  0.0041],\n",
       "                      [-0.0064,  0.0967,  0.0127,  ..., -0.0495, -0.0651,  0.0131],\n",
       "                      [-0.0510, -0.0223, -0.0036,  ..., -0.0256,  0.0020,  0.0249]])),\n",
       "             ('encoder.layer.17.intermediate.dense.bias',\n",
       "              tensor([-0.0438, -0.0904, -0.0249,  ...,  0.0195, -0.1116, -0.0451])),\n",
       "             ('encoder.layer.17.output.dense.weight',\n",
       "              tensor([[ 0.0265,  0.0684, -0.0136,  ...,  0.0040, -0.0244, -0.0589],\n",
       "                      [ 0.0073, -0.0053,  0.0203,  ...,  0.0046,  0.0037, -0.0066],\n",
       "                      [-0.0197, -0.0246, -0.0002,  ...,  0.0128,  0.0307, -0.0166],\n",
       "                      ...,\n",
       "                      [ 0.0145,  0.0978, -0.0142,  ...,  0.0163, -0.0339,  0.0019],\n",
       "                      [ 0.0088,  0.0091, -0.0236,  ...,  0.0172,  0.0401, -0.0001],\n",
       "                      [-0.0217, -0.0355, -0.0162,  ...,  0.0091, -0.0202,  0.0149]])),\n",
       "             ('encoder.layer.17.output.dense.bias',\n",
       "              tensor([-0.0694,  0.0156, -0.1519,  ..., -0.0156, -0.0876, -0.0513])),\n",
       "             ('encoder.layer.17.output.LayerNorm.weight',\n",
       "              tensor([0.9854, 0.9918, 1.0019,  ..., 0.9815, 0.9864, 0.9851])),\n",
       "             ('encoder.layer.17.output.LayerNorm.bias',\n",
       "              tensor([-0.0040, -0.0270,  0.1226,  ..., -0.0129, -0.0369, -0.0077])),\n",
       "             ('encoder.layer.18.attention.self.query.weight',\n",
       "              tensor([[ 0.0309, -0.0019,  0.1379,  ...,  0.0221,  0.0678, -0.0150],\n",
       "                      [ 0.0327,  0.0206,  0.0123,  ..., -0.1441, -0.0500, -0.0419],\n",
       "                      [-0.0531,  0.0581,  0.1865,  ...,  0.0396, -0.0252, -0.0238],\n",
       "                      ...,\n",
       "                      [-0.0512, -0.0031, -0.0275,  ...,  0.0214, -0.0194, -0.0312],\n",
       "                      [-0.0635,  0.0321, -0.0130,  ...,  0.0161,  0.0527,  0.1093],\n",
       "                      [-0.0177,  0.0415,  0.1422,  ..., -0.0316,  0.0135, -0.0522]])),\n",
       "             ('encoder.layer.18.attention.self.query.bias',\n",
       "              tensor([-0.2750, -0.0117, -0.1435,  ...,  0.0094, -0.0061,  0.0209])),\n",
       "             ('encoder.layer.18.attention.self.key.weight',\n",
       "              tensor([[-0.0605, -0.0170,  0.0506,  ...,  0.0111, -0.0075,  0.0451],\n",
       "                      [-0.0252,  0.0815,  0.1123,  ..., -0.0169, -0.0040, -0.0599],\n",
       "                      [ 0.0677,  0.0492,  0.2560,  ..., -0.0259,  0.0064,  0.0099],\n",
       "                      ...,\n",
       "                      [-0.0535, -0.0309, -0.0140,  ..., -0.0364,  0.0647,  0.0242],\n",
       "                      [ 0.0623, -0.0156, -0.0197,  ..., -0.0184, -0.0430, -0.0032],\n",
       "                      [-0.0191, -0.0163,  0.1445,  ...,  0.0354, -0.0145,  0.0790]])),\n",
       "             ('encoder.layer.18.attention.self.key.bias',\n",
       "              tensor([-7.8834e-04,  9.0233e-05, -1.8489e-04,  ..., -8.9032e-05,\n",
       "                      -1.2732e-06,  1.6719e-04])),\n",
       "             ('encoder.layer.18.attention.self.value.weight',\n",
       "              tensor([[ 0.0253,  0.0204, -0.0017,  ...,  0.0125,  0.0012,  0.0071],\n",
       "                      [-0.0345,  0.0606,  0.0166,  ...,  0.0098,  0.0021,  0.0243],\n",
       "                      [ 0.0110, -0.0398, -0.0329,  ...,  0.1036, -0.0275,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0119, -0.0052,  0.0175,  ...,  0.0244, -0.0265, -0.0345],\n",
       "                      [ 0.0370, -0.0035, -0.0432,  ...,  0.0087,  0.0322,  0.0018],\n",
       "                      [ 0.0257,  0.0676, -0.0172,  ..., -0.0437,  0.0174,  0.0033]])),\n",
       "             ('encoder.layer.18.attention.self.value.bias',\n",
       "              tensor([-0.0072,  0.0052, -0.0172,  ..., -0.0066,  0.0027, -0.0110])),\n",
       "             ('encoder.layer.18.attention.output.dense.weight',\n",
       "              tensor([[-0.0366, -0.0225, -0.0615,  ..., -0.0257, -0.0084, -0.0366],\n",
       "                      [-0.0211,  0.0704, -0.0521,  ..., -0.0006, -0.0230,  0.0150],\n",
       "                      [-0.0385,  0.0182, -0.0072,  ...,  0.0099, -0.0239, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0334,  0.0093,  0.0713,  ..., -0.0064,  0.0258,  0.0191],\n",
       "                      [-0.0050,  0.0211,  0.0230,  ..., -0.0166,  0.0012,  0.0125],\n",
       "                      [ 0.0006,  0.0091, -0.0015,  ..., -0.0008,  0.0269,  0.0224]])),\n",
       "             ('encoder.layer.18.attention.output.dense.bias',\n",
       "              tensor([-0.0155,  0.0795,  0.0295,  ...,  0.0716,  0.0374,  0.0005])),\n",
       "             ('encoder.layer.18.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9845, 0.9865, 1.0049,  ..., 0.9897, 0.9781, 0.9881])),\n",
       "             ('encoder.layer.18.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0960, -0.0229, -0.2538,  ..., -0.0723, -0.0621, -0.0682])),\n",
       "             ('encoder.layer.18.intermediate.dense.weight',\n",
       "              tensor([[ 0.0525, -0.0234,  0.0327,  ...,  0.0262,  0.0050, -0.0263],\n",
       "                      [ 0.0635, -0.0407,  0.0076,  ..., -0.0134,  0.0013, -0.0488],\n",
       "                      [ 0.0184, -0.0304, -0.0299,  ..., -0.0374,  0.0100, -0.0246],\n",
       "                      ...,\n",
       "                      [ 0.0171,  0.0613,  0.0413,  ...,  0.0292,  0.0306,  0.0035],\n",
       "                      [-0.0306, -0.0536,  0.0042,  ...,  0.0368, -0.0271,  0.0064],\n",
       "                      [ 0.0405,  0.0548, -0.0642,  ...,  0.0058,  0.0103, -0.0025]])),\n",
       "             ('encoder.layer.18.intermediate.dense.bias',\n",
       "              tensor([-0.0721, -0.1020, -0.0049,  ..., -0.0470, -0.0975, -0.0812])),\n",
       "             ('encoder.layer.18.output.dense.weight',\n",
       "              tensor([[-0.0398,  0.0015,  0.0177,  ...,  0.0199, -0.0293, -0.0347],\n",
       "                      [-0.0400, -0.0371, -0.0216,  ...,  0.0433, -0.0299, -0.0076],\n",
       "                      [ 0.0147, -0.0054, -0.0082,  ..., -0.0107, -0.0049, -0.0165],\n",
       "                      ...,\n",
       "                      [ 0.0179,  0.0183,  0.0399,  ..., -0.0041, -0.0016, -0.0113],\n",
       "                      [-0.0109, -0.0131, -0.0013,  ..., -0.0040,  0.0500,  0.0120],\n",
       "                      [-0.0305, -0.0141,  0.0424,  ...,  0.0025, -0.0570, -0.0229]])),\n",
       "             ('encoder.layer.18.output.dense.bias',\n",
       "              tensor([-0.0766,  0.0008, -0.1593,  ...,  0.0430, -0.0415, -0.0421])),\n",
       "             ('encoder.layer.18.output.LayerNorm.weight',\n",
       "              tensor([0.9831, 0.9986, 1.0053,  ..., 0.9813, 0.9873, 0.9801])),\n",
       "             ('encoder.layer.18.output.LayerNorm.bias',\n",
       "              tensor([-0.0074, -0.0487,  0.1216,  ..., -0.0182, -0.0267, -0.0290])),\n",
       "             ('encoder.layer.19.attention.self.query.weight',\n",
       "              tensor([[-0.0498, -0.0254, -0.0153,  ..., -0.0722,  0.0582, -0.1396],\n",
       "                      [ 0.0341,  0.0152, -0.1323,  ...,  0.0325,  0.0452,  0.0745],\n",
       "                      [-0.0242, -0.0160,  0.0441,  ..., -0.0635, -0.0771,  0.0160],\n",
       "                      ...,\n",
       "                      [-0.0045, -0.0127, -0.1966,  ...,  0.0897, -0.0094, -0.0331],\n",
       "                      [-0.1259,  0.0580,  0.0537,  ..., -0.0115,  0.0596, -0.0865],\n",
       "                      [-0.0274,  0.0566,  0.0614,  ...,  0.1000,  0.0644, -0.0153]])),\n",
       "             ('encoder.layer.19.attention.self.query.bias',\n",
       "              tensor([-0.0137, -0.0612, -0.0111,  ..., -0.0233,  0.0387, -0.0219])),\n",
       "             ('encoder.layer.19.attention.self.key.weight',\n",
       "              tensor([[-0.0221,  0.0091, -0.0019,  ...,  0.0385, -0.0859, -0.0075],\n",
       "                      [ 0.0620,  0.0581, -0.0962,  ...,  0.0608, -0.0432,  0.0594],\n",
       "                      [-0.0529, -0.0154,  0.0384,  ...,  0.0245,  0.1103, -0.0490],\n",
       "                      ...,\n",
       "                      [-0.0791, -0.0215, -0.2597,  ...,  0.0126,  0.0651,  0.0314],\n",
       "                      [-0.0156,  0.0065,  0.1630,  ...,  0.0238,  0.0867,  0.0154],\n",
       "                      [ 0.0141, -0.0749,  0.1267,  ..., -0.0479, -0.0164,  0.0720]])),\n",
       "             ('encoder.layer.19.attention.self.key.bias',\n",
       "              tensor([-4.1641e-05, -1.2430e-04,  6.9864e-05,  ..., -7.7097e-05,\n",
       "                       1.4655e-03, -9.0868e-05])),\n",
       "             ('encoder.layer.19.attention.self.value.weight',\n",
       "              tensor([[ 0.0258,  0.0146,  0.0308,  ...,  0.0291,  0.0189,  0.0254],\n",
       "                      [-0.0119,  0.0079, -0.0033,  ...,  0.0393,  0.0281, -0.0224],\n",
       "                      [-0.0374, -0.0096,  0.0155,  ...,  0.0319, -0.0213, -0.0328],\n",
       "                      ...,\n",
       "                      [ 0.0147,  0.0171,  0.0115,  ..., -0.0215, -0.0124, -0.0180],\n",
       "                      [ 0.0267,  0.0259, -0.0371,  ...,  0.0271, -0.0203, -0.0452],\n",
       "                      [-0.0200,  0.0597, -0.0245,  ...,  0.0357,  0.0259, -0.0065]])),\n",
       "             ('encoder.layer.19.attention.self.value.bias',\n",
       "              tensor([ 3.2538e-03,  6.8839e-02,  2.0269e-04,  ..., -4.0571e-03,\n",
       "                      -8.7237e-06, -1.7165e-02])),\n",
       "             ('encoder.layer.19.attention.output.dense.weight',\n",
       "              tensor([[-0.0017,  0.0262, -0.0209,  ...,  0.0248,  0.0142, -0.0042],\n",
       "                      [ 0.0057, -0.0311, -0.0449,  ..., -0.0045,  0.0384,  0.0033],\n",
       "                      [ 0.0021, -0.0474, -0.0277,  ...,  0.0389, -0.0243,  0.0148],\n",
       "                      ...,\n",
       "                      [-0.0613, -0.0599, -0.0305,  ...,  0.0018, -0.0016,  0.0149],\n",
       "                      [-0.0201,  0.0448,  0.0053,  ...,  0.0130,  0.0195, -0.0075],\n",
       "                      [-0.0238, -0.0372,  0.0063,  ..., -0.0080, -0.0271, -0.0450]])),\n",
       "             ('encoder.layer.19.attention.output.dense.bias',\n",
       "              tensor([ 0.0035,  0.0605,  0.0793,  ..., -0.0008,  0.0255,  0.0066])),\n",
       "             ('encoder.layer.19.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9786, 0.9886, 1.0068,  ..., 0.9742, 0.9873, 0.9918])),\n",
       "             ('encoder.layer.19.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0810, -0.0380, -0.2175,  ..., -0.0632, -0.0832, -0.0628])),\n",
       "             ('encoder.layer.19.intermediate.dense.weight',\n",
       "              tensor([[ 0.0500, -0.0391,  0.0131,  ...,  0.0854,  0.0766,  0.1538],\n",
       "                      [ 0.0511, -0.0091,  0.0529,  ...,  0.0209,  0.0282,  0.0247],\n",
       "                      [ 0.0205,  0.0150,  0.0367,  ..., -0.0170, -0.0046,  0.0041],\n",
       "                      ...,\n",
       "                      [ 0.0783, -0.0223,  0.0237,  ..., -0.0042, -0.0509,  0.0399],\n",
       "                      [ 0.0250, -0.0101, -0.0385,  ..., -0.0169,  0.0344, -0.0830],\n",
       "                      [ 0.0148,  0.0420,  0.0343,  ..., -0.0323, -0.0143,  0.0348]])),\n",
       "             ('encoder.layer.19.intermediate.dense.bias',\n",
       "              tensor([-0.1027,  0.0130, -0.0413,  ..., -0.1161,  0.0037, -0.0175])),\n",
       "             ('encoder.layer.19.output.dense.weight',\n",
       "              tensor([[ 0.0157, -0.0176, -0.0245,  ..., -0.0395, -0.0061,  0.0087],\n",
       "                      [-0.0573, -0.0378,  0.0090,  ...,  0.0515, -0.0121,  0.0195],\n",
       "                      [ 0.0135, -0.0142,  0.0011,  ..., -0.0165,  0.0087, -0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0081, -0.0096,  0.0262,  ..., -0.0501,  0.0143,  0.0225],\n",
       "                      [ 0.0266, -0.0103, -0.0321,  ..., -0.0209, -0.0057, -0.0068],\n",
       "                      [ 0.0064,  0.0135, -0.0025,  ...,  0.0360,  0.0399,  0.0317]])),\n",
       "             ('encoder.layer.19.output.dense.bias',\n",
       "              tensor([-0.0392,  0.0273, -0.0497,  ...,  0.1117, -0.0570,  0.0029])),\n",
       "             ('encoder.layer.19.output.LayerNorm.weight',\n",
       "              tensor([0.9631, 0.9944, 1.0042,  ..., 0.9780, 0.9918, 0.9817])),\n",
       "             ('encoder.layer.19.output.LayerNorm.bias',\n",
       "              tensor([-0.0114, -0.0332,  0.0751,  ..., -0.0200, -0.0158, -0.0068])),\n",
       "             ('encoder.layer.20.attention.self.query.weight',\n",
       "              tensor([[-1.3115e-02,  1.4936e-02, -9.7625e-03,  ...,  4.3980e-03,\n",
       "                       -5.3045e-02,  1.8160e-02],\n",
       "                      [-7.1189e-02, -7.4657e-02,  2.9476e-02,  ..., -2.9077e-02,\n",
       "                       -3.5764e-02, -4.2780e-03],\n",
       "                      [-2.0440e-02, -2.3971e-02,  3.0726e-05,  ...,  3.1105e-02,\n",
       "                        4.1519e-02, -2.0832e-03],\n",
       "                      ...,\n",
       "                      [ 9.4175e-02, -8.4588e-03, -2.8879e-02,  ...,  7.2924e-02,\n",
       "                       -2.2383e-02,  2.0226e-02],\n",
       "                      [ 1.9405e-03,  2.5292e-03,  1.9532e-01,  ..., -2.3678e-02,\n",
       "                        2.5783e-02,  1.7640e-03],\n",
       "                      [-2.5784e-02,  1.5523e-02, -1.1787e-01,  ..., -4.7303e-02,\n",
       "                       -3.7190e-02, -4.7594e-02]])),\n",
       "             ('encoder.layer.20.attention.self.query.bias',\n",
       "              tensor([-0.0267,  0.0258,  0.0140,  ...,  0.0910, -0.0346,  0.0514])),\n",
       "             ('encoder.layer.20.attention.self.key.weight',\n",
       "              tensor([[ 0.0145, -0.0718, -0.0236,  ..., -0.1068, -0.0220, -0.0877],\n",
       "                      [-0.0050, -0.0022,  0.0373,  ..., -0.0205,  0.0026, -0.0093],\n",
       "                      [ 0.0264,  0.0512, -0.0865,  ...,  0.1083,  0.0240,  0.0357],\n",
       "                      ...,\n",
       "                      [-0.0404, -0.0556, -0.1095,  ..., -0.0243, -0.0121,  0.0292],\n",
       "                      [-0.0261,  0.0520,  0.2238,  ...,  0.0249, -0.0611,  0.0284],\n",
       "                      [-0.0197,  0.0216, -0.0483,  ..., -0.0921, -0.0149,  0.0154]])),\n",
       "             ('encoder.layer.20.attention.self.key.bias',\n",
       "              tensor([ 3.0853e-04, -2.8179e-05,  2.8121e-04,  ...,  1.7422e-04,\n",
       "                       1.2292e-08,  5.3512e-05])),\n",
       "             ('encoder.layer.20.attention.self.value.weight',\n",
       "              tensor([[-0.0486, -0.0083,  0.0256,  ..., -0.0257, -0.0005,  0.0611],\n",
       "                      [-0.0171, -0.0028, -0.0178,  ..., -0.0246,  0.1037,  0.0197],\n",
       "                      [ 0.0008,  0.0336, -0.0108,  ..., -0.0682, -0.0788,  0.0147],\n",
       "                      ...,\n",
       "                      [-0.0616,  0.0318,  0.0210,  ...,  0.0682,  0.0111,  0.0197],\n",
       "                      [ 0.0906,  0.0066,  0.0601,  ..., -0.0046,  0.0152, -0.0504],\n",
       "                      [ 0.0186,  0.0405, -0.0109,  ..., -0.0271, -0.0064, -0.0724]])),\n",
       "             ('encoder.layer.20.attention.self.value.bias',\n",
       "              tensor([ 1.4106e-05,  5.7234e-03,  1.9764e-02,  ...,  2.2555e-03,\n",
       "                       2.5247e-03, -6.1455e-03])),\n",
       "             ('encoder.layer.20.attention.output.dense.weight',\n",
       "              tensor([[ 0.0135, -0.0618, -0.0506,  ...,  0.0575, -0.0529, -0.0153],\n",
       "                      [ 0.0115,  0.0437,  0.0142,  ..., -0.0175, -0.0112, -0.0564],\n",
       "                      [ 0.0255,  0.0086,  0.0075,  ..., -0.0167, -0.0534, -0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0404, -0.0272, -0.0497,  ..., -0.0742, -0.0227,  0.0357],\n",
       "                      [ 0.0183,  0.0035,  0.0066,  ..., -0.0199, -0.0139,  0.0315],\n",
       "                      [ 0.0122,  0.0035, -0.0150,  ...,  0.0205,  0.0145,  0.0349]])),\n",
       "             ('encoder.layer.20.attention.output.dense.bias',\n",
       "              tensor([-0.0112,  0.0780,  0.0613,  ...,  0.0723,  0.0872,  0.0218])),\n",
       "             ('encoder.layer.20.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9692, 0.9957, 1.0072,  ..., 0.9798, 0.9866, 0.9914])),\n",
       "             ('encoder.layer.20.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0311, -0.0348, -0.1602,  ..., -0.0451, -0.0886, -0.0468])),\n",
       "             ('encoder.layer.20.intermediate.dense.weight',\n",
       "              tensor([[ 0.0095, -0.0093,  0.0576,  ...,  0.0083,  0.0102,  0.0082],\n",
       "                      [ 0.0200, -0.0148,  0.0025,  ...,  0.0102, -0.0648,  0.0683],\n",
       "                      [-0.0119, -0.0201, -0.0564,  ..., -0.0208, -0.0349, -0.0389],\n",
       "                      ...,\n",
       "                      [ 0.0057, -0.0302,  0.0013,  ...,  0.0400,  0.0906,  0.0208],\n",
       "                      [ 0.0597,  0.0462,  0.0585,  ..., -0.0488, -0.0088, -0.0310],\n",
       "                      [-0.0171, -0.0431,  0.0484,  ..., -0.0083,  0.0033, -0.0075]])),\n",
       "             ('encoder.layer.20.intermediate.dense.bias',\n",
       "              tensor([-0.0221, -0.0369, -0.0474,  ..., -0.0264, -0.0183, -0.0261])),\n",
       "             ('encoder.layer.20.output.dense.weight',\n",
       "              tensor([[-0.0152,  0.0656,  0.0176,  ...,  0.0622, -0.0086, -0.0111],\n",
       "                      [ 0.0092,  0.0312,  0.0496,  ...,  0.0039,  0.0142, -0.0372],\n",
       "                      [-0.0139, -0.0025, -0.0081,  ..., -0.0087, -0.0086, -0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0115,  0.0090,  0.0501,  ..., -0.0162,  0.0141, -0.0056],\n",
       "                      [-0.0196, -0.0479,  0.0235,  ...,  0.0297,  0.0586,  0.0198],\n",
       "                      [-0.0328,  0.0063,  0.0376,  ...,  0.0191,  0.0393, -0.0064]])),\n",
       "             ('encoder.layer.20.output.dense.bias',\n",
       "              tensor([-0.0565,  0.0040, -0.0080,  ...,  0.0629, -0.0944,  0.0491])),\n",
       "             ('encoder.layer.20.output.LayerNorm.weight',\n",
       "              tensor([0.9627, 0.9911, 1.0036,  ..., 0.9786, 0.9847, 0.9765])),\n",
       "             ('encoder.layer.20.output.LayerNorm.bias',\n",
       "              tensor([-0.0322, -0.0316,  0.0779,  ..., -0.0294, -0.0228, -0.0139])),\n",
       "             ('encoder.layer.21.attention.self.query.weight',\n",
       "              tensor([[ 0.0315, -0.0814, -0.2116,  ..., -0.0112, -0.0087, -0.0401],\n",
       "                      [-0.0272, -0.0827,  0.0521,  ..., -0.0228,  0.0216,  0.0027],\n",
       "                      [ 0.0586,  0.0356,  0.0588,  ...,  0.0329, -0.0431, -0.0717],\n",
       "                      ...,\n",
       "                      [ 0.0068,  0.0119, -0.1494,  ...,  0.0716,  0.0672,  0.0533],\n",
       "                      [-0.0633,  0.0126,  0.0349,  ...,  0.0070,  0.0113,  0.0067],\n",
       "                      [-0.0435, -0.0405,  0.0455,  ...,  0.0176, -0.0253,  0.0661]])),\n",
       "             ('encoder.layer.21.attention.self.query.bias',\n",
       "              tensor([ 0.1653, -0.0315, -0.0168,  ..., -0.0684,  0.0426, -0.0455])),\n",
       "             ('encoder.layer.21.attention.self.key.weight',\n",
       "              tensor([[ 0.0130,  0.0519, -0.1547,  ...,  0.0728,  0.0305,  0.0097],\n",
       "                      [ 0.0467,  0.0456,  0.0685,  ...,  0.0112,  0.1297, -0.0381],\n",
       "                      [ 0.0017, -0.0026, -0.0007,  ..., -0.0787,  0.0141, -0.0095],\n",
       "                      ...,\n",
       "                      [-0.0560, -0.0401, -0.1232,  ...,  0.0264, -0.0542,  0.0314],\n",
       "                      [ 0.0835, -0.0179,  0.0703,  ..., -0.0970,  0.0286, -0.0070],\n",
       "                      [-0.0285,  0.0190,  0.0790,  ..., -0.0427,  0.0081, -0.0051]])),\n",
       "             ('encoder.layer.21.attention.self.key.bias',\n",
       "              tensor([-5.0629e-02,  3.4351e-05,  5.9487e-04,  ...,  3.9062e-03,\n",
       "                      -3.6043e-04,  1.0155e-02])),\n",
       "             ('encoder.layer.21.attention.self.value.weight',\n",
       "              tensor([[ 3.2243e-02, -3.4174e-02, -5.3768e-02,  ..., -3.1361e-02,\n",
       "                        1.3854e-02, -2.7402e-02],\n",
       "                      [ 3.9037e-02, -5.0298e-02, -1.2667e-03,  ...,  2.5542e-02,\n",
       "                       -5.6431e-02,  7.1398e-02],\n",
       "                      [ 1.5327e-02, -4.0448e-02, -1.2028e-02,  ...,  8.6626e-03,\n",
       "                       -3.9227e-02,  2.5373e-02],\n",
       "                      ...,\n",
       "                      [ 2.4222e-02, -6.9314e-02, -1.0638e-02,  ..., -3.0884e-02,\n",
       "                       -7.5053e-02, -3.2356e-02],\n",
       "                      [ 2.8322e-02, -3.2017e-02, -4.3159e-02,  ...,  7.0874e-02,\n",
       "                       -1.6705e-02,  2.8124e-02],\n",
       "                      [-2.2395e-02, -4.9926e-05,  9.5526e-02,  ..., -3.2299e-02,\n",
       "                       -2.1981e-02,  2.3846e-02]])),\n",
       "             ('encoder.layer.21.attention.self.value.bias',\n",
       "              tensor([ 0.0075, -0.0021, -0.0008,  ..., -0.0111, -0.0249, -0.0127])),\n",
       "             ('encoder.layer.21.attention.output.dense.weight',\n",
       "              tensor([[-0.0017,  0.0455, -0.0311,  ..., -0.0232,  0.0059,  0.0366],\n",
       "                      [ 0.0351,  0.0160, -0.0300,  ...,  0.0087,  0.0267, -0.0187],\n",
       "                      [ 0.0117, -0.0413, -0.0056,  ..., -0.0126,  0.0144, -0.0413],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0409, -0.0051,  ...,  0.0206, -0.0205,  0.0498],\n",
       "                      [-0.0484,  0.0733, -0.0658,  ..., -0.0030, -0.0429, -0.0191],\n",
       "                      [ 0.0154,  0.0116, -0.0575,  ...,  0.0545, -0.0240, -0.0262]])),\n",
       "             ('encoder.layer.21.attention.output.dense.bias',\n",
       "              tensor([-0.0274,  0.0892,  0.0454,  ...,  0.0358,  0.1777,  0.0312])),\n",
       "             ('encoder.layer.21.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9668, 0.9895, 1.0053,  ..., 0.9770, 0.9913, 0.9896])),\n",
       "             ('encoder.layer.21.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0487, -0.0606, -0.1383,  ..., -0.0499, -0.1008, -0.0356])),\n",
       "             ('encoder.layer.21.intermediate.dense.weight',\n",
       "              tensor([[-0.0126, -0.0343,  0.0389,  ...,  0.0518,  0.0210, -0.0267],\n",
       "                      [ 0.0028,  0.0020, -0.0758,  ...,  0.0252,  0.0016,  0.0544],\n",
       "                      [ 0.0008,  0.0387, -0.0455,  ..., -0.0050,  0.0095,  0.0095],\n",
       "                      ...,\n",
       "                      [-0.0071, -0.0173,  0.0257,  ..., -0.0215, -0.0717, -0.0252],\n",
       "                      [ 0.0077, -0.0702,  0.0165,  ..., -0.0217,  0.0086, -0.0221],\n",
       "                      [ 0.0022, -0.0021,  0.0193,  ..., -0.0100,  0.0342,  0.0370]])),\n",
       "             ('encoder.layer.21.intermediate.dense.bias',\n",
       "              tensor([-0.0271, -0.0449,  0.0201,  ..., -0.0285, -0.0026, -0.0149])),\n",
       "             ('encoder.layer.21.output.dense.weight',\n",
       "              tensor([[-1.4080e-02, -9.5534e-04, -4.6170e-03,  ...,  3.7788e-02,\n",
       "                       -2.1919e-02, -3.8003e-02],\n",
       "                      [ 2.5406e-03,  9.6984e-03, -3.7046e-02,  ...,  1.4697e-02,\n",
       "                       -3.4064e-02,  5.4071e-02],\n",
       "                      [-1.5692e-02,  4.5745e-03,  4.0317e-03,  ...,  1.0759e-02,\n",
       "                        7.1313e-03,  1.3717e-02],\n",
       "                      ...,\n",
       "                      [-2.9787e-02,  1.0477e-02,  2.5063e-02,  ..., -1.8926e-02,\n",
       "                       -1.2228e-02, -1.8685e-03],\n",
       "                      [-3.3385e-02,  5.2130e-05, -4.0169e-02,  ..., -3.5337e-02,\n",
       "                       -1.0499e-01,  4.1640e-03],\n",
       "                      [-9.8540e-03, -8.4219e-04,  2.5743e-03,  ..., -3.5863e-03,\n",
       "                       -3.0583e-02,  1.7789e-03]])),\n",
       "             ('encoder.layer.21.output.dense.bias',\n",
       "              tensor([-0.0099,  0.0060, -0.0720,  ...,  0.0923, -0.1211,  0.0150])),\n",
       "             ('encoder.layer.21.output.LayerNorm.weight',\n",
       "              tensor([0.9673, 0.9894, 1.0011,  ..., 0.9811, 0.9847, 0.9928])),\n",
       "             ('encoder.layer.21.output.LayerNorm.bias',\n",
       "              tensor([-0.0451, -0.0148,  0.0372,  ..., -0.0385, -0.0568, -0.0171])),\n",
       "             ('encoder.layer.22.attention.self.query.weight',\n",
       "              tensor([[ 0.0026,  0.0378,  0.0854,  ...,  0.0152,  0.0212,  0.0614],\n",
       "                      [ 0.0149, -0.0396,  0.0051,  ..., -0.1006,  0.0524,  0.0116],\n",
       "                      [ 0.1007, -0.0234,  0.1845,  ...,  0.0099,  0.0581, -0.0071],\n",
       "                      ...,\n",
       "                      [-0.0074, -0.0104, -0.0815,  ..., -0.0717, -0.0109,  0.0694],\n",
       "                      [-0.0419,  0.0749, -0.0131,  ...,  0.0588, -0.0370, -0.0314],\n",
       "                      [ 0.0329,  0.0582,  0.1000,  ...,  0.0261, -0.0223, -0.0784]])),\n",
       "             ('encoder.layer.22.attention.self.query.bias',\n",
       "              tensor([-0.0048,  0.0064,  0.1516,  ...,  0.0031,  0.0999, -0.0561])),\n",
       "             ('encoder.layer.22.attention.self.key.weight',\n",
       "              tensor([[-0.0685, -0.0499,  0.0713,  ..., -0.0005, -0.0572,  0.0451],\n",
       "                      [ 0.0254, -0.0257,  0.0250,  ...,  0.0302, -0.0138,  0.0120],\n",
       "                      [ 0.0057,  0.0316,  0.0902,  ...,  0.0178, -0.0221,  0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0834, -0.0023, -0.1013,  ..., -0.0576, -0.0543, -0.0917],\n",
       "                      [ 0.0232,  0.0664, -0.0603,  ...,  0.0353,  0.0380, -0.0356],\n",
       "                      [-0.0486,  0.0168, -0.0307,  ..., -0.0006,  0.0192, -0.0096]])),\n",
       "             ('encoder.layer.22.attention.self.key.bias',\n",
       "              tensor([ 6.6019e-06, -1.4790e-04,  4.6691e-04,  ..., -2.0335e-04,\n",
       "                      -5.9094e-04, -6.9443e-04])),\n",
       "             ('encoder.layer.22.attention.self.value.weight',\n",
       "              tensor([[ 0.0249,  0.0363,  0.0442,  ...,  0.0461, -0.0213,  0.0507],\n",
       "                      [-0.0470, -0.0136, -0.0152,  ...,  0.0514, -0.0743,  0.0575],\n",
       "                      [-0.0205,  0.0294,  0.0064,  ..., -0.0035, -0.0165,  0.0182],\n",
       "                      ...,\n",
       "                      [-0.0432,  0.0030, -0.0155,  ..., -0.0341,  0.0296, -0.0486],\n",
       "                      [ 0.0133,  0.0126,  0.0344,  ...,  0.0061,  0.0057, -0.0362],\n",
       "                      [-0.0226, -0.0177, -0.0385,  ..., -0.0273,  0.0153,  0.0226]])),\n",
       "             ('encoder.layer.22.attention.self.value.bias',\n",
       "              tensor([ 0.0080, -0.0129,  0.0143,  ..., -0.0117, -0.0054, -0.0076])),\n",
       "             ('encoder.layer.22.attention.output.dense.weight',\n",
       "              tensor([[ 0.0414,  0.0517, -0.0049,  ..., -0.0323, -0.0328, -0.0129],\n",
       "                      [ 0.0059,  0.0239,  0.0303,  ..., -0.0470,  0.0041,  0.0304],\n",
       "                      [ 0.0199,  0.0043, -0.0265,  ..., -0.0198,  0.0528, -0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0317,  0.0227, -0.0212,  ...,  0.0122, -0.0228, -0.0110],\n",
       "                      [ 0.0445, -0.0167,  0.0204,  ..., -0.0174, -0.0060, -0.0431],\n",
       "                      [ 0.0304,  0.0418,  0.0384,  ..., -0.0349,  0.0053, -0.0037]])),\n",
       "             ('encoder.layer.22.attention.output.dense.bias',\n",
       "              tensor([ 0.0753,  0.0941,  0.0675,  ...,  0.0084,  0.1457, -0.0308])),\n",
       "             ('encoder.layer.22.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9515, 0.9852, 0.9995,  ..., 0.9725, 0.9930, 0.9850])),\n",
       "             ('encoder.layer.22.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.0501, -0.0150, -0.1247,  ..., -0.0379, -0.1028,  0.0324])),\n",
       "             ('encoder.layer.22.intermediate.dense.weight',\n",
       "              tensor([[-0.0009, -0.0700, -0.0485,  ..., -0.0086,  0.0134,  0.0811],\n",
       "                      [-0.0197, -0.0194,  0.0608,  ...,  0.0033,  0.0439,  0.0020],\n",
       "                      [-0.0281, -0.0663,  0.0730,  ...,  0.0117, -0.0139, -0.0493],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0190,  0.0444,  ...,  0.0502,  0.0126,  0.0402],\n",
       "                      [ 0.0633,  0.0161,  0.0454,  ...,  0.0209, -0.0024,  0.0065],\n",
       "                      [ 0.0063, -0.0767, -0.0605,  ..., -0.0088,  0.0199, -0.0466]])),\n",
       "             ('encoder.layer.22.intermediate.dense.bias',\n",
       "              tensor([ 0.0177, -0.0072, -0.0126,  ..., -0.0454,  0.0153, -0.0282])),\n",
       "             ('encoder.layer.22.output.dense.weight',\n",
       "              tensor([[-0.0261,  0.0351, -0.0261,  ...,  0.0077, -0.0058, -0.0847],\n",
       "                      [ 0.0125, -0.0250,  0.0233,  ..., -0.0074, -0.0301,  0.0166],\n",
       "                      [-0.0164, -0.0115, -0.0037,  ...,  0.0010, -0.0032,  0.0124],\n",
       "                      ...,\n",
       "                      [-0.0120, -0.0294,  0.0122,  ..., -0.0220, -0.0054,  0.0575],\n",
       "                      [ 0.0175, -0.0363,  0.0412,  ..., -0.0109, -0.0258, -0.0282],\n",
       "                      [-0.0080,  0.0700,  0.0311,  ..., -0.0215,  0.0059, -0.0248]])),\n",
       "             ('encoder.layer.22.output.dense.bias',\n",
       "              tensor([ 0.0236,  0.0462, -0.0752,  ...,  0.0247, -0.1327,  0.0456])),\n",
       "             ('encoder.layer.22.output.LayerNorm.weight',\n",
       "              tensor([0.9593, 0.9942, 1.0005,  ..., 0.9805, 0.9891, 0.9810])),\n",
       "             ('encoder.layer.22.output.LayerNorm.bias',\n",
       "              tensor([-0.0237,  0.0375,  0.0468,  ..., -0.0320, -0.0132,  0.0166])),\n",
       "             ('encoder.layer.23.attention.self.query.weight',\n",
       "              tensor([[-0.0409,  0.0191,  0.0937,  ..., -0.0863,  0.0839,  0.0599],\n",
       "                      [-0.0432,  0.0182,  0.1023,  ..., -0.0069, -0.0157, -0.0030],\n",
       "                      [ 0.0404,  0.0438,  0.0305,  ..., -0.0095, -0.0849, -0.0327],\n",
       "                      ...,\n",
       "                      [ 0.1009, -0.0497,  0.1281,  ...,  0.0009, -0.0081,  0.0643],\n",
       "                      [-0.0516, -0.0006,  0.0004,  ..., -0.0482, -0.0116, -0.0151],\n",
       "                      [-0.0051, -0.0501, -0.0565,  ...,  0.0479, -0.0630,  0.0153]])),\n",
       "             ('encoder.layer.23.attention.self.query.bias',\n",
       "              tensor([ 0.0774,  0.2727,  0.0311,  ..., -0.2014,  0.1130,  0.0798])),\n",
       "             ('encoder.layer.23.attention.self.key.weight',\n",
       "              tensor([[ 0.0571, -0.0263,  0.2126,  ..., -0.0136,  0.0258,  0.0240],\n",
       "                      [ 0.0730, -0.0683,  0.0741,  ...,  0.0123, -0.0393,  0.0109],\n",
       "                      [-0.1113, -0.0295, -0.0356,  ..., -0.1253,  0.0404, -0.0294],\n",
       "                      ...,\n",
       "                      [ 0.0471,  0.0106,  0.0711,  ..., -0.0422,  0.0003,  0.0538],\n",
       "                      [ 0.0226,  0.0111, -0.0681,  ..., -0.0218,  0.0364,  0.0355],\n",
       "                      [-0.0145, -0.0096, -0.0155,  ...,  0.1088,  0.0025,  0.0643]])),\n",
       "             ('encoder.layer.23.attention.self.key.bias',\n",
       "              tensor([ 8.6450e-08,  3.7336e-07,  4.0049e-05,  ...,  6.2500e-02,\n",
       "                       1.4043e-05, -1.9511e-04])),\n",
       "             ('encoder.layer.23.attention.self.value.weight',\n",
       "              tensor([[-0.0673, -0.0654, -0.0138,  ..., -0.0355,  0.0018,  0.0115],\n",
       "                      [ 0.0215,  0.0055,  0.0401,  ..., -0.0246,  0.0015, -0.0496],\n",
       "                      [-0.0486,  0.0671, -0.0138,  ...,  0.0061, -0.0483, -0.0119],\n",
       "                      ...,\n",
       "                      [ 0.0515,  0.0257, -0.0294,  ..., -0.0102, -0.0249,  0.0134],\n",
       "                      [ 0.0011, -0.0348,  0.0243,  ..., -0.0157, -0.0015,  0.0152],\n",
       "                      [ 0.0111, -0.0228,  0.0260,  ..., -0.0005,  0.0200,  0.0172]])),\n",
       "             ('encoder.layer.23.attention.self.value.bias',\n",
       "              tensor([-0.0155,  0.0177,  0.0123,  ...,  0.0079,  0.0072,  0.0138])),\n",
       "             ('encoder.layer.23.attention.output.dense.weight',\n",
       "              tensor([[-0.0215, -0.0081, -0.0026,  ...,  0.0418, -0.0301,  0.0336],\n",
       "                      [-0.0413, -0.0079,  0.0248,  ...,  0.0499, -0.0092, -0.0455],\n",
       "                      [-0.0200,  0.0063,  0.0087,  ..., -0.0090,  0.0118, -0.0317],\n",
       "                      ...,\n",
       "                      [-0.0008, -0.0508, -0.0132,  ..., -0.0216,  0.0003, -0.0037],\n",
       "                      [ 0.0149,  0.0423,  0.0136,  ...,  0.0098,  0.0046,  0.0036],\n",
       "                      [ 0.0370, -0.0166, -0.0272,  ..., -0.0150,  0.0152,  0.0120]])),\n",
       "             ('encoder.layer.23.attention.output.dense.bias',\n",
       "              tensor([ 0.0636,  0.0719, -0.0054,  ...,  0.0315,  0.0202,  0.1614])),\n",
       "             ('encoder.layer.23.attention.output.LayerNorm.weight',\n",
       "              tensor([0.9604, 0.9786, 0.9909,  ..., 0.9761, 0.9882, 0.9603])),\n",
       "             ('encoder.layer.23.attention.output.LayerNorm.bias',\n",
       "              tensor([-0.1368,  0.0302, -0.1639,  ..., -0.0658, -0.1448, -0.0929])),\n",
       "             ('encoder.layer.23.intermediate.dense.weight',\n",
       "              tensor([[-0.0119, -0.0864, -0.0105,  ..., -0.0117,  0.0108, -0.0252],\n",
       "                      [-0.0165, -0.0247, -0.0123,  ..., -0.0120,  0.0182, -0.0586],\n",
       "                      [-0.0007, -0.0929, -0.0037,  ...,  0.0337,  0.0321, -0.0171],\n",
       "                      ...,\n",
       "                      [-0.0368,  0.0211,  0.0122,  ...,  0.0050, -0.0397, -0.0084],\n",
       "                      [ 0.0432,  0.0011, -0.0078,  ..., -0.0187,  0.0621,  0.0443],\n",
       "                      [ 0.0391, -0.0222, -0.0280,  ...,  0.0496,  0.0291,  0.0443]])),\n",
       "             ('encoder.layer.23.intermediate.dense.bias',\n",
       "              tensor([ 0.0084, -0.0676, -0.1270,  ..., -0.0404, -0.0937, -0.0125])),\n",
       "             ('encoder.layer.23.output.dense.weight',\n",
       "              tensor([[ 0.0262, -0.0394, -0.0192,  ...,  0.0155,  0.0264, -0.0054],\n",
       "                      [ 0.0111,  0.0634, -0.0235,  ..., -0.0176, -0.0399,  0.0271],\n",
       "                      [-0.0018, -0.0094,  0.0056,  ...,  0.0098, -0.0262, -0.0155],\n",
       "                      ...,\n",
       "                      [ 0.0151, -0.0319,  0.0787,  ...,  0.0274, -0.0185,  0.0414],\n",
       "                      [ 0.0103, -0.0147,  0.0472,  ...,  0.0058,  0.0256, -0.0068],\n",
       "                      [-0.0396,  0.0106,  0.0004,  ...,  0.0351,  0.0151, -0.0144]])),\n",
       "             ('encoder.layer.23.output.dense.bias',\n",
       "              tensor([-0.0765, -0.0247, -0.0954,  ...,  0.0770, -0.1594,  0.0791])),\n",
       "             ('encoder.layer.23.output.LayerNorm.weight',\n",
       "              tensor([0.9800, 0.9922, 0.9934,  ..., 0.9763, 0.9999, 0.9859])),\n",
       "             ('encoder.layer.23.output.LayerNorm.bias',\n",
       "              tensor([-0.0250, -0.0420, -0.0178,  ..., -0.0464,  0.0048, -0.0090])),\n",
       "             ('pooler.dense.weight',\n",
       "              tensor([[-2.1352e-02, -6.3008e-03, -2.8840e-02,  ...,  9.4264e-03,\n",
       "                       -7.8792e-03,  1.5523e-02],\n",
       "                      [-2.5612e-03, -1.5404e-02,  5.1179e-03,  ..., -4.0077e-02,\n",
       "                       -1.0304e-02, -4.0112e-02],\n",
       "                      [ 7.8875e-03,  1.9900e-02, -1.4256e-03,  ...,  5.3420e-03,\n",
       "                       -1.0706e-02,  1.6005e-03],\n",
       "                      ...,\n",
       "                      [ 1.4794e-05, -7.4761e-03, -1.6566e-02,  ..., -2.6039e-02,\n",
       "                       -7.8030e-04, -6.7619e-03],\n",
       "                      [-8.0222e-03, -1.4700e-02, -2.6027e-02,  ..., -3.0777e-02,\n",
       "                       -1.1340e-02,  2.3156e-02],\n",
       "                      [-6.0101e-03,  2.7930e-02, -4.3496e-02,  ...,  2.1970e-02,\n",
       "                        1.4160e-03, -1.3375e-02]])),\n",
       "             ('pooler.dense.bias', tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('embeddings.word_embeddings.weight',\n",
       "              tensor([[-0.1404, -0.0087,  0.0383,  ...,  0.0512, -0.0066, -0.0372],\n",
       "                      [ 0.0078, -0.0156,  0.0156,  ..., -0.0156,  0.0229,  0.0156],\n",
       "                      [-0.0797,  0.0003, -0.1160,  ...,  0.1081,  0.0652, -0.0377],\n",
       "                      ...,\n",
       "                      [ 0.0396,  0.0010,  0.0478,  ..., -0.0250, -0.0500,  0.0353],\n",
       "                      [ 0.0481,  0.0262,  0.0424,  ..., -0.0371, -0.0062,  0.0085],\n",
       "                      [-0.0130, -0.0106, -0.0229,  ...,  0.0451,  0.0108, -0.0358]])),\n",
       "             ('embeddings.position_embeddings.weight',\n",
       "              tensor([[-0.0038,  0.0252, -0.0092,  ...,  0.0177,  0.0062, -0.0161],\n",
       "                      [ 0.0116, -0.0018, -0.0266,  ...,  0.0061, -0.0192,  0.0262],\n",
       "                      [ 0.0306,  0.0155, -0.0553,  ..., -0.0707, -0.0462,  0.0452],\n",
       "                      ...,\n",
       "                      [-0.0203, -0.0061,  0.0471,  ..., -0.0387,  0.0448,  0.0537],\n",
       "                      [-0.0275,  0.1194,  0.0451,  ...,  0.0209, -0.1188,  0.0506],\n",
       "                      [ 0.0959, -0.0756,  0.0514,  ..., -0.1151, -0.1053,  0.0489]])),\n",
       "             ('embeddings.token_type_embeddings.weight',\n",
       "              tensor([[-1.1283e-03,  2.8004e-04,  1.0483e-03,  ...,  9.2064e-05,\n",
       "                       -6.4389e-04, -1.2607e-03]])),\n",
       "             ('embeddings.LayerNorm.weight',\n",
       "              tensor([0.9318, 0.9240, 0.9121,  ..., 0.9391, 0.9139, 0.9027])),\n",
       "             ('embeddings.LayerNorm.bias',\n",
       "              tensor([ 0.0298,  0.0421,  0.1938,  ..., -0.2250, -0.0895,  0.1242])),\n",
       "             ('entity_embeddings.entity_embeddings.weight',\n",
       "              tensor([[-0.0420, -0.0610, -0.0482,  ..., -0.0336, -0.0534, -0.0462],\n",
       "                      [ 0.0699,  0.0621,  0.0855,  ...,  0.0551,  0.0466,  0.0777],\n",
       "                      [ 0.0074, -0.0067,  0.0015,  ..., -0.0110, -0.0052, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0780, -0.0544, -0.0635,  ...,  0.0061, -0.0908, -0.0801],\n",
       "                      [-0.0913, -0.0692, -0.0123,  ..., -0.1385, -0.0340, -0.0620],\n",
       "                      [ 0.0124, -0.0218, -0.0023,  ..., -0.1331, -0.0721, -0.0953]])),\n",
       "             ('entity_embeddings.entity_embedding_dense.weight',\n",
       "              tensor([[-0.1982,  0.0018, -0.0203,  ...,  0.0639, -0.0417, -0.0401],\n",
       "                      [-0.0741,  0.0551, -0.0618,  ...,  0.0467,  0.2286,  0.1207],\n",
       "                      [-0.1025,  0.0275,  0.0391,  ...,  0.1444,  0.0420, -0.0251],\n",
       "                      ...,\n",
       "                      [ 0.0622,  0.0238,  0.0709,  ..., -0.0283, -0.0438, -0.0187],\n",
       "                      [-0.1013,  0.0199,  0.0937,  ..., -0.1863, -0.1312,  0.0492],\n",
       "                      [-0.0088, -0.0434,  0.0287,  ..., -0.0318,  0.0040,  0.0186]])),\n",
       "             ('entity_embeddings.position_embeddings.weight',\n",
       "              tensor([[-0.0203, -0.0177, -0.0124,  ..., -0.0187,  0.0235,  0.0100],\n",
       "                      [-0.0509,  0.0135, -0.0595,  ..., -0.2041, -0.0272, -0.0601],\n",
       "                      [-0.0264, -0.1050, -0.0073,  ..., -0.1039,  0.0195, -0.0697],\n",
       "                      ...,\n",
       "                      [ 0.0192,  0.0089,  0.0098,  ..., -0.0163, -0.0025, -0.0114],\n",
       "                      [-0.0093,  0.0056,  0.0043,  ..., -0.0050, -0.0219,  0.0092],\n",
       "                      [ 0.0004,  0.0029, -0.0044,  ..., -0.0157, -0.0013, -0.0172]])),\n",
       "             ('entity_embeddings.token_type_embeddings.weight',\n",
       "              tensor([[ 0.0201,  0.0638,  0.0469,  ..., -0.0644, -0.0466,  0.0503]])),\n",
       "             ('entity_embeddings.LayerNorm.weight',\n",
       "              tensor([1.0415, 1.0246, 1.0098,  ..., 1.0065, 1.0872, 0.9889])),\n",
       "             ('entity_embeddings.LayerNorm.bias',\n",
       "              tensor([ 0.3529, -0.1463,  0.0741,  ...,  0.5572,  0.0607, -0.2043])),\n",
       "             ('lm_head.bias',\n",
       "              tensor([ 0.3295, -0.0163, -0.0058,  ..., -0.0156, -0.0156, -0.2568])),\n",
       "             ('lm_head.dense.weight',\n",
       "              tensor([[ 0.1632,  0.1019, -0.0025,  ..., -0.0330,  0.1246,  0.0449],\n",
       "                      [-0.1906,  0.1546, -0.1123,  ...,  0.0416,  0.0078,  0.0102],\n",
       "                      [ 0.1089,  0.1496,  0.0908,  ...,  0.0173,  0.0318,  0.0263],\n",
       "                      ...,\n",
       "                      [-0.0824,  0.0716,  0.0727,  ...,  0.2268, -0.0069,  0.0324],\n",
       "                      [-0.0286,  0.0801,  0.2645,  ...,  0.0004,  0.1038,  0.0283],\n",
       "                      [-0.2233,  0.0367, -0.0502,  ...,  0.0384,  0.0578,  0.2591]])),\n",
       "             ('lm_head.dense.bias',\n",
       "              tensor([ 0.0343,  0.0180, -0.0022,  ...,  0.0114,  0.0038,  0.0422])),\n",
       "             ('lm_head.layer_norm.weight',\n",
       "              tensor([1.0218, 1.0211, 1.0197,  ..., 1.0220, 1.0028, 1.0224])),\n",
       "             ('lm_head.layer_norm.bias',\n",
       "              tensor([-0.2555, -0.2534, -0.2543,  ...,  0.1374,  0.1212, -0.2528])),\n",
       "             ('lm_head.decoder.weight',\n",
       "              tensor([[-0.1404, -0.0087,  0.0383,  ...,  0.0512, -0.0066, -0.0372],\n",
       "                      [ 0.0078, -0.0156,  0.0156,  ..., -0.0156,  0.0229,  0.0156],\n",
       "                      [-0.0797,  0.0003, -0.1160,  ...,  0.1081,  0.0652, -0.0377],\n",
       "                      ...,\n",
       "                      [ 0.0396,  0.0010,  0.0478,  ..., -0.0250, -0.0500,  0.0353],\n",
       "                      [ 0.0481,  0.0262,  0.0424,  ..., -0.0371, -0.0062,  0.0085],\n",
       "                      [-0.0130, -0.0106, -0.0229,  ...,  0.0451,  0.0108, -0.0358]])),\n",
       "             ('entity_predictions.bias',\n",
       "              tensor([-0.0972,  0.9144, -2.8480,  ..., -0.0286, -0.0522, -0.0247])),\n",
       "             ('entity_predictions.transform.dense.weight',\n",
       "              tensor([[-0.0272,  0.0312, -0.0100,  ...,  0.0612, -0.0432, -0.0259],\n",
       "                      [ 0.0123, -0.1055,  0.0383,  ..., -0.0163, -0.0093, -0.0945],\n",
       "                      [ 0.0235, -0.0276,  0.0741,  ...,  0.0397,  0.0171,  0.0399],\n",
       "                      ...,\n",
       "                      [-0.0053, -0.0123,  0.0522,  ...,  0.1255,  0.0818,  0.0846],\n",
       "                      [-0.0064, -0.0004,  0.0647,  ...,  0.1070, -0.0957,  0.0028],\n",
       "                      [-0.0536,  0.0037, -0.0180,  ...,  0.0427,  0.0580,  0.0496]])),\n",
       "             ('entity_predictions.transform.dense.bias',\n",
       "              tensor([ 0.0026, -0.0018, -0.0047, -0.0144, -0.0241,  0.0110, -0.0192, -0.1007,\n",
       "                       0.0065,  0.0028, -0.0092, -0.0473, -0.0138, -0.1132,  0.0025, -0.0181,\n",
       "                      -0.0201, -0.0708, -0.0095, -0.1104,  0.0249,  0.0004,  0.0072, -0.1146,\n",
       "                      -0.0172, -0.0080, -0.1031,  0.0009,  0.0378, -0.0135, -0.0072, -0.0222,\n",
       "                       0.0097, -0.0270,  0.0051,  0.0208, -0.0267, -0.0081, -0.0968, -0.0124,\n",
       "                      -0.0213, -0.0143, -0.0051,  0.0143, -0.0469, -0.0370, -0.1155,  0.0131,\n",
       "                       0.0466, -0.0020,  0.0091,  0.0091, -0.1042, -0.0451,  0.0009, -0.1100,\n",
       "                       0.0060, -0.1760, -0.0612,  0.0045, -0.0137,  0.0021,  0.0106, -0.0116,\n",
       "                       0.0079,  0.0102, -0.0227,  0.0122,  0.0065,  0.0031,  0.0105, -0.0705,\n",
       "                      -0.0197, -0.0073, -0.1526, -0.1260, -0.0048, -0.0190, -0.0017, -0.0094,\n",
       "                       0.0086, -0.0320, -0.0189,  0.0102, -0.0087, -0.0056,  0.0012, -0.0119,\n",
       "                      -0.0117, -0.0073, -0.0049, -0.0235, -0.1540, -0.0949, -0.0192, -0.0165,\n",
       "                      -0.0256,  0.0029,  0.0167, -0.0280, -0.0336, -0.0052, -0.0090, -0.0835,\n",
       "                      -0.0154, -0.0046,  0.0093,  0.0239, -0.0155, -0.1528, -0.0116, -0.0903,\n",
       "                      -0.0098, -0.0126,  0.0052, -0.0200, -0.0104,  0.0055, -0.0044,  0.0031,\n",
       "                      -0.1289, -0.0032,  0.0101, -0.0341, -0.0142,  0.0097, -0.0038, -0.0240,\n",
       "                      -0.0224,  0.0058,  0.0080,  0.0113, -0.0125, -0.0979,  0.0193, -0.0033,\n",
       "                       0.0036, -0.0029, -0.0071, -0.0236, -0.0107, -0.0588,  0.0002,  0.0043,\n",
       "                      -0.0147,  0.0083,  0.0139, -0.0079, -0.0118, -0.0166, -0.1036, -0.0095,\n",
       "                       0.0051, -0.0136, -0.0331,  0.0057, -0.0107, -0.0206,  0.0016, -0.1633,\n",
       "                       0.0045, -0.0082, -0.0731, -0.0322, -0.0274, -0.0234, -0.1113,  0.0003,\n",
       "                       0.0035,  0.0062,  0.0005, -0.0331, -0.0101, -0.1605,  0.0439, -0.1747,\n",
       "                       0.0049, -0.0010, -0.0086, -0.0297, -0.0217, -0.0024, -0.1533, -0.0328,\n",
       "                      -0.1235, -0.0064, -0.0114, -0.0149, -0.0144,  0.0058, -0.1371, -0.0226,\n",
       "                       0.0075, -0.1062, -0.1727, -0.0991, -0.0356, -0.0049,  0.0159, -0.0005,\n",
       "                       0.0013, -0.0051, -0.0057, -0.0223, -0.0180, -0.0140, -0.0041, -0.0022,\n",
       "                      -0.0967, -0.0098, -0.0158,  0.0012,  0.0099, -0.0010, -0.0225, -0.0374,\n",
       "                      -0.0257, -0.0202, -0.0150, -0.0063,  0.0152, -0.0198, -0.0048, -0.0305,\n",
       "                       0.0132, -0.0192,  0.0099, -0.0097, -0.1061, -0.0039,  0.0016,  0.0154,\n",
       "                      -0.0134, -0.0022, -0.0118, -0.0110, -0.1003,  0.0098, -0.1264, -0.0052,\n",
       "                       0.0139, -0.0147, -0.0298, -0.0086, -0.1015, -0.0635, -0.1230, -0.0159,\n",
       "                      -0.0090, -0.0019, -0.0065, -0.0018, -0.0074, -0.0393, -0.0263, -0.0014])),\n",
       "             ('entity_predictions.transform.LayerNorm.weight',\n",
       "              tensor([ 4.6629e+00,  4.8788e+00,  3.9105e+00,  4.6968e+00,  4.8175e+00,\n",
       "                       3.7288e+00,  4.9451e+00,  1.0784e-01,  4.3741e+00,  4.9433e+00,\n",
       "                       4.3727e+00,  1.5616e+00,  4.4255e+00,  5.4732e-01,  4.3484e+00,\n",
       "                       4.5584e+00,  5.2402e+00,  5.0982e+00,  4.7467e+00, -7.4830e-02,\n",
       "                       4.6308e+00,  4.3313e+00,  4.5981e+00, -1.7292e-01,  4.7149e+00,\n",
       "                       4.4520e+00, -5.1885e-02,  4.9435e+00,  2.1507e+00,  4.8450e+00,\n",
       "                       4.3616e+00,  4.9097e+00,  5.0949e+00,  5.2151e+00,  4.7536e+00,\n",
       "                       3.8911e+00,  4.5391e+00,  4.2729e+00,  3.1182e-02,  4.4313e+00,\n",
       "                       5.0526e+00,  4.9187e+00,  4.5825e+00,  7.4624e-02,  2.6524e+00,\n",
       "                       5.1469e+00, -1.6904e-01,  4.8107e+00,  1.8882e+00,  4.1674e+00,\n",
       "                       4.5266e+00,  4.5007e+00,  4.0585e-02,  1.2309e+00,  4.6785e+00,\n",
       "                      -1.0577e-01,  2.9556e+00, -2.6214e-01,  2.7258e-01,  5.0342e+00,\n",
       "                       4.6006e+00,  4.0033e+00,  4.6991e+00,  4.8536e+00,  4.7276e+00,\n",
       "                       4.8513e+00,  5.1423e+00,  4.3919e+00,  4.7429e+00,  4.7098e+00,\n",
       "                       4.3278e+00,  7.3041e-01,  4.8560e+00,  4.9605e+00,  9.0967e-01,\n",
       "                      -1.9074e-01,  5.3845e+00,  4.5593e+00,  4.4840e+00,  4.5710e+00,\n",
       "                       4.5160e+00,  4.7010e+00,  5.2552e+00,  3.2832e+00,  4.6135e+00,\n",
       "                       4.7499e+00,  4.4040e+00,  4.3679e+00,  4.2885e+00,  4.5216e+00,\n",
       "                       4.8164e+00,  4.0357e+00,  6.0365e-01,  7.5406e-03,  4.5684e+00,\n",
       "                       3.1417e+00,  4.2947e+00,  4.7757e+00,  3.3801e+00,  2.6716e+00,\n",
       "                       4.7739e+00,  4.7444e+00,  4.6350e+00, -2.8402e-03,  2.4631e+00,\n",
       "                       4.9032e+00,  4.5268e+00,  4.0488e+00,  3.7226e+00,  6.5752e-01,\n",
       "                       4.6525e+00,  4.1537e-02,  5.0343e+00,  4.2938e+00,  4.6950e+00,\n",
       "                       4.8740e+00,  4.4354e+00,  4.2203e+00,  5.1392e+00,  1.2117e+00,\n",
       "                      -2.5468e-01,  3.9803e+00,  4.3335e+00,  2.3782e+00,  5.2215e+00,\n",
       "                       4.2974e+00,  4.4775e+00,  5.4885e+00,  4.5009e+00,  4.4531e+00,\n",
       "                       4.7007e+00,  4.8309e+00,  4.4103e+00, -9.7696e-02,  4.6741e+00,\n",
       "                       4.6367e+00,  4.1610e+00,  4.5585e+00,  4.6085e+00,  4.8919e+00,\n",
       "                       3.8912e+00,  1.4827e+00,  3.8399e+00,  4.8074e+00,  4.5267e+00,\n",
       "                       4.2605e+00,  4.4738e+00,  4.6338e+00,  4.9540e+00,  4.7307e+00,\n",
       "                       8.2631e-02,  4.4070e+00,  4.5612e+00,  4.9937e+00,  4.5188e+00,\n",
       "                       3.7540e+00,  4.7920e+00,  1.4582e+00,  4.5620e+00,  6.5482e-01,\n",
       "                       4.5347e+00,  4.4192e+00,  2.1191e+00,  5.1797e+00,  5.2253e+00,\n",
       "                       4.1670e+00,  1.0633e+00,  4.6496e+00,  4.6278e+00,  4.4111e+00,\n",
       "                       4.5037e+00,  2.7588e+00,  4.2573e+00, -2.8384e-01,  1.4191e+00,\n",
       "                       3.5284e-01,  3.8086e+00,  4.4574e+00,  3.2704e+00,  4.9308e+00,\n",
       "                       1.7416e+00,  4.3647e+00,  8.6427e-01,  5.1520e+00,  1.7652e-01,\n",
       "                       5.0328e+00,  5.1682e+00,  4.0995e+00,  4.2440e+00,  4.4144e+00,\n",
       "                       4.2542e-01,  4.7361e+00,  4.3823e+00,  8.8227e-01,  3.2907e-01,\n",
       "                       2.9388e-03,  5.1480e+00,  4.5945e+00,  4.5277e+00,  4.6940e+00,\n",
       "                       4.6711e+00,  4.8303e+00,  4.7962e+00,  1.0522e+00,  4.2829e+00,\n",
       "                       4.7029e+00,  4.6632e+00,  4.0782e+00, -1.1151e-01,  4.3277e+00,\n",
       "                       4.5961e+00,  3.0291e+00,  4.7282e+00,  4.2931e+00,  3.1443e+00,\n",
       "                       2.9138e+00,  4.9049e+00,  5.1496e+00,  4.4953e+00,  3.7558e+00,\n",
       "                       3.7816e+00,  5.0460e+00,  4.4093e+00,  4.7787e+00,  3.2720e+00,\n",
       "                       4.5470e+00,  3.9479e+00,  4.2739e+00, -5.5783e-02,  4.0292e+00,\n",
       "                       4.8170e+00,  4.3409e+00,  5.1443e+00,  3.6575e+00,  4.4567e+00,\n",
       "                       4.5726e+00, -1.9126e-01,  4.6817e+00,  2.5507e-01,  4.4777e+00,\n",
       "                       5.0486e+00,  4.6036e+00,  5.2079e+00,  4.8908e+00, -2.3948e-01,\n",
       "                       1.4079e-01,  5.9516e-02,  4.1544e+00,  4.6132e+00,  4.9324e+00,\n",
       "                       4.5044e+00,  4.4572e+00,  2.2034e+00,  5.0269e+00,  5.2112e+00,\n",
       "                       4.5265e+00])),\n",
       "             ('entity_predictions.transform.LayerNorm.bias',\n",
       "              tensor([-2.7774e-01, -3.8438e-02,  2.3855e-01,  4.6022e-02, -1.6221e-01,\n",
       "                      -5.7713e-01,  3.4188e-01,  9.4594e-02, -1.0331e-01,  7.0782e-02,\n",
       "                      -1.3103e-01,  1.2240e+00, -9.9986e-02,  4.2073e-01, -2.6909e-02,\n",
       "                       1.4353e-01,  5.7309e-01,  6.2463e-01, -1.4312e-01, -5.1888e-02,\n",
       "                      -2.9614e-02, -5.0582e-01, -8.1043e-02,  1.8511e-02,  2.0732e-01,\n",
       "                       8.9968e-02,  9.3041e-03, -4.2836e-01, -1.4932e+00,  1.2092e-01,\n",
       "                      -1.0123e-02,  2.4600e-01, -1.2499e-01,  3.2328e-01,  8.4937e-02,\n",
       "                      -2.5967e-02, -1.4407e-01, -1.1573e-02,  2.8829e-02,  3.0070e-01,\n",
       "                       1.5912e-01,  8.5161e-02,  1.4328e-01, -4.3643e-01,  2.0789e+00,\n",
       "                       4.9977e-01, -2.2916e-02,  2.1736e-02, -7.5447e-01, -1.5208e-01,\n",
       "                      -2.2516e-01, -4.6510e-02, -3.5065e-03,  1.0505e+00, -2.4636e-01,\n",
       "                      -4.4190e-02,  2.1336e+00,  4.7657e-02,  2.4684e-01, -7.6994e-02,\n",
       "                      -1.6399e-01, -1.2959e-01,  2.4702e-01,  1.5207e-01, -2.0745e-01,\n",
       "                      -3.3928e-01, -1.1838e-02, -1.6943e-01,  1.5920e-02, -1.5477e-01,\n",
       "                       2.4679e-01,  8.0064e-01,  2.7838e-01,  2.3924e-01,  8.8776e-01,\n",
       "                       1.5282e-03,  2.5879e-01,  2.0924e-01, -3.1632e-01,  1.3426e-01,\n",
       "                      -6.5566e-02,  4.7118e-01,  1.8527e-01,  4.2432e-02,  2.4828e-01,\n",
       "                       4.1704e-02,  1.0916e-01, -4.8712e-01,  4.7757e-02, -1.2954e-01,\n",
       "                       1.8162e-01,  1.2366e-01,  5.1122e-01, -5.5167e-03,  1.2945e-01,\n",
       "                       1.5622e+00,  1.0481e-01,  2.2553e-02, -1.2705e+00,  2.1997e+00,\n",
       "                       1.2466e-01, -2.3305e-01,  6.1977e-01,  2.1675e-02,  1.5717e+00,\n",
       "                       1.3647e-01, -3.1754e-02, -4.5348e-01,  1.3289e+00,  6.7609e-01,\n",
       "                      -2.0827e-01, -3.7966e-02,  3.3640e-01,  7.0312e-01,  1.8389e-02,\n",
       "                       3.2644e-01, -1.5307e-01, -3.7261e-01,  3.3291e-01, -5.1798e-01,\n",
       "                      -1.4921e-01, -2.2520e-01, -2.2051e-01,  1.6873e+00,  1.5998e-01,\n",
       "                      -2.9189e-01, -6.9193e-03,  2.8153e-01,  2.0694e-01, -5.2138e-01,\n",
       "                      -3.3818e-01, -2.1025e-01, -1.0708e-02, -6.7162e-02, -3.4202e-01,\n",
       "                      -6.7309e-02, -2.2193e-01, -1.3800e-02,  2.8950e-01, -3.4715e-02,\n",
       "                       6.9542e-01,  1.0013e+00, -3.7437e-01,  4.7156e-02,  1.9295e-02,\n",
       "                      -4.5735e-01, -2.2062e-01, -1.0230e-02,  9.1533e-02,  1.0729e-01,\n",
       "                      -2.6980e-02, -9.9511e-02, -3.0920e-01,  1.6362e-01, -2.2178e-01,\n",
       "                      -5.6160e-01,  1.4187e-01,  9.2510e-01, -1.7300e-01,  6.7722e-01,\n",
       "                       5.8003e-02, -2.8391e-01,  1.9236e+00,  5.0779e-01,  2.7314e-01,\n",
       "                       2.4984e-01,  8.9356e-01, -3.7433e-01,  2.5811e-01, -8.2286e-02,\n",
       "                      -1.5101e-01,  1.9378e+00, -1.2072e-03,  7.6210e-02, -5.9610e-01,\n",
       "                       4.3836e-01, -4.9118e-01,  1.3824e-01,  6.1742e-01,  2.9730e-01,\n",
       "                       7.8849e-01, -3.9821e-01,  8.0370e-01,  3.0779e-01,  1.3836e-01,\n",
       "                       1.4983e-01, -1.7851e-02, -3.1360e-01, -5.1848e-01, -3.7332e-01,\n",
       "                       7.6517e-01,  2.1501e-01,  1.8103e-02,  7.8951e-01,  4.7290e-01,\n",
       "                      -3.1915e-02,  4.5620e-01, -1.6021e-01, -1.6265e-01,  7.6533e-02,\n",
       "                       1.5655e-01,  2.1463e-01, -9.7485e-02,  9.1846e-01,  4.0264e-02,\n",
       "                      -2.1060e-02, -1.6400e-02, -6.0199e-01,  3.2977e-02, -1.1057e-01,\n",
       "                       1.1126e-01, -4.0873e-01, -1.2280e-01, -2.6308e-01,  1.6386e+00,\n",
       "                       1.6909e+00,  8.9900e-02,  3.4154e-01,  1.1868e-01, -3.7040e-01,\n",
       "                      -4.9873e-01, -1.9449e-02, -3.2950e-01,  2.5497e-01, -1.0792e+00,\n",
       "                       2.7814e-02, -5.4155e-01, -3.3953e-01, -1.2009e-01,  2.4147e-01,\n",
       "                       1.3828e-01, -4.4101e-01,  4.3984e-01,  7.8583e-02,  1.7349e-01,\n",
       "                       2.3987e-02,  9.9922e-02, -1.9099e-01,  4.8677e-01,  2.4896e-01,\n",
       "                      -2.1151e-01,  1.3106e-01,  1.7442e-01,  8.3241e-02, -1.3238e-01,\n",
       "                       2.1164e-01,  8.9505e-02, -1.5776e-01, -3.0445e-01,  5.3412e-01,\n",
       "                      -1.6167e-01,  9.6001e-02,  1.4137e+00,  3.2742e-01,  2.0150e-01,\n",
       "                       7.1712e-02])),\n",
       "             ('entity_predictions.decoder.weight',\n",
       "              tensor([[-0.0420, -0.0610, -0.0482,  ..., -0.0336, -0.0534, -0.0462],\n",
       "                      [ 0.0699,  0.0621,  0.0855,  ...,  0.0551,  0.0466,  0.0777],\n",
       "                      [ 0.0074, -0.0067,  0.0015,  ..., -0.0110, -0.0052, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0780, -0.0544, -0.0635,  ...,  0.0061, -0.0908, -0.0801],\n",
       "                      [-0.0913, -0.0692, -0.0123,  ..., -0.1385, -0.0340, -0.0620],\n",
       "                      [ 0.0124, -0.0218, -0.0023,  ..., -0.1331, -0.0721, -0.0953]]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pytorch_luke ori\n",
    "torch_model = torch.load(\"./torch2ms/pytorch_model.bin\",map_location='cpu')# luke github官方上面提供的squad微调后权重\n",
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50265, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.query.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.key.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.key.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.value.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.value.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.w2e_query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.w2e_query.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.e2w_query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.e2w_query.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.e2e_query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.self.e2e_query.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.output.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.output.dense.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.output.LayerNorm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.attention.output.LayerNorm.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1024])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.intermediate.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.intermediate.dense.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.output.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.output.dense.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.output.LayerNorm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model['encoder.layer.0.output.LayerNorm.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for weight_name, weight_value in torch_model.items():\n",
    "#         print(weight_value.shape)\n",
    "#         y=weight_value.numpy()\n",
    "#         print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6693:140567675840320,MainProcess):2021-10-26-11:15:57.114.192 [mindspore/run_check/_check_version.py:181] Cuda ['10.1', '11.1'] version(need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(6693:140567675840320,MainProcess):2021-10-26-11:15:57.158.977 [mindspore/run_check/_check_version.py:181] Cuda ['10.1', '11.1'] version(need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, please refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "from readingcomprehension.models.luke import LukeForReadingComprehension\n",
    "import mindspore.common.dtype as mstype\n",
    "from model.bert_model import BertConfig\n",
    "from mindspore import context\n",
    "from model.luke import LukeModel, EntityAwareEncoder\n",
    "import numpy as np\n",
    "from mindspore import Tensor, context\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "from model.bert_model import BertOutput\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "import math\n",
    "from mindspore.ops import composite as C\n",
    "import mindspore\n",
    "from mindspore.ops import operations as P\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "model = LukeForReadingComprehension(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('luke.embeddings.word_embeddings.embedding_table',\n",
       "              Parameter (name=luke.embeddings.word_embeddings.embedding_table, shape=(50265, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.embeddings.position_embeddings.embedding_table',\n",
       "              Parameter (name=luke.embeddings.position_embeddings.embedding_table, shape=(514, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.embeddings.token_type_embeddings.embedding_table',\n",
       "              Parameter (name=luke.embeddings.token_type_embeddings.embedding_table, shape=(1, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.embeddings.layer_norm.gamma',\n",
       "              Parameter (name=luke.embeddings.layer_norm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.embeddings.layer_norm.beta',\n",
       "              Parameter (name=luke.embeddings.layer_norm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.entity_embeddings.embedding_table',\n",
       "              Parameter (name=luke.entity_embeddings.entity_embeddings.embedding_table, shape=(500000, 256), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.entity_embedding_dense.weight',\n",
       "              Parameter (name=luke.entity_embeddings.entity_embedding_dense.weight, shape=(1024, 256), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.position_embeddings.embedding_table',\n",
       "              Parameter (name=luke.entity_embeddings.position_embeddings.embedding_table, shape=(514, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.token_type_embeddings.embedding_table',\n",
       "              Parameter (name=luke.entity_embeddings.token_type_embeddings.embedding_table, shape=(1, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.layer_norm.gamma',\n",
       "              Parameter (name=luke.entity_embeddings.layer_norm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.entity_embeddings.layer_norm.beta',\n",
       "              Parameter (name=luke.entity_embeddings.layer_norm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.0.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.0.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.0.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.0.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.0.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.0.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.1.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.1.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.1.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.1.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.1.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.1.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.2.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.2.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.2.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.2.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.2.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.2.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.3.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.3.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.3.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.3.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.3.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.3.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.4.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.4.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.4.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.4.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.4.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.4.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.5.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.5.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.5.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.5.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.5.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.5.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.6.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.6.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.6.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.6.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.6.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.6.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.7.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.7.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.7.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.7.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.7.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.7.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.8.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.8.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.8.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.8.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.8.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.8.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.9.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.9.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.9.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.9.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.9.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.9.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.10.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.10.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.10.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.10.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.10.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.10.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.11.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.11.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.11.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.11.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.11.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.11.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.12.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.12.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.12.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.12.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.12.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.12.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.13.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.13.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.13.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.13.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.13.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.13.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.14.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.14.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.14.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.14.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.14.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.14.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.15.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.15.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.15.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.15.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.15.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.15.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.16.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.16.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.16.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.16.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.16.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.16.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.17.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.17.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.17.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.17.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.17.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.17.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.18.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.18.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.18.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.18.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.18.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.18.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.19.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.19.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.19.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.19.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.19.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.19.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.20.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.20.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.20.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.20.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.20.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.20.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.21.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.21.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.21.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.21.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.21.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.21.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.22.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.22.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.22.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.22.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.22.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.22.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.query.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.query.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.key.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.key.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.key.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.key.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.value.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.value.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.value.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.value.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.w2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.w2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.w2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.w2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.e2w_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.e2w_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.e2w_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.e2w_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.e2e_query.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.e2e_query.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.self_attention.e2e_query.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.self_attention.e2e_query.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.output.dense.weight, shape=(1024, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.attention.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.23.attention.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.intermediate.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.intermediate.weight, shape=(4096, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.intermediate.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.intermediate.bias, shape=(4096,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.output.dense.weight',\n",
       "              Parameter (name=luke.encoder.layer.23.output.dense.weight, shape=(1024, 4096), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.output.dense.bias',\n",
       "              Parameter (name=luke.encoder.layer.23.output.dense.bias, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.output.layernorm.gamma',\n",
       "              Parameter (name=luke.encoder.layer.23.output.layernorm.gamma, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('luke.encoder.layer.23.output.layernorm.beta',\n",
       "              Parameter (name=luke.encoder.layer.23.output.layernorm.beta, shape=(1024,), dtype=Float32, requires_grad=True)),\n",
       "             ('qa_outputs.weight',\n",
       "              Parameter (name=qa_outputs.weight, shape=(2, 1024), dtype=Float32, requires_grad=True)),\n",
       "             ('qa_outputs.bias',\n",
       "              Parameter (name=qa_outputs.bias, shape=(2,), dtype=Float32, requires_grad=True))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_model = model.parameters_dict()\n",
    "ms_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embeddings.word_embeddings.embedding_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10525/3122517995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mms_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings.word_embeddings.embedding_table'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'embeddings.word_embeddings.embedding_table'"
     ]
    }
   ],
   "source": [
    "ms_model['embeddings.word_embeddings.embedding_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model['encoder.layer.0.output.dense.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch2ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from mindspore import log as logger\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_params_map(layer_num=24):\n",
    "    \"\"\"\n",
    "    build params map from torch's LUKE to mindspore's LUKE\n",
    "    map=> key：value，torch_name：ms_name\n",
    "    键：torch权重名称，值：mindspore权重名称\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    weight_map = collections.OrderedDict({\n",
    "        'embeddings.word_embeddings.weight': \"luke.embeddings.word_embeddings.embedding_table\",\n",
    "        'embeddings.position_embeddings.weight': \"luke.embeddings.position_embeddings.embedding_table\",\n",
    "        'embeddings.token_type_embeddings.weight': \"luke.embeddings.token_type_embeddings.embedding_table\",\n",
    "        'embeddings.LayerNorm.weight': 'luke.embeddings.layer_norm.gamma',\n",
    "        'embeddings.LayerNorm.bias': 'luke.embeddings.layer_norm.beta',\n",
    "        'entity_embeddings.entity_embeddings.weight':'luke.entity_embeddings.entity_embeddings.embedding_table',\n",
    "        'entity_embeddings.entity_embedding_dense.weight':'luke.entity_embeddings.entity_embedding_dense.weight',\n",
    "        'entity_embeddings.position_embeddings.weight':'luke.entity_embeddings.position_embeddings.embedding_table',\n",
    "        'entity_embeddings.token_type_embeddings.weight':'luke.entity_embeddings.token_type_embeddings.embedding_table',\n",
    "        'entity_embeddings.LayerNorm.weight':'luke.entity_embeddings.layer_norm.gamma',\n",
    "        'entity_embeddings.LayerNorm.bias':'luke.entity_embeddings.layer_norm.beta',\n",
    "        'qa_outputs.weight':'qa_outputs.weight',\n",
    "        'qa_outputs.bias':'qa_outputs.bias',\n",
    "#         'pooler.dense.weight':'pooler.weight',\n",
    "#         'pooler.dense.bias':'pooler.bias'\n",
    "        \n",
    "    })\n",
    "    \n",
    "    # add attention layers\n",
    "    for i in range(layer_num):\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.query.weight'] = \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.query.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.query.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.query.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.key.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.key.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.key.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.key.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.value.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.value.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.value.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.value.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.w2e_query.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.w2e_query.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.w2e_query.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.w2e_query.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.e2w_query.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.e2w_query.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.e2w_query.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.e2w_query.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.e2e_query.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.e2e_query.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.self.e2e_query.bias']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.self_attention.e2e_query.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.output.dense.weight']= \\\n",
    "            f'luke.encoder.layer.{i}.attention.output.dense.weight'\n",
    "        weight_map[f'encoder.layer.{i}.attention.output.dense.bias'] = \\\n",
    "            f'luke.encoder.layer.{i}.attention.output.dense.bias'\n",
    "        weight_map[f'encoder.layer.{i}.attention.output.LayerNorm.weight'] = \\\n",
    "            f'luke.encoder.layer.{i}.attention.output.layernorm.gamma'\n",
    "        weight_map[f'encoder.layer.{i}.attention.output.LayerNorm.bias'] = \\\n",
    "            f'luke.encoder.layer.{i}.attention.output.layernorm.beta'\n",
    "        weight_map[f'encoder.layer.{i}.intermediate.dense.weight'] = \\\n",
    "            f'luke.encoder.layer.{i}.intermediate.weight'\n",
    "        weight_map[f'encoder.layer.{i}.intermediate.dense.bias'] = \\\n",
    "            f'luke.encoder.layer.{i}.intermediate.bias'\n",
    "        weight_map[f'encoder.layer.{i}.output.dense.weight'] = \\\n",
    "            f'luke.encoder.layer.{i}.output.dense.weight'\n",
    "        weight_map[f'encoder.layer.{i}.output.dense.bias'] = \\\n",
    "            f'luke.encoder.layer.{i}.output.dense.bias'\n",
    "        weight_map[f'encoder.layer.{i}.output.LayerNorm.weight'] = \\\n",
    "            f'luke.encoder.layer.{i}.output.layernorm.gamma'\n",
    "        weight_map[f'encoder.layer.{i}.output.LayerNorm.bias'] = \\\n",
    "            f'luke.encoder.layer.{i}.output.layernorm.beta'\n",
    "    # add pooler\n",
    "#     weight_map.update(\n",
    "#         {\n",
    "#             'pooled_fc.w_0': 'ernie.ernie.dense.weight',\n",
    "#             'pooled_fc.b_0': 'ernie.ernie.dense.bias',\n",
    "#             'cls_out_w': 'ernie.dense_1.weight',\n",
    "#             'cls_out_b': 'ernie.dense_1.bias'\n",
    "#         }\n",
    "#    )\n",
    "    return weight_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_param(param, new_param):\n",
    "    \"\"\"Updates param's data from new_param's data.\"\"\"\n",
    "\n",
    "    if isinstance(param.data, Tensor) and isinstance(new_param.data, Tensor):\n",
    "        if param.data.dtype != new_param.data.dtype:\n",
    "            logger.error(\"Failed to combine the net and the parameters for param %s.\", param.name)\n",
    "            msg = (\"Net parameters {} type({}) different from parameter_dict's({})\"\n",
    "                   .format(param.name, param.data.dtype, new_param.data.dtype))\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        if param.data.shape != new_param.data.shape:\n",
    "            if not _special_process_par(param, new_param):\n",
    "                logger.error(\"Failed to combine the net and the parameters for param %s.\", param.name)\n",
    "                msg = (\"Net parameters {} shape({}) different from parameter_dict's({})\"\n",
    "                       .format(param.name, param.data.shape, new_param.data.shape))\n",
    "                raise RuntimeError(msg)\n",
    "            return\n",
    "\n",
    "        param.set_data(new_param.data)\n",
    "        return\n",
    "\n",
    "    if isinstance(param.data, Tensor) and not isinstance(new_param.data, Tensor):\n",
    "        if param.data.shape != (1,) and param.data.shape != ():\n",
    "            logger.error(\"Failed to combine the net and the parameters for param %s.\", param.name)\n",
    "            msg = (\"Net parameters {} shape({}) is not (1,), inconsistent with parameter_dict's(scalar).\"\n",
    "                   .format(param.name, param.data.shape))\n",
    "            raise RuntimeError(msg)\n",
    "        param.set_data(initializer(new_param.data, param.data.shape, param.data.dtype))\n",
    "\n",
    "    elif isinstance(new_param.data, Tensor) and not isinstance(param.data, Tensor):\n",
    "        logger.error(\"Failed to combine the net and the parameters for param %s.\", param.name)\n",
    "        msg = (\"Net parameters {} type({}) different from parameter_dict's({})\"\n",
    "               .format(param.name, type(param.data), type(new_param.data)))\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "    else:\n",
    "        param.set_data(type(param.data)(new_param.data))\n",
    "\n",
    "\n",
    "def _special_process_par(par, new_par):\n",
    "    \"\"\"\n",
    "    Processes the special condition.\n",
    "\n",
    "    Like (12,2048,1,1)->(12,2048), this case is caused by GE 4 dimensions tensor.\n",
    "    \"\"\"\n",
    "    par_shape_len = len(par.data.shape)\n",
    "    new_par_shape_len = len(new_par.data.shape)\n",
    "    delta_len = new_par_shape_len - par_shape_len\n",
    "    delta_i = 0\n",
    "    for delta_i in range(delta_len):\n",
    "        if new_par.data.shape[par_shape_len + delta_i] != 1:\n",
    "            break\n",
    "    if delta_i == delta_len - 1:\n",
    "        new_val = new_par.data.asnumpy()\n",
    "        new_val = new_val.reshape(par.data.shape)\n",
    "        par.set_data(Tensor(new_val, par.data.dtype))\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert(torch_model, ms_model):\n",
    "    \"\"\"extract weights and convert to mindspore\"\"\"\n",
    "    print('=' * 20 + 'extract weights' + '=' * 20)\n",
    "    state_dict = []\n",
    "    weight_map = build_params_map(layer_num=24)\n",
    "    \n",
    "    for weight_name, weight_value in torch_model.items():\n",
    "        if weight_name not in weight_map.keys():\n",
    "            continue\n",
    "       \n",
    "        state_dict.append({'name': weight_map[weight_name], 'data': Tensor(weight_value.numpy())})\n",
    "        value = Parameter(Tensor(weight_value.numpy()),name=weight_map[weight_name])\n",
    "        key = ms_model[weight_map[weight_name]]\n",
    "        _update_param(key, value)\n",
    "        print(weight_name, '->', weight_map[weight_name], weight_value.shape)\n",
    "        \n",
    "    # save_checkpoint(model, os.path.join(\"./luke-large-qa.ckpt\"))\n",
    "    save_checkpoint(model, os.path.join(\"./luke-large.ckpt\"))\n",
    "    print('=' * 20 + 'extract weights finished' + '=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================extract weights====================\n",
      "encoder.layer.0.attention.self.query.weight -> luke.encoder.layer.0.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.self.query.bias -> luke.encoder.layer.0.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.0.attention.self.key.weight -> luke.encoder.layer.0.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.self.key.bias -> luke.encoder.layer.0.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.0.attention.self.value.weight -> luke.encoder.layer.0.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.self.value.bias -> luke.encoder.layer.0.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.0.attention.output.dense.weight -> luke.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.output.dense.bias -> luke.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight -> luke.encoder.layer.0.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias -> luke.encoder.layer.0.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.0.intermediate.dense.weight -> luke.encoder.layer.0.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.0.intermediate.dense.bias -> luke.encoder.layer.0.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.0.output.dense.weight -> luke.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.0.output.dense.bias -> luke.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.0.output.LayerNorm.weight -> luke.encoder.layer.0.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.0.output.LayerNorm.bias -> luke.encoder.layer.0.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.1.attention.self.query.weight -> luke.encoder.layer.1.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.self.query.bias -> luke.encoder.layer.1.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.1.attention.self.key.weight -> luke.encoder.layer.1.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.self.key.bias -> luke.encoder.layer.1.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.1.attention.self.value.weight -> luke.encoder.layer.1.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.self.value.bias -> luke.encoder.layer.1.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.1.attention.output.dense.weight -> luke.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.output.dense.bias -> luke.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight -> luke.encoder.layer.1.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias -> luke.encoder.layer.1.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.1.intermediate.dense.weight -> luke.encoder.layer.1.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.1.intermediate.dense.bias -> luke.encoder.layer.1.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.1.output.dense.weight -> luke.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.1.output.dense.bias -> luke.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.1.output.LayerNorm.weight -> luke.encoder.layer.1.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.1.output.LayerNorm.bias -> luke.encoder.layer.1.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.2.attention.self.query.weight -> luke.encoder.layer.2.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.self.query.bias -> luke.encoder.layer.2.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.2.attention.self.key.weight -> luke.encoder.layer.2.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.self.key.bias -> luke.encoder.layer.2.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.2.attention.self.value.weight -> luke.encoder.layer.2.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.self.value.bias -> luke.encoder.layer.2.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.2.attention.output.dense.weight -> luke.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.output.dense.bias -> luke.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight -> luke.encoder.layer.2.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias -> luke.encoder.layer.2.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.2.intermediate.dense.weight -> luke.encoder.layer.2.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.2.intermediate.dense.bias -> luke.encoder.layer.2.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.2.output.dense.weight -> luke.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.2.output.dense.bias -> luke.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.2.output.LayerNorm.weight -> luke.encoder.layer.2.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.2.output.LayerNorm.bias -> luke.encoder.layer.2.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.3.attention.self.query.weight -> luke.encoder.layer.3.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.self.query.bias -> luke.encoder.layer.3.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.3.attention.self.key.weight -> luke.encoder.layer.3.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.self.key.bias -> luke.encoder.layer.3.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.3.attention.self.value.weight -> luke.encoder.layer.3.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.self.value.bias -> luke.encoder.layer.3.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.3.attention.output.dense.weight -> luke.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.output.dense.bias -> luke.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight -> luke.encoder.layer.3.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias -> luke.encoder.layer.3.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.3.intermediate.dense.weight -> luke.encoder.layer.3.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.3.intermediate.dense.bias -> luke.encoder.layer.3.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.3.output.dense.weight -> luke.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.3.output.dense.bias -> luke.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.3.output.LayerNorm.weight -> luke.encoder.layer.3.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.3.output.LayerNorm.bias -> luke.encoder.layer.3.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.4.attention.self.query.weight -> luke.encoder.layer.4.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.self.query.bias -> luke.encoder.layer.4.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.4.attention.self.key.weight -> luke.encoder.layer.4.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.self.key.bias -> luke.encoder.layer.4.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.4.attention.self.value.weight -> luke.encoder.layer.4.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.self.value.bias -> luke.encoder.layer.4.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.4.attention.output.dense.weight -> luke.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.output.dense.bias -> luke.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight -> luke.encoder.layer.4.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias -> luke.encoder.layer.4.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.4.intermediate.dense.weight -> luke.encoder.layer.4.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.4.intermediate.dense.bias -> luke.encoder.layer.4.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.4.output.dense.weight -> luke.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.4.output.dense.bias -> luke.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.4.output.LayerNorm.weight -> luke.encoder.layer.4.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.4.output.LayerNorm.bias -> luke.encoder.layer.4.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.5.attention.self.query.weight -> luke.encoder.layer.5.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.self.query.bias -> luke.encoder.layer.5.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.5.attention.self.key.weight -> luke.encoder.layer.5.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.self.key.bias -> luke.encoder.layer.5.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.5.attention.self.value.weight -> luke.encoder.layer.5.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.self.value.bias -> luke.encoder.layer.5.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.5.attention.output.dense.weight -> luke.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.output.dense.bias -> luke.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight -> luke.encoder.layer.5.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias -> luke.encoder.layer.5.attention.output.layernorm.beta torch.Size([1024])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.5.intermediate.dense.weight -> luke.encoder.layer.5.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.5.intermediate.dense.bias -> luke.encoder.layer.5.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.5.output.dense.weight -> luke.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.5.output.dense.bias -> luke.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.5.output.LayerNorm.weight -> luke.encoder.layer.5.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.5.output.LayerNorm.bias -> luke.encoder.layer.5.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.6.attention.self.query.weight -> luke.encoder.layer.6.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.self.query.bias -> luke.encoder.layer.6.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.6.attention.self.key.weight -> luke.encoder.layer.6.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.self.key.bias -> luke.encoder.layer.6.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.6.attention.self.value.weight -> luke.encoder.layer.6.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.self.value.bias -> luke.encoder.layer.6.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.6.attention.output.dense.weight -> luke.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.output.dense.bias -> luke.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight -> luke.encoder.layer.6.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias -> luke.encoder.layer.6.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.6.intermediate.dense.weight -> luke.encoder.layer.6.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.6.intermediate.dense.bias -> luke.encoder.layer.6.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.6.output.dense.weight -> luke.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.6.output.dense.bias -> luke.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.6.output.LayerNorm.weight -> luke.encoder.layer.6.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.6.output.LayerNorm.bias -> luke.encoder.layer.6.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.7.attention.self.query.weight -> luke.encoder.layer.7.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.self.query.bias -> luke.encoder.layer.7.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.7.attention.self.key.weight -> luke.encoder.layer.7.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.self.key.bias -> luke.encoder.layer.7.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.7.attention.self.value.weight -> luke.encoder.layer.7.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.self.value.bias -> luke.encoder.layer.7.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.7.attention.output.dense.weight -> luke.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.output.dense.bias -> luke.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight -> luke.encoder.layer.7.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias -> luke.encoder.layer.7.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.7.intermediate.dense.weight -> luke.encoder.layer.7.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.7.intermediate.dense.bias -> luke.encoder.layer.7.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.7.output.dense.weight -> luke.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.7.output.dense.bias -> luke.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.7.output.LayerNorm.weight -> luke.encoder.layer.7.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.7.output.LayerNorm.bias -> luke.encoder.layer.7.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.8.attention.self.query.weight -> luke.encoder.layer.8.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.self.query.bias -> luke.encoder.layer.8.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.8.attention.self.key.weight -> luke.encoder.layer.8.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.self.key.bias -> luke.encoder.layer.8.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.8.attention.self.value.weight -> luke.encoder.layer.8.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.self.value.bias -> luke.encoder.layer.8.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.8.attention.output.dense.weight -> luke.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.output.dense.bias -> luke.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight -> luke.encoder.layer.8.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias -> luke.encoder.layer.8.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.8.intermediate.dense.weight -> luke.encoder.layer.8.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.8.intermediate.dense.bias -> luke.encoder.layer.8.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.8.output.dense.weight -> luke.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.8.output.dense.bias -> luke.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.8.output.LayerNorm.weight -> luke.encoder.layer.8.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.8.output.LayerNorm.bias -> luke.encoder.layer.8.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.9.attention.self.query.weight -> luke.encoder.layer.9.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.self.query.bias -> luke.encoder.layer.9.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.9.attention.self.key.weight -> luke.encoder.layer.9.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.self.key.bias -> luke.encoder.layer.9.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.9.attention.self.value.weight -> luke.encoder.layer.9.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.self.value.bias -> luke.encoder.layer.9.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.9.attention.output.dense.weight -> luke.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.output.dense.bias -> luke.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight -> luke.encoder.layer.9.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias -> luke.encoder.layer.9.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.9.intermediate.dense.weight -> luke.encoder.layer.9.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.9.intermediate.dense.bias -> luke.encoder.layer.9.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.9.output.dense.weight -> luke.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.9.output.dense.bias -> luke.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.9.output.LayerNorm.weight -> luke.encoder.layer.9.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.9.output.LayerNorm.bias -> luke.encoder.layer.9.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.10.attention.self.query.weight -> luke.encoder.layer.10.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.self.query.bias -> luke.encoder.layer.10.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.10.attention.self.key.weight -> luke.encoder.layer.10.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.self.key.bias -> luke.encoder.layer.10.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.10.attention.self.value.weight -> luke.encoder.layer.10.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.self.value.bias -> luke.encoder.layer.10.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.10.attention.output.dense.weight -> luke.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.output.dense.bias -> luke.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight -> luke.encoder.layer.10.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias -> luke.encoder.layer.10.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.10.intermediate.dense.weight -> luke.encoder.layer.10.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.10.intermediate.dense.bias -> luke.encoder.layer.10.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.10.output.dense.weight -> luke.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.10.output.dense.bias -> luke.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.10.output.LayerNorm.weight -> luke.encoder.layer.10.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.10.output.LayerNorm.bias -> luke.encoder.layer.10.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.11.attention.self.query.weight -> luke.encoder.layer.11.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.self.query.bias -> luke.encoder.layer.11.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.11.attention.self.key.weight -> luke.encoder.layer.11.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.self.key.bias -> luke.encoder.layer.11.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.11.attention.self.value.weight -> luke.encoder.layer.11.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.self.value.bias -> luke.encoder.layer.11.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.11.attention.output.dense.weight -> luke.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.output.dense.bias -> luke.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight -> luke.encoder.layer.11.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias -> luke.encoder.layer.11.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.11.intermediate.dense.weight -> luke.encoder.layer.11.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.11.intermediate.dense.bias -> luke.encoder.layer.11.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.11.output.dense.weight -> luke.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.11.output.dense.bias -> luke.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.11.output.LayerNorm.weight -> luke.encoder.layer.11.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.11.output.LayerNorm.bias -> luke.encoder.layer.11.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.12.attention.self.query.weight -> luke.encoder.layer.12.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.12.attention.self.query.bias -> luke.encoder.layer.12.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.12.attention.self.key.weight -> luke.encoder.layer.12.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.12.attention.self.key.bias -> luke.encoder.layer.12.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.12.attention.self.value.weight -> luke.encoder.layer.12.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.12.attention.self.value.bias -> luke.encoder.layer.12.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.12.attention.output.dense.weight -> luke.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.12.attention.output.dense.bias -> luke.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.12.attention.output.LayerNorm.weight -> luke.encoder.layer.12.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.12.attention.output.LayerNorm.bias -> luke.encoder.layer.12.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.12.intermediate.dense.weight -> luke.encoder.layer.12.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.12.intermediate.dense.bias -> luke.encoder.layer.12.intermediate.bias torch.Size([4096])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.12.output.dense.weight -> luke.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.12.output.dense.bias -> luke.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.12.output.LayerNorm.weight -> luke.encoder.layer.12.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.12.output.LayerNorm.bias -> luke.encoder.layer.12.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.13.attention.self.query.weight -> luke.encoder.layer.13.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.13.attention.self.query.bias -> luke.encoder.layer.13.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.13.attention.self.key.weight -> luke.encoder.layer.13.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.13.attention.self.key.bias -> luke.encoder.layer.13.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.13.attention.self.value.weight -> luke.encoder.layer.13.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.13.attention.self.value.bias -> luke.encoder.layer.13.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.13.attention.output.dense.weight -> luke.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.13.attention.output.dense.bias -> luke.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.13.attention.output.LayerNorm.weight -> luke.encoder.layer.13.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.13.attention.output.LayerNorm.bias -> luke.encoder.layer.13.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.13.intermediate.dense.weight -> luke.encoder.layer.13.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.13.intermediate.dense.bias -> luke.encoder.layer.13.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.13.output.dense.weight -> luke.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.13.output.dense.bias -> luke.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.13.output.LayerNorm.weight -> luke.encoder.layer.13.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.13.output.LayerNorm.bias -> luke.encoder.layer.13.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.14.attention.self.query.weight -> luke.encoder.layer.14.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.14.attention.self.query.bias -> luke.encoder.layer.14.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.14.attention.self.key.weight -> luke.encoder.layer.14.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.14.attention.self.key.bias -> luke.encoder.layer.14.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.14.attention.self.value.weight -> luke.encoder.layer.14.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.14.attention.self.value.bias -> luke.encoder.layer.14.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.14.attention.output.dense.weight -> luke.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.14.attention.output.dense.bias -> luke.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.14.attention.output.LayerNorm.weight -> luke.encoder.layer.14.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.14.attention.output.LayerNorm.bias -> luke.encoder.layer.14.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.14.intermediate.dense.weight -> luke.encoder.layer.14.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.14.intermediate.dense.bias -> luke.encoder.layer.14.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.14.output.dense.weight -> luke.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.14.output.dense.bias -> luke.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.14.output.LayerNorm.weight -> luke.encoder.layer.14.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.14.output.LayerNorm.bias -> luke.encoder.layer.14.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.15.attention.self.query.weight -> luke.encoder.layer.15.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.15.attention.self.query.bias -> luke.encoder.layer.15.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.15.attention.self.key.weight -> luke.encoder.layer.15.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.15.attention.self.key.bias -> luke.encoder.layer.15.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.15.attention.self.value.weight -> luke.encoder.layer.15.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.15.attention.self.value.bias -> luke.encoder.layer.15.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.15.attention.output.dense.weight -> luke.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.15.attention.output.dense.bias -> luke.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.15.attention.output.LayerNorm.weight -> luke.encoder.layer.15.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.15.attention.output.LayerNorm.bias -> luke.encoder.layer.15.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.15.intermediate.dense.weight -> luke.encoder.layer.15.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.15.intermediate.dense.bias -> luke.encoder.layer.15.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.15.output.dense.weight -> luke.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.15.output.dense.bias -> luke.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.15.output.LayerNorm.weight -> luke.encoder.layer.15.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.15.output.LayerNorm.bias -> luke.encoder.layer.15.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.16.attention.self.query.weight -> luke.encoder.layer.16.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.16.attention.self.query.bias -> luke.encoder.layer.16.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.16.attention.self.key.weight -> luke.encoder.layer.16.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.16.attention.self.key.bias -> luke.encoder.layer.16.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.16.attention.self.value.weight -> luke.encoder.layer.16.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.16.attention.self.value.bias -> luke.encoder.layer.16.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.16.attention.output.dense.weight -> luke.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.16.attention.output.dense.bias -> luke.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.16.attention.output.LayerNorm.weight -> luke.encoder.layer.16.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.16.attention.output.LayerNorm.bias -> luke.encoder.layer.16.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.16.intermediate.dense.weight -> luke.encoder.layer.16.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.16.intermediate.dense.bias -> luke.encoder.layer.16.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.16.output.dense.weight -> luke.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.16.output.dense.bias -> luke.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.16.output.LayerNorm.weight -> luke.encoder.layer.16.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.16.output.LayerNorm.bias -> luke.encoder.layer.16.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.17.attention.self.query.weight -> luke.encoder.layer.17.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.17.attention.self.query.bias -> luke.encoder.layer.17.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.17.attention.self.key.weight -> luke.encoder.layer.17.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.17.attention.self.key.bias -> luke.encoder.layer.17.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.17.attention.self.value.weight -> luke.encoder.layer.17.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.17.attention.self.value.bias -> luke.encoder.layer.17.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.17.attention.output.dense.weight -> luke.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.17.attention.output.dense.bias -> luke.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.17.attention.output.LayerNorm.weight -> luke.encoder.layer.17.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.17.attention.output.LayerNorm.bias -> luke.encoder.layer.17.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.17.intermediate.dense.weight -> luke.encoder.layer.17.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.17.intermediate.dense.bias -> luke.encoder.layer.17.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.17.output.dense.weight -> luke.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.17.output.dense.bias -> luke.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.17.output.LayerNorm.weight -> luke.encoder.layer.17.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.17.output.LayerNorm.bias -> luke.encoder.layer.17.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.18.attention.self.query.weight -> luke.encoder.layer.18.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.18.attention.self.query.bias -> luke.encoder.layer.18.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.18.attention.self.key.weight -> luke.encoder.layer.18.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.18.attention.self.key.bias -> luke.encoder.layer.18.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.18.attention.self.value.weight -> luke.encoder.layer.18.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.18.attention.self.value.bias -> luke.encoder.layer.18.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.18.attention.output.dense.weight -> luke.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.18.attention.output.dense.bias -> luke.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.18.attention.output.LayerNorm.weight -> luke.encoder.layer.18.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.18.attention.output.LayerNorm.bias -> luke.encoder.layer.18.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.18.intermediate.dense.weight -> luke.encoder.layer.18.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.18.intermediate.dense.bias -> luke.encoder.layer.18.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.18.output.dense.weight -> luke.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.18.output.dense.bias -> luke.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.18.output.LayerNorm.weight -> luke.encoder.layer.18.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.18.output.LayerNorm.bias -> luke.encoder.layer.18.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.19.attention.self.query.weight -> luke.encoder.layer.19.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.19.attention.self.query.bias -> luke.encoder.layer.19.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.19.attention.self.key.weight -> luke.encoder.layer.19.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.19.attention.self.key.bias -> luke.encoder.layer.19.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.19.attention.self.value.weight -> luke.encoder.layer.19.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.19.attention.self.value.bias -> luke.encoder.layer.19.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.19.attention.output.dense.weight -> luke.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.19.attention.output.dense.bias -> luke.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.19.attention.output.LayerNorm.weight -> luke.encoder.layer.19.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.19.attention.output.LayerNorm.bias -> luke.encoder.layer.19.attention.output.layernorm.beta torch.Size([1024])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.19.intermediate.dense.weight -> luke.encoder.layer.19.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.19.intermediate.dense.bias -> luke.encoder.layer.19.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.19.output.dense.weight -> luke.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.19.output.dense.bias -> luke.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.19.output.LayerNorm.weight -> luke.encoder.layer.19.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.19.output.LayerNorm.bias -> luke.encoder.layer.19.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.20.attention.self.query.weight -> luke.encoder.layer.20.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.20.attention.self.query.bias -> luke.encoder.layer.20.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.20.attention.self.key.weight -> luke.encoder.layer.20.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.20.attention.self.key.bias -> luke.encoder.layer.20.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.20.attention.self.value.weight -> luke.encoder.layer.20.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.20.attention.self.value.bias -> luke.encoder.layer.20.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.20.attention.output.dense.weight -> luke.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.20.attention.output.dense.bias -> luke.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.20.attention.output.LayerNorm.weight -> luke.encoder.layer.20.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.20.attention.output.LayerNorm.bias -> luke.encoder.layer.20.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.20.intermediate.dense.weight -> luke.encoder.layer.20.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.20.intermediate.dense.bias -> luke.encoder.layer.20.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.20.output.dense.weight -> luke.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.20.output.dense.bias -> luke.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.20.output.LayerNorm.weight -> luke.encoder.layer.20.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.20.output.LayerNorm.bias -> luke.encoder.layer.20.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.21.attention.self.query.weight -> luke.encoder.layer.21.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.21.attention.self.query.bias -> luke.encoder.layer.21.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.21.attention.self.key.weight -> luke.encoder.layer.21.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.21.attention.self.key.bias -> luke.encoder.layer.21.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.21.attention.self.value.weight -> luke.encoder.layer.21.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.21.attention.self.value.bias -> luke.encoder.layer.21.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.21.attention.output.dense.weight -> luke.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.21.attention.output.dense.bias -> luke.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.21.attention.output.LayerNorm.weight -> luke.encoder.layer.21.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.21.attention.output.LayerNorm.bias -> luke.encoder.layer.21.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.21.intermediate.dense.weight -> luke.encoder.layer.21.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.21.intermediate.dense.bias -> luke.encoder.layer.21.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.21.output.dense.weight -> luke.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.21.output.dense.bias -> luke.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.21.output.LayerNorm.weight -> luke.encoder.layer.21.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.21.output.LayerNorm.bias -> luke.encoder.layer.21.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.22.attention.self.query.weight -> luke.encoder.layer.22.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.22.attention.self.query.bias -> luke.encoder.layer.22.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.22.attention.self.key.weight -> luke.encoder.layer.22.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.22.attention.self.key.bias -> luke.encoder.layer.22.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.22.attention.self.value.weight -> luke.encoder.layer.22.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.22.attention.self.value.bias -> luke.encoder.layer.22.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.22.attention.output.dense.weight -> luke.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.22.attention.output.dense.bias -> luke.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.22.attention.output.LayerNorm.weight -> luke.encoder.layer.22.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.22.attention.output.LayerNorm.bias -> luke.encoder.layer.22.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.22.intermediate.dense.weight -> luke.encoder.layer.22.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.22.intermediate.dense.bias -> luke.encoder.layer.22.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.22.output.dense.weight -> luke.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.22.output.dense.bias -> luke.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.22.output.LayerNorm.weight -> luke.encoder.layer.22.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.22.output.LayerNorm.bias -> luke.encoder.layer.22.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.23.attention.self.query.weight -> luke.encoder.layer.23.attention.self_attention.query.weight torch.Size([1024, 1024])\n",
      "encoder.layer.23.attention.self.query.bias -> luke.encoder.layer.23.attention.self_attention.query.bias torch.Size([1024])\n",
      "encoder.layer.23.attention.self.key.weight -> luke.encoder.layer.23.attention.self_attention.key.weight torch.Size([1024, 1024])\n",
      "encoder.layer.23.attention.self.key.bias -> luke.encoder.layer.23.attention.self_attention.key.bias torch.Size([1024])\n",
      "encoder.layer.23.attention.self.value.weight -> luke.encoder.layer.23.attention.self_attention.value.weight torch.Size([1024, 1024])\n",
      "encoder.layer.23.attention.self.value.bias -> luke.encoder.layer.23.attention.self_attention.value.bias torch.Size([1024])\n",
      "encoder.layer.23.attention.output.dense.weight -> luke.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "encoder.layer.23.attention.output.dense.bias -> luke.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.23.attention.output.LayerNorm.weight -> luke.encoder.layer.23.attention.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.23.attention.output.LayerNorm.bias -> luke.encoder.layer.23.attention.output.layernorm.beta torch.Size([1024])\n",
      "encoder.layer.23.intermediate.dense.weight -> luke.encoder.layer.23.intermediate.weight torch.Size([4096, 1024])\n",
      "encoder.layer.23.intermediate.dense.bias -> luke.encoder.layer.23.intermediate.bias torch.Size([4096])\n",
      "encoder.layer.23.output.dense.weight -> luke.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "encoder.layer.23.output.dense.bias -> luke.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "encoder.layer.23.output.LayerNorm.weight -> luke.encoder.layer.23.output.layernorm.gamma torch.Size([1024])\n",
      "encoder.layer.23.output.LayerNorm.bias -> luke.encoder.layer.23.output.layernorm.beta torch.Size([1024])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight -> luke.embeddings.word_embeddings.embedding_table torch.Size([50265, 1024])\n",
      "embeddings.position_embeddings.weight -> luke.embeddings.position_embeddings.embedding_table torch.Size([514, 1024])\n",
      "embeddings.token_type_embeddings.weight -> luke.embeddings.token_type_embeddings.embedding_table torch.Size([1, 1024])\n",
      "embeddings.LayerNorm.weight -> luke.embeddings.layer_norm.gamma torch.Size([1024])\n",
      "embeddings.LayerNorm.bias -> luke.embeddings.layer_norm.beta torch.Size([1024])\n",
      "entity_embeddings.entity_embeddings.weight -> luke.entity_embeddings.entity_embeddings.embedding_table torch.Size([500000, 256])\n",
      "entity_embeddings.entity_embedding_dense.weight -> luke.entity_embeddings.entity_embedding_dense.weight torch.Size([1024, 256])\n",
      "entity_embeddings.position_embeddings.weight -> luke.entity_embeddings.position_embeddings.embedding_table torch.Size([514, 1024])\n",
      "entity_embeddings.token_type_embeddings.weight -> luke.entity_embeddings.token_type_embeddings.embedding_table torch.Size([1, 1024])\n",
      "entity_embeddings.LayerNorm.weight -> luke.entity_embeddings.layer_norm.gamma torch.Size([1024])\n",
      "entity_embeddings.LayerNorm.bias -> luke.entity_embeddings.layer_norm.beta torch.Size([1024])\n",
      "====================extract weights finished====================\n"
     ]
    }
   ],
   "source": [
    "extract_and_convert(torch_model,ms_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (LUKE_mindspore)",
   "language": "python",
   "name": "pycharm-6a7c9ef4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
